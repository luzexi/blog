---
layout: post
status: publish
published: false
title: 《Unity3D高级编程之进阶主程》第七章，渲染管线与图形学(三) - 渲染原理与知识2
description: "unity3d 高级编程 主程 shader 漫反射 镜面反射 自发光 球面反射"
excerpt_separator: ===
tags:
- 书籍著作
- Unity3D
- 前端技术
---

这节我们将继续补充前面渲染管线中没有讲到的渲染知识和原理。

### Filter 滤波方式

Filter滤波其实在图形引擎中被用到的地方有很多，我们在做项目时却很少察觉到，它的重要性不容忽视。Filter 滤波到底是什么呢？下面我们来讲讲它的来龙去脉。

一张纹理贴图可以是线性的，正方形的，长方形，甚至三维形式的，当它们被映射到网格表面，再变换到屏幕坐标系之后，纹理上的独立纹素(纹素)几乎不可能直接和屏幕上的最终画面像素直接对应起来。为什么呢？

因为屏幕上的物体可能是放大后的效果，或者离摄像机很近导致物体需要被放大在屏幕上，这样就导致是屏幕上的一个像素只是对应纹理贴图上一个纹素的一部分（因为被放大了），或者也有可能物体被缩小了，或者离摄像机很远，这时一个像素可能包含很多纹素（因为被缩小了）。因此一个纹素与一个像素通常都是无法一一对应的。

无论哪种情况我们都无法精确的知道应该使用哪些纹素值，以及如何对它们求平均值或插值。OpenGL就为我们提高了多种Filter 滤波方式，不同的滤波方式在速度和画质上做出了不同的权衡。

滤波一般分两种，一种是最近采样即Nearest，一种是线性采样即Linear。在Unity3D中Point类型的采样就是最近采样(Nearest Point Sampling)，线性采样在Unity3D中又分为双线性采样(Bilinear)和三线性采样(Trilinear)。

最近采样，即当纹素与像素大小不一致时，取最接近的纹素进行采样。虽然使用了Mipmap技术，但像素点与纹素仍然没有得到很好的匹配，所以这种方法在纹理需要进行放大或缩小时，结果会变形难看。于是就有了更好的双线性采样算法。

双线性采样，是取离纹素最近的4个纹素，取得的纹素与中心点的纹素的距离就是参与计算的权重值，把所有采样得到的纹素进行加权平均后得到最终的像素颜色。

双线性过滤是怎么计算的这4个纹素呢？假设源图像大小为m x n，目标图像为a x b。那么两幅图像的边长比分别为：m/a和n/b。目标图像的第（i,j）个像素点（i行j列）可以通过边长比对应到源图像。其对应坐标应该为（i*m/a,j*n/b）。显然这个对应坐标一般来说不是整数，而非整数的坐标是无法在图像中使用。双线性插值通过寻找距离这个对应坐标最近的四个像素点，来计算该点的像素值。

		映射点计算方法:

		srcX=dstX* (srcWidth/dstWidth)+0.5*(srcWidth/dstWidth-1)

		srcY=dstY* (srcWidth/dstWidth)+0.5*(srcWidth/dstWidth-1)

![双线性过滤](/assets/book/7/g1.png)

双线性过滤像素之间的过渡更加平滑，但是它只作用于一个MipMap Level,它选取纹素和像素之间大小最接近的那一层MipMap进行采样。当和像素大小匹配的纹素大小在两层Mipmap Level之间时，双线性过滤在有些情况效果就不太好。于是就有了三线性过滤。

三线性过滤以双线性过滤为基础。对像素大小与纹素大小最接近的两层Mipmap Level分别再进行一次双线性过滤，然后再对两层得到的结果进行线性插值。

上面的几种线性过滤外，还有各向异性过滤(Anisotropic Filtering)。什么是各向异性和同性呢：

		各向同性，当需要贴图的三维表面平行于屏幕就是各向同性。

		各向异性，当要贴图的三维表面与屏幕有一定角度的倾斜则是各向异性。

各向异性过滤，除了会把Mipmap因素考虑进去外，还会把纹理与屏幕空间的角度这个因素考虑时去。它会考滤一个像素对应到纹理空间中在u和v方向上与u和v的比例关系，如果u:v不是1:1时，将会按比例在各方向上采样不同数量的点来计算最终的结果。

各向异性采样的多少取决于Anisotropic Filtering的X值，所以在Unity3D的纹理图片设置上有一个Aniso Level的设置选项，用来设置Anisotropic Filtering的级别。

采样方式从，最近采样，到双线性过滤，到三线性过滤，再到各向异性过滤，采样次数一级级提高，最近采样1次，双线性采样4次，三线性采样8次，各向异性采样随着等级不同各有不同，效果也是一级级的提高，随着采样次数的提高需要消耗的GPU也会逐级提高(这些都是在OpenGL或DirectX中完成的)，因此我们在设置图片过滤时需要权衡画质与性能。

### 光照阴影是如何生成的

前面讲了很多关于Mipmap和纹理采样的知识，对Mipmap和纹理采样的理解对底层画面渲染的理解有很大的帮助。这些所讲的内容也同样具有重大意义，阴影涉及到实际生活中的知识在渲染中是如何模拟的，让原本虚拟的计算机画面更加拟真现实生活。

为了能让场景和人物看起来更加真实，光影效果是不可或缺的。我们经常能在画面中看到阴影跟随着物体摆动而变动，并且物体被光照遮挡的阴影投射在其他物体上，这样的效果十分动人，那么阴影是如何产生的呢？我们来细致的解析一下，通过解析我们能够更加深刻的理解阴影的生成原理，还可以通过对阴影原理的理解来有针对性的优化阴影对性能的消耗。

我们可以首先考虑真实生活中阴影的产生流程，当一个光源发射一条光线遇到一个不透明物体时，这条光想不能再继续照亮它背后的物体。因此这个物体背后的物体就会产生阴影，这些阴影区域的产生是因为光线无法到达这些区域。

在计算机的实时渲染中我们无法用表达出每条光照的射线，我们是如何做到阴影的投射的呢？

其实可以很简单，假设，我们将摄像机放在光源的位置上，方向与光源照射的方向重合，那些看不到的地方就成了阴影产生的地方。不过，我们不可能真的将摄像机放在那里，但却可以用这种形式的方式，单独渲染一次在摄像机在该位置的图像。只有图像不行，我们需要的是阴影，刚好物体从该位置渲染出来的片元的深度值提供了我们需要的数据，将所有片元的深度值都写入深度缓存中后，我们就可以利用这个深度缓存做阴影的计算了，深度值越大的片元被遮挡的可能性越大，深度值最小的片元不会被遮挡。

这就是阴影映射纹理(Shadow Map)技术，即在渲染中第一个渲染流程(pass)负责在光源点位置计算得到深度值，输出像素到阴影映射纹理(Shadow Map)。我们实质上得到是一张深度图，它记录了从该光源的位置出发，能看到的场景中距离它最近的表面位置的深度信息。

只是这张阴影映射纹理对于我们人类来说不够直观，如果想要更好更直观的看到当前摄像机屏幕上的阴影图就更好了。屏幕空间的阴影投影技术(Screenspace Shadow Map)就产生了这种直观的效果，性能和效率也更高，不过它需要显卡支持MRT(Multiple Render Targets)，有些移动平台并不支持这种特性。

当使用了屏幕空间阴影投射技术(Screenspace Shadow Map)时，Unity3D会让从光源出发的深度图与摄像机产生的深度图做比较，如果摄像机的深度图中记录的点的表面深度大于转化到光源出发生成的深度图的点的深度，那么就说明表面虽然是可见的但却处于该光源的阴影中。通过这样的方式，屏幕空间阴影投射技术(Screenspace Shadow Map)得到了当前摄像机屏幕空间中的阴影区域，即得到了当前摄像机屏幕的阴影图。

到这里，我们已经得到了阴影映射纹理，不管是传统的阴影图还是屏幕空间阴影图，我们都可以根据这张阴影图来计算出投射到物体的阴影了，那么阴影图有了，应该怎么投射呢？

主动计算投射到其他物体产生阴影是比较难的，但反过来，根据阴影图主动计算当前渲染物体上的片元是否被阴影是相对比较容易。我们会看到Unity3D在渲染物体上看到有生成阴影和接受阴影两个选项，即Cast Shadows 和 Receive Shadows。

传统的接受阴影的方式，是将当前顶点的位置变换到光源点的空间下得到它在光源空间中的位置，再根据xy轴分量对阴影映射纹理(Shadow Map)进行采样，从而得到阴影映射纹理中该位置的深度值，如果这个深度值小于该顶点的深度值即z轴分量，那么说明该点位于阴影中，于是在片元颜色输出上加深阴影颜色，反之则没有被阴影遮盖。

在屏幕空间阴影映射技术中(Screenspace Shadow Map)则可以更加方便一点，因为已经得到了当前摄像机整个屏幕的阴影图，不需要再进行额外的计算，只需要在当前屏幕位置对阴影图进行采样便能知道是否在阴影下，即将当前顶点坐标从模型空间变换到屏幕空间后使用这个坐标对阴影图进行采样即可。相对于传统的阴影渲染来说，屏幕空间阴影映射技术提高了更多的GPU性能效率。

在Unity3D中使用 LightMode 为 ShadowCaster 的Pass标记为阴影生成管线。当Unity3D在渲染时会首先在当前Shader中找到LightMode为ShadowCaster的Pass，如果没有则会在Fallback指定的Shader中继续寻找，如果没有则无法产生阴影，无论传统的阴影投射还是屏幕空间阴影投射都需要第一步先产生阴影纹理图(Shadow Map)。当找到LightMode为ShadowCaster的Pass后，Unity3D会使用该Pass来制作光源的阴影映射纹理(Shadow Map)。

### Lightmap原理

随着硬件技术的发展，人们对场景的画质效果越来越高，实时光照早已经满足不了人们对画质的需求，想要更加细腻真实光照效果，只能通过离线的烘培技术才能达到理想画质的效果。

全局光照，简称GI(Global Illumination)，是在真实的大自然中，光从太阳照射到物体和地面再经过无数次的反射和折射，使得地面的任何物体和地面都叠加着直接照射的光和许许多多物体反射过来的间接光(反射光)，导致我们从眼睛里看到画面是光亮又丰富的。

这种无数次反射和折射形成的高质量画面，才是人们在3D游戏画面中真正想要看到的。但是即使今天硬件技术发展的如此迅速，也无法做到实时的进行全局光照(Realtime Global Illumination)，这样计算量太大，CPU和GPU都无法负担起这个重任。

离线全局光照就担负起了这个丰富画面光照效果的重任，它不再需要这么多的实时计算的CPU和GPU算力，只要1张或几张光照图(Lightmap)就能将全局光照的效果复原到物体上，不过也仅限于场景静态物体的光照烘培。

其实烘培这趟水很深，如果要具体深入到工程上的实现，涉及到的算法和图形学知识非常多，这里并不打算深究，而是讲讲我们能相对容易获得的关于Lightmap的原理和知识。根据这个原理，我们在项目的制作和优化中能起到很好的作用。

什么是烘焙？个人认为从英文‘Bake’翻译过来有点偏差，导致很多工具按钮用‘Bake’表示时，很多人都同样把它理解成了烘培，其实更应该理解为‘制作’。

烘培简单地说, 就是把物体光照的明暗信息保存到纹理上, 实时绘制时不再需要进行光照计算, 而是采用预先生成的光照纹理(Lightmap)来表示明暗效果。

我们在渲染3D模型时用到的基本都是顶点，UV，纹理贴图等(这里不多展开)，在顶点上设置UV，形成片元后就成了顶点间的插值UV，用这个UV坐标去纹理贴图上取得颜色值，再填充到帧缓存中最后显示到画面上。

光照纹理(Lightmap)的显示也是同样道理，用UV坐标来取得光照纹理(Lightmap)上的颜色，最后叠加到片元颜色上输出给缓存。

这其中的UV有一点讲究。我们在制作模型时的UV数据可以有很多个，其中UV0主要是为了映射贴图纹理而用，在模型制作过程就在模型数据中制作好了，而UV1也就是我们程序中的uv2或俗称的2u，主要是为Lightmap做准备的，还有UV2即程序中的uv3，是为实时全局光照准备的，只有UV3即程序中的uv4开始才是我们程序可以自定义使用的UV数据，其实uv可以有很多个uv5，uv6，不过Unity3D的网格类(Mesh)暂时只提供到uv4的获取接口。

既然光照纹理(Lightmap)存储的是光照信息，那么它到底存了哪些信息呢？我们先来看下这幅图：

		缺图

这幅图解释了烘培的简单模型，它分为三个部分，第一部分为光线射到墙壁后反射过来照到模型上，第二部分为光线照射过来时被其他模型挡住，导致当前的模型没有被光线照射到并且有阴影产生，第三部分为光线直接照射到模型上产生的颜色信息。这三者之和最终形成了完全的光照颜色。由于直接光的计算代价比较少，在一些并不复杂的场景中并不记录直接光的照射。

我们在光照贴图中记录的就是从其他物体反射过来的光产生颜色的总和，以及产生阴影部位的信息。


### GPU Instancing 是什么？

### 多重采样(Multisampling)与反走样(Antialiasing)

		Quality Settings
		
		AntiAliasing	This sets the level of antialiasing that will be used. The options are 2x, 4x and 8x multi-sampling.

### 颜色缓存与高清屏意味着什么

### 2幂次纹理的读取与纹理压缩

### 闪烁的根源：坐标系变化后的z精度问题

### 什么是Overdraw

### 什么是卷积，卷积对后渲染的意义是什么

### 什么是PBR，现在为什么对PBR如此重视

PBR管线不是仅仅是为了画面上的效果，更为了大量重复工作规定了规范可流水线操作。

### 什么是渲染管线的升级，为什么要升级渲染管线

### SPR(Scriptable Render Pipeline)可编程管线，什么是高清管线HDRP(High-Definition Rendering Pipeline)，Unity3D中的轻量渲染管线LWRP与传统管线有什么差别，以及最近升级的通用管线

### 着色器编译原理

### 为什么要用CG来写Shader而不是其他的

