---
layout: post
status: publish
published: false
title: 《Unity3D高级编程之进阶主程》第六章，网络层(四) - 封装HTTP
description: "unity3d 高级编程 主程 network tcp udp http"
excerpt_separator: ===
tags:
- 书籍著作
- Unity3D
- 前端技术
---

### HTTP协议原理

HTTP俗称短连接，由于其连接的时间较短，不受前端所控制，所以在游戏开发圈内，通常被以‘短’字开头。

在Unity3D短连接可以用.NET库来编写，也可以用U3D的内置API的WWW来写。差别不是很大，WWW对.Net做了封装，其功能已经完全够用在游戏开发上了，即使有情况不够用再用.NET补充也是很容易的事。

其两者的区别主要还是WWW把.NET库封装后再加了层用协程使得我们开发者在使用时更加的便捷，.NET库则直接用了线程，导致开发者还需要关注主线程与子线程的资源锁。另外WWW里面已经实现了IP6转换，而使用.Net库则还需要自己解析IP6地址，虽然对程序员来说自己用.NET实现更加接近底层实现，但同时也带来了更多需要关注的地方，也夺走了程序员些许宝贵的精力。

不管协程还是线程，其实两边都需要缓存和队列，如果没有缓存和队列，当网络请求量放大时就会出现数据丢失的情况。所以缓存机制在网络层是不可或缺的。

我们所说的HTTP，最形象的描述就是网页(Web)形式的请求与回调。它被运用在最多的就是网页(Web)请求上，它是网页(Web) 上进行数据交换的基础。

HTTP是一种 Client-Server 协议。也就是说，请求通常是由像浏览器这样的User-
Agent发起的，当像浏览器接收方收到数据后，再通过数据来处理相应的逻辑，每个请求都是一一对应的，一个请求有且最多只能得到一个响应。

当我们要要展现一个网页时，浏览器首先发送一个请求目的是从服务器获取页面的HTML文档，再解析文档中的资源信息发送其他请求，例如获取可执行脚本或CSS样式来进行页面布局渲染，以及一些其它页面资源（如图片和视频等）。

然后，浏览器将这些资源整合到一起，展现出一个完整的网页。浏览器执行的脚本可以在之后的阶段发起更多的请求来获取更多信息和资源，从而可以不断的更新当前的网页上的内容。

我们常常会在游戏项目中把这套 Client-Server 协议搬到了游戏中运用，那么为什么要用HTTP呢，用HTTP有什么好处呢？

其最重要的原因是HTTP简单易用，上手成本低，扩展难度相对较低，因此被广大互联网程序员所喜爱。

HTTP是应用层上的协议。HTTP并不需要其底层的传输层协议是面向连接的，只需要它是可靠的，或不丢失消息的（至少返回错误）。在互联网中，有两个最常用的传输层协议：TCP是可靠的，而UDP不是。因此，HTTP依赖于面向连接的TCP进行消息传递，但连接并不是必须的。

也就是说，它本身并没有检测是否连接，数据是否传输准确，是否有数据到达等的机制，而是依赖于TCP协议，由TCP协议来做这些传输层的事情。HTTP在TCP之上，制定了自己的规则。

HTTP在TCP之上制定了自己的规则：

1.HTTP有自己的协议，HEADS和BODY

        GET /root1/module?name1=value1&name2=value2 HTTP/1.1
        Host: localhost:8080
        Accept-Language: fr
        //这里HEADS 和 BODY一定是用空行隔开的
        body content

其中 GET为方式，HTTP有两种方式，一种是GET,另一种是POST。GET方式是把参数值放在地址中，而POST则是把参数值放在协议数据包中，并且POST可以使用二进制作为参数值，而GET则不能。其两者的实质是一样的，都是以Key1=Value1&Key2=Value2的形式作为请求内容。

其协议中 HTTP/1.1为HTTP协议的版本号，我们现在常用的协议为1.1，也有少部分还在使用1.0。HTTP1.1兼容了1.0，并且在1.0之上改进了诸多内容，比如同一个地址不同host，增加了cache特性，增加Chunked transfer-coding标志切割数据块等。而2.0由于和1.1差别太大，不能兼容1.0和1.1，因此世界被分成了两块，HTTP2.0主要运用在HTTPS上，而HTTP1.1和1.0则运行在原有的HTTP上。

body content是主要存储请求内容的，POST内容都放在这里。

服务器收到请求处理相应逻辑后发送响应数据，其数据格式为：

        HTTP/1.1 200
        Date:Mon,31Dec200104:25:57GMT
        Server:Apache/1.3.14(Unix)
        Content-type:text/html
        Last-modified:Tue,17Apr200106:46:28GMT
        Content-length:xxx
        
        //这里HEADS 和 BODY一定是用空行隔开的
        body content

其中200为响应后代表请求成功的错误码，其他常用的错误码可以简单理解为 404 找不到请求页，500 服务器程序报错，400 访问请求参数错误，403 被拒绝访问。

2.HTTP是无状态但有会话的，两个执行成功的请求之间是没有关系的。

无状态，是指对于事务处理没有记忆能力，前后两次的请求并没有任何相关性，可以是不同的连接，也可以是不同的客户端，服务器在处理无状态请求时，只关注当下这个连接请求时的可获取到的数据，不会去关心也没有记忆去关联上一次请求的状态。

HTTP的访问请求一般都由软硬件做负载均衡来决定访问哪台物理服务器，进而两次同样地址的访问请求，有可能选择的服务器是不同的。

无状态很好的匹配了这种近乎随机的访问方式，也就是说HTTP客户端可以任意选择一个部署在不同区域的服务器进行访问，得到的结果是相同的。

3.HTTP每次请求访问结束会断开连接。

HTTP是依据什么来断开连接的呢？content-length。

content-length为 heads 上的标记，表示 body 内容长度。

带content-length，body长度是可知的，客户端在接收body内容时，就可以依据这个长度来接受数据。接受完毕后，就表示这个请求完毕了。客户端主动调用close进入四次挥手断开连接。

假如没有content-length 标记，那么body内容长度是不可知的，客户端会一直接受数据，直到服务端主动断开。

HTTP1.1在这个断开规则之上又扩展了一种新规则，即增加了Transfer-encoding标记。如果Transfer-encoding为chunked，则表示body是流式输出，body会被分成多个块，每块的开始会标识出当前块的长度，此时body不需要通过content-length长度来指定了。

HEADS上带有Transfer-encoding:chunked 就表示body被分成很多块，每块的长度也是可知的，当客户端根据长度接受完毕数据后，就会主动断开连接。

假如说Transfer-encoding 和 content-length 这两个标记都没有，那么就只能一直接受数据直到服务器主动断开连接。

那么有没有一种可能，既想使用HTTP协议又不想断开连接的？

使用keep-alive标识，keep-alive标识会让客户端与服务器的连接保持状态，直到服务器发现空闲时间结束而断开连接，在结束时间内我们仍然能发送数据。也在就是说，可以减少多次的与服务器3次握手建立连接，以及多次与服务器4次握手断开连接，提高了传输效率。

而在服务器端上，Nginx的 keepalive_timeout，和Apache的 KeepAliveTimeout 都能设置 Keep-alive 的空闲时间大小，当httpd守护进程发送完一个响应后，理应马上主动关闭相应的TCP连接，但设置 keepalive_timeout后，httpd守护进程会说：”再等等吧，看看客户端还有没有请求过来”，这一等，便是 keepalive_timeout时间。如果守护进程在这个等待的时间里，一直没有收到客户端发过来HTTP请求，则关闭这个HTTP连接。

但也不一定说使用keep-alive标识能提高效率，有时也会反而降低了效率。比如经常会没有数据需要发送，导致长时间的Tcp连接保持导致系统资源无效占用，浪费系统资源，巨量的保持连接会浪费巨量的资源。

如果客户端使用keep-alive，则需要在连续发送数据时，使用同一个HTTP连接实例。并且在发送完毕后要记录空闲时间，以便再次发送时，可以判断是否继续使用该连接，因为通常服务器端主动断开连接后并没有被客户端及时的得知，所以自行判断是否有可能已经被服务器端断开连接为好。还有一个问题是，如果网络环境不好导致发送请求无法到达时，则要尽可能的自己记录和判断，哪些数据是需要重发的。这几个问题增加了HTTP作为keep-alive来保持连接的操作难度，将本来简单便捷的HTTP，推向了有点长连接的味道。

###### 在Unity3D中的HTTP封装

Unity3D的2018版本中原本经常用的WWW类已经被废弃，取而代之的是UnityWebRequest。

API改了但功能都是一样的，我们主要还是围绕HTTP的原理来写程序，也只有这样才能真正写出程序的精髓。

UnityWebRequest里有几个接口对我们来说比较重要，一个是Post(string uri, WWWForm postData)接口用来创建一个带有地址和Post数据的UnityWebRequest实例，一个是SendWebRequest()用来开始发送请求和迭代请求的接口，另一个是SetRequestHeader(string name, string value)用来设置HTTP头的，其中它说下面这些HEAD标记不支持设置：

        These headers cannot be set with custom values on any platform: accept-charset, access-control-request-headers, access-control-request-method, connection, date, dnt, expect, host, keep-alive, origin, referer, te, trailer, transfer-encoding, upgrade, via.

这些标记都是什么意思呢，

        accept-charset: 用于告诉服务器，客户机采用的编码格式

        access-control-request-headers: 在预检请求中，用于通知服务器在真正的请求中会采用哪些请求首部

        access-control-request-method: 在预检请求中，用于通知服务器在真正的请求中会采用哪种 HTTP 方法

        connection: 处理完这次请求后是否断开连接还是继续保持连接

        date: 当前时间值

        dnt:  (Do Not Track) 表明了用户对于网站追踪的偏好。

        expect: 是一个请求消息头，包含一个期望条件，表示服务器只有在满足此期望条件的情况下才能妥善地处理请求。服务器开始检查请求消息头，可能会返回一个状态码为 100 (Continue) 的回复来告知客户端继续发送消息体，也可能会返回一个状态码为417 (Expectation Failed) 的回复来告知对方要求不能得到满足。

        host: 请求头指明了服务器的域名(对于虚拟主机来说)，以及服务器监听的TCP端口号。

        keep-alive: 允许消息发送者暗示连接的状态，还可以用来设置超时时长和最大请求数。

        origin: 指示了此次请求发起者来自于哪个站点。

        referer: 表示当前页面是通过此来源页面里的链接进入的，与origin相似。

        te: 指定用户代理希望使用的传输编码类型

        trailer: 允许发送方在分块发送的消息后面添加额外的元信息，这些元信息可能是随着消息主体的发送动态生成的，比如消息的完整性校验，消息的数字签名，或者消息经过处理之后的最终状态等。

        transfer-encoding: 指明了将 entity 安全传递给用户所采用的编码形式。transfer-encoding是一个逐跳传输消息首部，即仅应用于两个节点之间的消息传递，而不是所请求的资源本身。一个多节点连接中的每一段都可以应用不同的Transfer-Encoding 值。

        upgrade: 升级为其他协议

        via: 代理服务器相关的信息

以上这些由HEAD扩展的HTTP功能都不能进行自主的选择，其中包括了我们比较关心的标识 connection 和 keep-alive 保持连接的功能，代表了我们无法用UnityWebRequest来实现一次连接发送多次数据的需求。

另外我们最关心的content-length也不能被自定义设置，而是由API本身来自动设置。


如果需要发送head信息可以用

public WWW(string url, byte[] postData, Dictionary<string,string> headers);

最简单的就是设计个类，比如建个名称为HTTPRequest的类，每次请求服务器都调用这个类的方法来处理一个请求的操作。把地址，参数，回调传进去，等待服务器相应和回调。

短连接接口少，大概就是请求接口（包括，地址，参数，参数里包括，GET和POST，HEAD和BODY），回调接口，发生错误接口（错误包括超时，无响应，404，错误页面等）。只要做好这几个接口的封装基本就能搞定单个HTTP请求的组件需求。

接下来我们详细讲解下HTTP实现的几个关键点和难点，以及相关注意事项。这里以WWW为API来实现，核心部分的通信。

（基本发送与接收）
使用WWW发送接收很方便，可以用协程也可以在逻辑更新中进行判断收发过程。
在协程中不太可控，因为协程结束时随着函数调用结束而结束的，而我们需要时常暂停，继续等操作，协程确实不太方便。所以一般我都是把WWW的收发判断移到逻辑更新中Update里去的，这样做容易理解，也容易实现，更容易掌控。

下面是WWW的收发过程：
1.建立实例开始连接和发送请求www = new WWW( URL )，以及设置好，这次请求回应的回调句柄。
2.判断是否完成连接和发送请求，用www.isDone判断是否完成连接与发送。

伪码如下：
void StartRequest(string url, Callback _callback)
{
    www = new WWW(url);
    Callback =  _callback;
}

void Update()
{
    if(www != null)
    {
        if(www.isDone)
        {
            ProcessResponse(www);
            www.Despot();
            www = null;
}
}
}
3.处理数据，先判断有错误存在，如果不是则对数据进行处理，HTTP回应的数据都是文本数据，比如Json格式的，Xml格式的，等等。对数据进行解析后变为具体的类实例，再传给相应的函数句柄进行调用。所以这里数据格式协议也是关键。

void ProcessResponse(WWW _www)
{
    if(_www.error != null)
    {
        NetworkErrorReport(_www.error);
        return;
}
CallbackResponse(_www.text);
}

以上就是用WWW做HTTP请求的基本步骤。看似简单的连接，发送，接收过程，里面需要我们做的事情也不少。这里主要是针对，游戏逻辑中，大量多次，频繁发送HTTP请求的问题的延伸和提出的解决方案。

（多次或者连续发送HTTP请求引起的问题）
多次或者连续请求HTTP是很常见的操作，比如客户端向服务器请求角色信息，并且请求军团信息，并且请求每日任务信息，然后再显示主界面。这种连续的多次的请求HTTP，会同时触发请求线程想服务器做请求操作，而得到服务器返回时，却不知道哪个在先哪个在后，例如军团信息先得到响应，然后再是每日任务信息得到了响应，再是角色信息得到了响应，因为是多个线程发起的多个连接，所以并不能确定响应的顺序也是同请求的顺序是一样的，导致在显示主界面时需要花更多的逻辑去判断请求信息是否得到回应。
所以客户端更希望是，在多次请求发出后，收到的响应也是按原来请求的顺序响应的。所以我主张只开一个线程处理请求，用排队的方式处理请求和响应。

伪码如下：
void RequestHttp(string url)
{
    PushRequest(url);
}
void Update()
{
    If( IsHttpFinished() )
    {
        Request  req = PopRequest();
        StartHttpRequest(req);
}
Else
{
    WaitingforRequestProcess();
}
}

从伪码看出，所有请求都会被推入到队列中，然后当请求空闲时立即开始Http请求。这样就每次都只进行一次请求，请求完毕后再进行下一次，确保了请求顺序和回应顺序的一致性。
但是问题又出来了，每次只发送一个请求，导致请求的数量太多，时间间隔太大，效率不够，当你连续请求10个的时候，由于网络延迟，有可能会等3-5秒，这是让人无法接受的。就单单一个操作造成的10个连续请求能让人等这么久，那么如果操作多了或者频繁了，请求得到的响应就更加久了，需要等待的时间也更加长，导致游戏体验非常差。当然这是在网络状况差的情况下发生，一般情况都不会有延迟感觉。那么为了优化这个问题，我们可以对多个请求进行合并，比如我现在10个请求，合并成一个，响应的结果中包含了这10个请求的数据。这样就快了不少，因为请求减少了，原本10个请求，现在变成了一个，即使网络环境差，丢包需要重发，也只要重发单个数据包就可以了，而不是每丢一次重发一个。

以上在多次连续发送HTTP请求的情况下出现的问题，下面将详细介绍这些解决方案的细节。

（发送队列和接收队列）
队列在请求和响应中起到了缓冲的作用，当连续使用HTTP请求，连续收到HTTP响应的时候，能够依次处理相应的逻辑。
因为全程是在主线程里发生的，所以并不需要线程锁之类的操作。发送时，向队列推送请求实例，在逻辑更新时，判断是否有请求在队列里，有的话推出来，做HTTP请求操作，并将请求的相关信息存起来，当得到服务器响应时，调用请求信息中的回调函数，处理回调句柄。

发送请求伪码
void RequestHttp(Request req)
{
    ListRequest.Push(req);
}

void Update()
{
    If( IsHttpFinished() )
    {
        Request  req = ListRequest.Pop();
        mCurrentRequest = req;
        StartHttpRequest(req);
}
Else
{
    If( HttpIsDone())
{
    ProcessResponse(data);
}
}
}

void ProcessResponse(Response data)
{
    If(mCurrentRequest != null
&& mCurrentRequest.Callback != null)
    {
        mCurrentRequest.Callback(data);
}
}

以上阐述了，在发送HTTP请求时的推入队列操作和在收到请求后的句柄响应操作。

（加速请求与响应，压缩和减少发送次数）
每次发送一个请求，都要等待前面的数据包的回调后才能进行，当数据包频繁发送时，就容易引起巨量延迟的问题。

怎么办？
能不能加快响应速度？可以。压缩数据，以降低宽带使用率，减少丢包时重发的数据包大小，从而提高发送和接受的速度。

具体步骤挺简单的，就是找一个压缩算法，或者压缩组件，比如zip，LZMA等，客户端在发送请求钱，对发送的数据进行压缩，当服务器收到请求数据后，先对请求数据进行解压操作，然后再提取请求数据。客户端得到服务器响应请求的时候也是同样的步骤，先对响应数据进行解压，然后再对响应数据进行解析。

压缩的操作实现起来比较简单，但能得到的效果确实不那么好。因为关键的响应速度还是靠网络环境和服务器的响应时间，宽度的问题只是次要问题，没有网络环境和服务器响应速度的支持，光减少数据大小来提高响应速度，效果肯定让人失望。

能不能减少请求次数？可以，合并数据包，让多次发送的请求数据合并到一个包内，作为一次发送的数据包，相应时也是一样，多个相应数据包在同一个相应数据包内，分别做了客户端可识别的标记。

如何实现呢？
我们以json为例，客户端请求是json格式，服务器响应也是json格式。每次请求，客户端都会发送格式给服务器，服务器收到后解析json格式数据，然后提取变量。
我们在合并请求的时候可以这么做。多个json请求格式，推入一个大的json框中，为每个json数据加一个序列号，代表请求顺序，如下：

请求1
{
    "request-order" : 1,
    "data1" : "i am text",
    "data2" : "i am num",
}

请求2
{
    "request-order" : 2,
    "book" : "i am text",
    "chat" : "i am num",
    "level" : 1，
}

请求3
{
    "request-order" : 3,
    "image" : "i am text",
    "doc" : "i am num",
}

合并后
{
    "order" : 1,
    "data":
    [
        {
            "request-order" : 1,
            "data1" : "i am text",
            "data2" : "i am num",
        }
        ,
        {
            "request-order" : 2,
            "book" : "i am text",
            "chat" : "i am num",
            "level" : 1;
        }
        ,
        {
            "request-order" : 3,
            "image" : "i am text",
            "doc" : "i am num",
        }
    ],
}

合并后多个json被推入数组中发送给服务器，服务器拿到数据后对data中的数据进行提取和处理，处理顺序按照request-order来做，所有响应数据都被推入同一个json实例中，并且附上请求数据中的request-order，再把整个响应数据发送给客户端，当客户端收到数据时，就可以通过request-order知道响应数据的顺序了，再按照响应顺序进行解析和回调。

相应合例子如下。

响应数据1
{
    "response-order" : 1,
    "data1" : "i am text",
    "data2" : "i am num",
    "status" : "success", 
}

响应数据2
{
    "response-order" : 2,
    "data1" : "i am text",
    "data2" : "i am num",
    "status" : "success",
}

响应数据3
{
    "response-order" : 3
    "data1" : "i am text",
    "data2" : "i am num",
    "status" : "success",
}

合并响应数据
{
    "order" : 1,
    "data":
    [
        {
            "response-order" : 1,
            "data1" : "i am text",
            "data2" : "i am num",
            "status" : "success", 
        }
        ,
        {
            "response-order" : 2,
            "data1" : "i am text",
            "data2" : "i am num",
            "status" : "success",
        }
        ,
        {
            "response-order" : 3
            "data1" : "i am text",
            "data2" : "i am num",
            "status" : "success",
        }
    ]
}

合并响应数据后，客户端先提取data数据中的所有数据，先进行排序，以respose-order为基准，从小到大排序，最小的次序先处理，解析，回调。

整个过程就是这样的，对多个请求进行合并后，减少了不少HTTP请求的次数，对速度的提升也是很有效果的。这里还有一些细节问题，比如多少个开始进行合并，或者多少时间内合并一次。我的经验是，为了让HTTP最大效率的得到提升，每次请求得到响应后就应该立即进行下一次HTTP请求，不要有等待合并间隔，反而减低了连接效率。但每次合并数量可以做一些限制，比如有1000个请求，我们不能统统合并了，为了保证发送数据大小和回调数据大小合适，也保证服务器不会以下为了处理1000个数据请求而当机，我们必须限制合并的个数，比如每次最多合并10个请求数据包，来保证减少连接次数减少的情况下，服务器不会在瞬间压力过大。

（其他注意点）
其他注意点，比如数据的解析与回调，针对不同格式的数据解析的步骤也是不一样的，协议后对存储的回调函数进行调用。还有当需要多服务器操作时，需要增加可选择的服务器地址。还有数据超时的重发机制，比如重发3次还是超时，再做弹框的提醒等，都是具体项目中具体应用的逻辑。
可以参考，https://github.com/luzexi/Unity3DNetwork-http


八．  网络加密与压缩
短连接可以用md5前面的所有参数作为校验依据，防止篡改。
加密方式
支持IP6以符合苹果审核，用域名
九．  优化
*短连接用www，做队列，单任务作业，合并请求和回调(每个请求有编号，对应回调)
*长连接自己写，协议可以用protobuff，缩小协议长度
*短连接用md5，token验证参数正确性
*两边加密可以把公钥写在so文件里
*长连接自己写，做队列，锁，多任务作业