<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title></title>
        <description>技术，哲学，金融</description>
        <link>http://www.luzexi.com</link>
        <atom:link href="http://www.luzexi.com/assets/feed.xml" rel="self" type="application/rss+xml" />
        
            <item>
               <title>读书笔记(五十一) 《游戏引擎架构》#4 低阶渲染器（4）</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247485096&amp;amp;idx=1&amp;amp;sn=c027b6af4677515ba1e5950aac8ba68d&amp;amp;chksm=fc2263afcb55eab9204898b39fb77ee04f5767ac450507b6192a5c6d5612a45b6d29b1bcee16&amp;amp;token=853743270&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景：&lt;/h1&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了几遍《游戏引擎架构》后对引擎架构的理解又深入了些。&lt;/p&gt;

&lt;p&gt;近段时间有对引擎剖析的想法，正好借这书本对游戏引擎架构做一个完整分析。&lt;/p&gt;

&lt;p&gt;此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节。&lt;/p&gt;

&lt;p&gt;借助《游戏引擎架构》这本书、结合引擎源码和自己的经验，深入分析游戏引擎的历史、架构、模块，最后通过实践简单引擎开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;同时《游戏引擎架构》中部分知识太过陈旧的部分，会重新深挖后总结出自己的观点。&lt;/p&gt;

&lt;h1 id=&quot;概述&quot;&gt;概述：&lt;/h1&gt;

&lt;p&gt;本系列文章对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;p&gt;1.时间库
2.自定义容器库
3.字符串散列库
4.内存管理框架
5.RTTI与反射模块
6.图形计算库
7.资产管理模块
8.低阶渲染器
9.剔除与合批模块
10.动画模块
11.物理模块
12.UI底层框架
13.性能剖析器的核心部分
14.脚本系统
15.视觉效果模块&lt;/p&gt;

&lt;p&gt;本篇内容为列表中的第8个部分的第1节。&lt;/p&gt;

&lt;h1 id=&quot;正文&quot;&gt;正文：&lt;/h1&gt;

&lt;p&gt;简单回顾下前文&lt;/p&gt;

&lt;p&gt;前文我们聊了下显卡在计算机硬件主板中的位置与结构，知道了CPU、GPU的通信介质，并简单介绍了手机上的主板结构。本篇开头对上一篇做一些内容补充，PC和手机的不同硬件组织，以及CPU与其他芯片的通信过程。&lt;/p&gt;

&lt;p&gt;下面我们开始这篇内容&lt;/p&gt;

&lt;p&gt;本次内容会围绕GPU来写，从硬件架构到软件驱动再到引擎架构，目标是帮大家理解GPU硬件的运作原理，理解图形接口的架构，理解引擎低阶渲染器的架构。&lt;/p&gt;

&lt;p&gt;目录：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;主板结构中的显卡&lt;/li&gt;
  &lt;li&gt;GPU功能发展史&lt;/li&gt;
  &lt;li&gt;GPU与CPU的差异&lt;/li&gt;
  &lt;li&gt;GPU硬件特点&lt;/li&gt;
  &lt;li&gt;图形驱动程序架构&lt;/li&gt;
  &lt;li&gt;引擎低阶渲染架构&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;内容结构：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CPU硬件结构&lt;/li&gt;
  &lt;li&gt;GPU硬件结构&lt;/li&gt;
  &lt;li&gt;GPU手机管线与PC管线的差异&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接着上篇的内容。前面说了CPU、GPU的硬件结构，CPU的构造和GPU的构造，下面我们来聊聊GPU是如何工作的，以及GPU的管线在手机端和PC端的差异。&lt;/p&gt;

&lt;h2 id=&quot;nvidia基于fermi管线的架构&quot;&gt;NVIDIA基于Fermi管线的架构&lt;/h2&gt;

&lt;p&gt;关于GPU的逻辑管线，这篇Nvidia这篇文章《Life of a triangle - NVIDIA’s logical pipeline》说的很清楚。&lt;/p&gt;

&lt;p&gt;（NVIDIA的整体架构图）&lt;/p&gt;

&lt;p&gt;下面我以此为标准进行翻译并重新剖析。&lt;/p&gt;

&lt;p&gt;为了简单起见，省略了几个细节，假设 drawcall 引用了一些已经充满数据并存在于 GPU DRAM 中的索引和顶点缓冲区，并且仅使用顶点和像素着色器（GL：片段着色器）。&lt;/p&gt;

&lt;p&gt;（从图形API调用到图元处理过程图）&lt;/p&gt;

&lt;p&gt;1.引擎或业务程序调用图形 API（DX 或 GL）中的绘图函数，接着驱动程序会被调用，驱动程序会进行一些验证以检查参数是否“合法”，再将指令写入到GPU可读写的缓冲队列中。在这个地方 CPU 方面可能会出现很多瓶颈，这也是为什么程序员要好好使用 API 以利用当今 GPU 的强大功能的技术很重要的原因。&lt;/p&gt;

&lt;p&gt;（绘制接口调用图）&lt;/p&gt;

&lt;p&gt;2.经过一段时间渲染，画面“刷新”被调用，驱动程序在缓冲区中已经缓冲了足够多的工作命令，接着将其发送给 GPU 进行处理（操作系统会参与）。最后 GPU 的主机接口接收命令并交给GPU前端的处理。&lt;/p&gt;

&lt;p&gt;（绘制队列与刷新图）&lt;/p&gt;

&lt;p&gt;3.接着图元分配器（Primitive Distributor）开始分配工作。为了批量处理索引和三角形，将数据发送给多个图形处理集群（GPC）并行处理。&lt;/p&gt;

&lt;p&gt;（SM整体结构图）&lt;/p&gt;

&lt;p&gt;4.在 GPC 中，每个 SM 的 Poly Morph 引擎负责从三角形索引中获取顶点数据(Vertex Fetch)。&lt;/p&gt;

&lt;p&gt;5.在获取数据后，SM中每32个线程为一捆线程束（Warp），它们被调度去处理这些顶点工作。 线程束（Warp）是典型的单指令多线程（SIMT，SIMD单指令多数据的升级）的实现，也就是32个线程同时执行的指令是一模一样的，只是线程数据不一样，这样的好处就是一个Warp只需要一套逻辑对指令进行解码和执行就可以了，芯片可以做的更小更快。&lt;/p&gt;

&lt;p&gt;（线程束与线程束调度器图）&lt;/p&gt;

&lt;p&gt;6.SM的线程束（Warp）调度器会按照顺序分发指令给整个线程束（Warp），单个线程束（Warp）中的线程会锁步(lock-step)执行各自的指令。线程束（Warp）会使用SIMT的方式来做分支预测，每个线程执行的分支会不同，当线程遇到到错误判断的执行情况会被遮蔽(be masked out)。&lt;/p&gt;

&lt;p&gt;（单个GPU线程与存储设备的关系图）&lt;/p&gt;

&lt;p&gt;被遮蔽的原因是SIMT执行中错误预测，例如当前的指令是if(true)的分支，但是当前线程的数据的条件是false，或者循环的次数不一样（比如for循环次数n不是常量，或被break提前终止了但是别的还在走），因此在Shader中的分支会显著增加时间消耗，在一个线程束（Warp）中的分支除非32个线程都走到同一个里面，否则相当于所有的分支都走了一遍，线程不能独立执行指令而是以线程束（Warp）为单位，而这些线程束中的线程之间才是相互独立的。&lt;/p&gt;

&lt;p&gt;（SIMT线程束做分支预测图）&lt;/p&gt;

&lt;p&gt;7、线程束（Warp）中的指令可以被一次完成，也可能经过多次调度，例如通常SM中的LD/ST(加载存取)单元数量明显少于基础数学操作单元。&lt;/p&gt;

&lt;p&gt;8、由于某些指令比其他指令需要更长的时间才能完成，特别是内存加载，线程束（Warp）调度器可能会简单地切换到另一个没有内存等待的线程束（Warp），这是GPU如何克服内存读取延迟的关键，其操作为简单地切换活动线程组。为了使这种切换更快，调度器管理的所有线程束（Warp）在寄存器列阵（Register File）中都有自己的寄存器。这里就会有个矛盾产生，Shader需要的寄存器越多，给线程束（Warp）留下的空间就越少，于是就会导致能用的线程束（Warp）就越少。此时如果碰到指令在内存获取数据等待就只会等待，而没有其他可以运行的线程束（Warp）可以切换。&lt;/p&gt;

&lt;p&gt;（线程与寄存器列阵关系图）&lt;/p&gt;

&lt;p&gt;（线程束调度器调度线程图）&lt;/p&gt;

&lt;p&gt;9、一旦线程束（Warp）完成了顶点着色器（vertex-shader）的所有指令，运算结果会被Viewport Transform模块处理，三角形会被裁剪然后准备光栅化，此时GPU会使用L1和L2缓存来进行顶点着色起（vertex-shader）和片元着色起（pixel-shader）的数据通信。&lt;/p&gt;

&lt;p&gt;（管线中节点、数据、存储器的关系图）&lt;/p&gt;

&lt;p&gt;10、接下来这些三角形将被分割，通过 Work Distribution Crossbar 将三角形再分配给多个GPC，三角形的范围决定着它将被分配到哪个光栅引擎(raster engines)，每个光栅引擎（raster engines）覆盖了多个屏幕上的图块（tile），这等于把三角形的渲染分配到多个图块（tile）上面。也就是说在像素阶段前就把三角形划分成了方块格式范围，三角形处理分成许多较小的工作。&lt;/p&gt;

&lt;p&gt;（三角形被拆分成多个块派发到多个光栅引擎图）&lt;/p&gt;

&lt;p&gt;（图块拆分任务派发图）&lt;/p&gt;

&lt;p&gt;11、SM上的属性安装器（Attribute Setup）保证了从顶点着色器（vertex-shader）生成的数据，经过插值后，在片元着色器（pixel-shade）上是可读的。&lt;/p&gt;

&lt;p&gt;12、GPC上的光栅引擎(raster engines)处理它接收到的三角形，并为它负责的那些部分生成像素信息（同时会处理裁剪Clipping、背面剔除和Early-Z剔除）。&lt;/p&gt;

&lt;p&gt;13、再次做批量处理，32个像素线程被分成一组（或者说8个2x2的像素块），这是在像素着色器上面的最小工作单元（2x2 四边形允许我们计算诸如纹理 mip 贴图过滤之类的导数–四边形内纹理坐标的大变化会导致更高的 mip）。在这个像素线程内，如果没有被任何三角形覆盖就会被剔除。SM中的线程束（Warp）调度器会管理像素着色器的任务。&lt;/p&gt;

&lt;p&gt;（2x2像素组传入到线程束处理像素着色器的图）&lt;/p&gt;

&lt;p&gt;14、接下来是同样的线程束调度策略，和顶点着色器（vertex-shader）中的逻辑步骤完全一样，但是变成了在像素着色器线程中执行。 由于不耗费任何性能从2x2四边形中获取一个像素，这使得锁步执行非常便利，于是所有的线程可以保证指令可以在同一点同步执行。&lt;/p&gt;

&lt;p&gt;（线程束锁步执行图）&lt;/p&gt;

&lt;p&gt;15、最后一步，现在像素着色器已经完成了颜色的计算和深度值的计算。在这个点上，我们必须考虑三角形的调用API顺序，然后才将数据移交给ROP(render output unit，渲染输出单元)，一个ROP内部有很多ROP单元，在ROP单元中处理深度测试和帧缓冲（framebuffer）的混合等，深度和颜色的设置必须是原子操作，否则两个不同的三角形在同一个像素点就会有冲突和错误。NVIDIA 通常应用内存压缩，以减少内存带宽要求，从而增加“有效”带宽。&lt;/p&gt;

&lt;p&gt;（像素着色器后的像素处理过程图）&lt;/p&gt;

&lt;p&gt;以上这些信息有助于我们理解 GPU 中的一些工作和数据流，还可以帮助我们理解CPU与GPU之间的交互。&lt;/p&gt;

&lt;h2 id=&quot;总结cpu和gpu的交互&quot;&gt;总结CPU和GPU的交互&lt;/h2&gt;

&lt;p&gt;GPU是设备，设备都有驱动，CPU可以直接执行二进制指令集，对于GPU设备，图形接口有opengl，directx标准及库封装，计算有cuda和opencl封装。程序代码调用这些图形或计算库，这些库调用驱动，驱动再来对接操作GPU设备，CPU与GPU直接的通信是遵循总线和内存的规则。&lt;/p&gt;

&lt;p&gt;原则上CPU、内存外的设备都属于IO设备，通过总线连上来，它们必须遵守IO总线规范，如显卡就走pcie总线，这里还有ionmu，统一内存等，来共享资源，缩短路径，提升效率等。&lt;/p&gt;

&lt;p&gt;这里专门说下驱动，计算机有专门的程序接口指定一个计算任务到GPU上，这个接口程序就是驱动程序。CPU给GPU下发任务时通过调用驱动程序，不同GPU厂商实现自己的驱动，并且提供了各种的编程接口。图形计算上实现了OpenGL标准接口规范的图形库，它会调用各厂商的驱动，用户可以通过GLSL编写计算任务进行通用计算。后来的CUDA编程模型专门推出用于编写通用计算任务的接口，于是OpenGL就专门用于图形渲染了。而CUDA则是通过kenel函数来编写计算任务，通过cudaLaunch接口来下发任务。&lt;/p&gt;

&lt;p&gt;（从图形库到驱动到GPU指令队列的图）&lt;/p&gt;

&lt;p&gt;从硬件角度看：&lt;/p&gt;

&lt;p&gt;（从硬件角度看指令和数据处理流程图）&lt;/p&gt;

&lt;p&gt;1.GPU设备的配置空间物理地址映射到虚拟地址，可以被程序直接访问；同时建立任务队列缓冲，声明中断等等；&lt;/p&gt;

&lt;p&gt;2.CPU在进程内准备数据和缓冲，基于虚拟地址VA、VM将其转换为显存的物理地址IPA。驱动程序获取任务，再将任务信息填充至任务队列内。&lt;/p&gt;

&lt;p&gt;3.根据虚拟内存绑定的地址信息，将任务队列的指针更新至GPU设备侧，这个端口称为doorbell寄存器；&lt;/p&gt;

&lt;p&gt;4.设备接收到doorbell操作，会触发中断，再读取主存中的任务队列，包括队列内的信息和其指向的任务数据，GPU设备侧读取该数据。&lt;/p&gt;

&lt;p&gt;5.完成后，再将数据发送给CPU侧。一般来说，GPU设备侧发送至CPU的读写请求使用的是虚拟地址，由CPU的IOMMU或SMMU转换为物理地址。&lt;/p&gt;

&lt;h2 id=&quot;gpu手机管线与pc管线的差异&quot;&gt;GPU手机管线与PC管线的差异&lt;/h2&gt;

&lt;p&gt;为什么要了解手机与PC管线的差异？
PC的能耗和发热比手机端可以更大一些，因此PC与手机在硬件架构上有天然的不同，进而使得它们在GPU管线上也有很大的差异，这使得我们在优化手机端时必须了解这种差异再做针对性的做优化。&lt;/p&gt;

&lt;h2 id=&quot;tbdrtile-base-deffered-rendering是现代移动端gpu的设计架构它同传统pc上irimmediate-rendering架构的gpu在硬件设计上有很大的差别&quot;&gt;TBDR(Tile-Base-Deffered-Rendering)是现代移动端GPU的设计架构，它同传统PC上IR（Immediate-Rendering）架构的GPU在硬件设计上有很大的差别。&lt;/h2&gt;

&lt;p&gt;为什么呢？因为功耗是Mobile设备设计的第一考虑因素，而带宽是功耗的第一杀手。&lt;/p&gt;

&lt;p&gt;我们来看PC的GPU管线，即传统的IR（Immediate-Rendering）模式：&lt;/p&gt;

&lt;p&gt;（IMR管线图：源自网络）&lt;/p&gt;

&lt;p&gt;IMR（Immediate Mode Rendering）模式中，GPU直接在主存或显存上读写深度缓存（Depth Buffer）和帧缓存（Frame Buffer），这导致带宽消耗很大，如果在手机上耗电和发热都无法承受。&lt;/p&gt;

&lt;p&gt;手机使用统一内存架构，CPU和GPU都通过总线来访问主存。GPU需要获取三角形数据（Geometry Data）、贴图数据（Texture Data）以及帧缓存（Frame Buffer），它们都在主存中。如果GPU直接从主存频繁地访问这些数据，就会导致带宽消耗大，成为性能瓶颈。&lt;/p&gt;

&lt;h2 id=&quot;tbrtile-based-rendering管线&quot;&gt;TBR（Tile-Based-Rendering）管线&lt;/h2&gt;

&lt;p&gt;基于以上所述原因，手机GPU使用自己的缓存区（SRAM），例如On-Chip深度缓存（On-Chip Depth Buffer）和On-Chip颜色缓存（On-Chip Color Buffer），它们与存取主存相比，速度更快，功耗更低。但它们的存储空间很小。（SRAM不需要充电来保持存储记忆，因此SRAM的读写基本不耗电，缺点是价格昂贵）&lt;/p&gt;

&lt;p&gt;如果手机直接读写帧缓存（Frame Buffer）就相当于让一辆火车在你家和公司之间来回奔跑，非常耗电。于是手机端想要拆分绘制内容，每次只绘制一小部分，再把所有绘制完成的部分拼起来。&lt;/p&gt;

&lt;p&gt;把帧缓存（Frame Buffer）拆分成很多个小块，使得每个小块可以被GPU附近的SRAM容纳，块的多少取决于GPU硬件的SRAM大小。这样GPU就可以分批的一块块的在SRAM上读写帧缓存（Frame Buffer），一整块都读写完毕后，再整体转移回主存上。&lt;/p&gt;

&lt;p&gt;这种模式就叫做TBR（Tile-Based-Rendering），整体管线如下图：&lt;/p&gt;

&lt;p&gt;（TBR管线图：源自网络）&lt;/p&gt;

&lt;p&gt;屏幕分块后的大小一般为16x16或32x32像素 ，在几何阶段之后再执行分块(Tiling)，接着将各个块（Tile）逐个光栅化，最后写入帧缓存中（Frame Buffer）中 。&lt;/p&gt;

&lt;p&gt;这里有一些细节要注意，TBR在接受每个指令（CommandBuffer）时并不立即绘制，而是先对这些数据做顶点处理，把顶点处理的结果暂时保存在主存上，等到非得刷新整个帧缓存时，才真正的用这批数据做光栅化。&lt;/p&gt;

&lt;p&gt;因此，TBR的管线实际可以认为被切分成两部分，前半部分为顶点数据部分，后半部分为片元数据部分：&lt;/p&gt;

&lt;p&gt;（TBR把管线切分为光栅化前和光栅化后）&lt;/p&gt;

&lt;h3 id=&quot;顶点数据先被处理并存储在frame-data中等到必须刷新时例如帧缓存置换调用glflush调用glfinish调用glreadpixels读取帧缓存像素时调用glcopytexiamge拷贝贴图时调用glbitframebuffer获取帧缓存时调用queryingocclusion解绑帧缓存时等等才被集中的拿去处理光栅化&quot;&gt;顶点数据先被处理并存储在Frame Data中，等到必须刷新时（例如帧缓存置换，调用glflush，调用glfinish，调用glreadpixels读取帧缓存像素时，调用glcopytexiamge拷贝贴图时，调用glbitframebuffer获取帧缓存时，调用queryingocclusion，解绑帧缓存时等等）才被集中的拿去处理光栅化。&lt;/h3&gt;

&lt;h3 id=&quot;那么为什么pc不使用tbr呢&quot;&gt;那么为什么PC不使用TBR呢？&lt;/h3&gt;

&lt;p&gt;实际上直接对主存或显存（这里也有多级缓存）进行整块数据的读写速度是最快的，而TBR需要一块块的绘制，然后再回拷给主存。可以简单的认为TBR牺牲了执行效率，换来了相对更难解决的带宽功耗。如果哪一天手机上解决了带宽的功耗问题，或者说SRAM足够大了，可能就没有TBR了。&lt;/p&gt;

&lt;h2 id=&quot;tbdrtile-based-deferred-rendering管线&quot;&gt;TBDR（Tile-based deferred rendering）管线&lt;/h2&gt;

&lt;p&gt;TBR会把顶点数据处理完毕后存储在Frame Data中，那么就会有很多厂商针对Frame Data做优化。&lt;/p&gt;

&lt;p&gt;TBDR整体的管线图如下：&lt;/p&gt;

&lt;p&gt;（TBDR管线图：源自网络）&lt;/p&gt;

&lt;p&gt;我们看到相比TBR，TBDR在光栅化（Raster）后多了一个HSR（Hidden Surface Removal）处理，这部分处理主要剔除无需绘制的元素，减少重绘（Overdraw）数量（高通通过优化划分块（Tile）之后执行顶点着色器（Vertex Shader）之前的节点来达到此目的，称为LRZ）。例如提前对不透明像素做深度测试并剔除，剔除被模板裁剪掉的像素等等，总之它们不会进入到像素着色器阶段（Pixel Shader）。&lt;/p&gt;

&lt;p&gt;因此在TBDR上，不透明物体的排序没有太大意义，Early-Z这种策略也不存在IOS上。这些GPU硬件巧妙的利用TBR的Frame Data队列实现了一种延迟渲染，尽可能只渲染那些会最终影响帧缓存（Frame Buffer）的像素。&lt;/p&gt;

&lt;h3 id=&quot;tbdr和软件上的延迟渲染相比有什么区别呢&quot;&gt;TBDR和软件上的延迟渲染相比有什么区别呢？&lt;/h3&gt;

&lt;p&gt;软件层面的延迟渲染与TBDR不同。软件层面的延迟渲染是针对一个Drawcall，对于从后到前的不透明物体绘制是每次都要绘制的，而硬件层面的延迟渲染，处理的是一整批Drawcall，剔除这一整批Drawcall中不会绘制的像素最后再渲染。可以说现在大部分的移动端的GPU都使用TBDR架构。&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料：&lt;/h3&gt;

&lt;p&gt;《How Shader Cores Work》
https://engineering.purdue.edu/~smidkiff/KKU/files/GPUIntro.pdf&lt;/p&gt;

&lt;p&gt;《CPU体系结构》
https://my.oschina.net/fileoptions/blog/1633021&lt;/p&gt;

&lt;p&gt;《深入理解CPU的分支预测(Branch Prediction)模型》
https://zhuanlan.zhihu.com/p/22469702&lt;/p&gt;

&lt;p&gt;《分析Unity在移动设备的GPU内存机制（iOS篇）》
https://www.jianshu.com/p/68b41a8d0b37&lt;/p&gt;

&lt;p&gt;《PC与Mobile硬件架构对比》
https://www.cnblogs.com/kekec/p/14487050.html&lt;/p&gt;

&lt;p&gt;《针对移动端TBDR架构GPU特性的渲染优化》
https://gameinstitute.qq.com/community/detail/123220&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Tile-based rendering》
https://www.imaginationtech.com/blog/a-look-at-the-powervr-graphics-architecture-tile-based-rendering/&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Deferred rendering》
https://www.imaginationtech.com/blog/the-dr-in-tbdr-deferred-rendering-in-rogue/&lt;/p&gt;

&lt;p&gt;《深入GPU硬件架构及运行机制》
https://www.cnblogs.com/timlly/p/11471507.html&lt;/p&gt;

&lt;p&gt;《深入浅出计算机组成原理》
https://time.geekbang.org/column/article/105401?code=7VZ-Md9oM7vSBSE6JyOgcoQhDWTOd-bz5CY8xqGx234%3D&lt;/p&gt;

&lt;p&gt;《Nvidia Geforce RTX-series is born》
https://www.fudzilla.com/reviews/47224-nvidia-geforce-rtx-series-is-born?start=2&lt;/p&gt;

&lt;p&gt;《渲染管线与GPU（Shading前置知识）》
https://zhuanlan.zhihu.com/p/336999443&lt;/p&gt;

&lt;p&gt;《剖析虚幻渲染体系（12）- 移动端专题Part 1（UE移动端渲染分析）》
https://www.cnblogs.com/timlly/p/15511402.html&lt;/p&gt;

&lt;p&gt;《tpc-texture-processing-cluster》
https://gputoaster.wordpress.com/2010/12/11/tpc-texture-processing-cluster/&lt;/p&gt;

&lt;p&gt;《Life of a triangle - NVIDIA’s logical pipeline》
https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline&lt;/p&gt;

&lt;p&gt;《Rasterisation wiki》
https://en.wikipedia.org/wiki/Rasterisation&lt;/p&gt;

&lt;p&gt;《PolyMorph engine and Data Caches by Hilbert Hagedoorn》
https://www.guru3d.com/articles-pages/nvidia-gf100-(fermi)-technology-preview,3.html&lt;/p&gt;

&lt;p&gt;《NVIDIA GPU的一些解析》
https://zhuanlan.zhihu.com/p/258196004&lt;/p&gt;

&lt;p&gt;《tensor-core-performance-the-ultimate-guide》
https://developer.download.nvidia.cn/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf&lt;/p&gt;

&lt;p&gt;《Understanding the Understanding the graphics pipeline》
https://www.seas.upenn.edu/~cis565/LECTURES/Lecture2%20New.pdf&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247485096&amp;amp;idx=1&amp;amp;sn=c027b6af4677515ba1e5950aac8ba68d&amp;amp;chksm=fc2263afcb55eab9204898b39fb77ee04f5767ac450507b6192a5c6d5612a45b6d29b1bcee16&amp;amp;token=853743270&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Sun, 12 Dec 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/12/12/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B051</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/12/12/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B051</guid>
            </item>
        
            <item>
               <title>读书笔记(五十) 《如何精彩演讲》#2 克服演讲时的恐惧</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247485068&amp;amp;idx=1&amp;amp;sn=464d36573e97c69aeab4ab996885ccee&amp;amp;chksm=fc22638bcb55ea9d3e4472e92a760b147dc2a816268e9d3a435842995723d3a02e9a26b0f4b5&amp;amp;token=1416441933&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;

&lt;p&gt;不知不觉看完了6本关于演讲的主题，按推荐顺序排列分别是《高效演讲》、《演讲与口才》、《即兴演讲》、《关键对话》、《TED演讲的力量》、《说话的艺术》。&lt;/p&gt;

&lt;p&gt;我希望自己能够对演讲有一个系统性的学习。于是自己在平常的生活和工作中实践了一段时间，前前后后加起来有两个月时间。&lt;/p&gt;

&lt;p&gt;目标是，改善平常的工作、生活中表达的流畅和清晰度，能让人感觉到舒服让人感觉有活力。同时在公众演讲上，希望自己能够很好的呈现自己的所思所想以及所知所学。&lt;/p&gt;

&lt;p&gt;因此开启这个演讲学习系列的总结文章，我将对这6本书的内容加上我的实践经验做一次全面的系统性的总结。&lt;/p&gt;

&lt;h3 id=&quot;概述&quot;&gt;概述：&lt;/h3&gt;

&lt;p&gt;1.恐惧的原因
2.克服恐惧的原理
3.制定克服恐惧的行动&lt;/p&gt;

&lt;h3 id=&quot;内容&quot;&gt;内容：&lt;/h3&gt;

&lt;p&gt;为什么要了解演讲时的恐惧？&lt;/p&gt;

&lt;p&gt;1.只有了解恐惧的原理才能正确认识它
2.它会使得事情更加糟糕
3.它会对我们的心理产生长短期的变化&lt;/p&gt;

&lt;p&gt;在平时的生活工作学习当中，当我们恐惧时很多时候并不能了解自己的恐惧情绪。由于在很多情况下，我们观察不到自己的情绪，导致我们常常无法在情绪有问题时，及时的调整它。&lt;/p&gt;

&lt;p&gt;因此你会看到很多时候，当出现了恐惧的症状时，我们没有调整它，这直接导致事情发展变得糟糕。&lt;/p&gt;

&lt;p&gt;（恐惧表情图：来源网络）&lt;/p&gt;

&lt;p&gt;不止如此，如果没有去很好的应对恐惧的话，长此以往我们在心里会有比较大的阴影。这使得，我们在一件比较重要的会议上，由于你的恐惧而退缩或退却，会使你在精神上遭受更大的打击。&lt;/p&gt;

&lt;p&gt;如果次数多了，你会发现你对这些重要的场所，重要的机会，会产生习惯性的恐惧，这常常使你对人生产生比较多的怀疑。&lt;/p&gt;

&lt;p&gt;因此无论从长期或短期来看，我们都需要去主动的了解恐惧。这是我们必须去了解恐惧的重要原因。它会将生活弄得更加糟糕，因此我们必须了解他，特别是在演讲时。&lt;/p&gt;

&lt;p&gt;恐惧有很多种，这里我们只关注演讲时的恐惧。但其实演讲的恐惧已经覆盖了生活中很多的方面，它与生活中的恐惧有很多共性。&lt;/p&gt;

&lt;h2 id=&quot;是什么让你恐惧&quot;&gt;是什么让你恐惧？&lt;/h2&gt;

&lt;p&gt;了解恐惧产生的原因，可以让我们对恐惧问题的形成有更深刻的认识。&lt;/p&gt;

&lt;p&gt;不管是普通焦虑还是惊恐障碍，都没有单一的成因，也不可能在消除这个成因后问题就得到彻底根除。
焦虑问题是有多方面，多种原因引起的，包括遗传因素，生物因素，家庭背景和教养方式、心理条件作用、近期生活转变、自我对话和个人信念体系、表达情感的能力、当前环境压力等。&lt;/p&gt;

&lt;p&gt;恐惧的原因：
1.面对重要场面，准备不充分
2.对周围的环境很陌生
3.经历不足，导致的对事情没有底气
4.在意别人对自己的看法
5.生理反应，肌肉紧张、心跳加速、头脑晕眩
6.近期自己身边的环境变动较大
7.长期积累的心里因素，过分在意过去发生的事情&lt;/p&gt;

&lt;p&gt;我们在面对演讲时通常会有很多的恐惧，特别是在上台前一段时间，自己的恐惧心理被放大的特别多。&lt;/p&gt;

&lt;p&gt;其实说来不只是演讲，恐惧在我们平常的生活跟工作当中也非常多，我们心里常常会对某件即将到来的事产生恐惧。&lt;/p&gt;

&lt;p&gt;因此呢。并不是说在演讲时你才会恐惧，在平常的生活当中，你同样会有很多令你焦虑不安的事情。&lt;/p&gt;

&lt;p&gt;演讲跟平常我们生活、工作时产生的恐惧是一样的，并没有说因为演讲而特殊。&lt;/p&gt;

&lt;p&gt;我们究竟为什么而恐惧？我们来分析一下。&lt;/p&gt;

&lt;p&gt;前面列了7条恐惧的原因，但其实这7条可以归类一下变得更精简一些。&lt;/p&gt;

&lt;p&gt;（恐惧原因）&lt;/p&gt;

&lt;p&gt;恐惧的原因总的来说为三方面：
1.准备不充分
2.环境变化导致的恐惧
3.长期积累的心理因素&lt;/p&gt;

&lt;p&gt;（恐惧原因细节图）&lt;/p&gt;

&lt;h3 id=&quot;第一准备不充分&quot;&gt;第一，准备不充分&lt;/h3&gt;

&lt;p&gt;当我们知道我们将要去面对一件事情，但是这件事情并没有充足的把握时我们就会感到恐惧。&lt;/p&gt;

&lt;p&gt;因为这件事情是未知的，而且我预感到他可能不会有太好的结果，因为我对他没有把握，我也没有对他做任何的准备工作。&lt;/p&gt;

&lt;p&gt;可能有这么一件事情你是从来没有做过的，或者说你也不知道他将会发生什么样的情况。甚至在你的预感当中，这件事会发生比较差的或者糟糕的情况，此时你就会对这件事情产生恐惧。因而有逃避去面对这件事情的心理状态。&lt;/p&gt;

&lt;p&gt;在演讲时也是同样的，大部分人没有台上演讲的经验和习惯，因此对于上台演讲这件事情，大部分人都是有恐惧心理的。&lt;/p&gt;

&lt;p&gt;这个恐惧心理是因为我们没有，为这件事情做过任何的准备，也不知道这件事情会发生怎样的情况。此时我们心里隐隐的感觉到这这件事情会比较糟糕。因此，我们对演讲产生了恐惧。&lt;/p&gt;

&lt;p&gt;总之，当你面对一件即将到来，但未知的事情时，当你面对一件你觉得可能会发生比较糟糕的事情时，通常是因为我们没有做好足够的准备工作。这个准备工作可能需要花去几小时、几天、几个月甚至几年的时间，总之我们并没有为这件事做好足够的准备。&lt;/p&gt;

&lt;h3 id=&quot;第二环境变化导致的恐惧&quot;&gt;第二，环境变化导致的恐惧&lt;/h3&gt;

&lt;p&gt;我们周围的环境时常在变化，包括我们的工作，上下级人员调动，以及你身边人的来往，以及你亲人的离去与归来，这些的环境都在时刻变化着，影响着我们的心理。
可能是你最近比较倒霉，碰到了一些小概率事件，或者可能会因为大环境的变化使得感到沮丧。
甚至有可能你出差到了某个陌生的环境，让你感到很不舒服。&lt;/p&gt;

&lt;p&gt;这些环境的变化都会让你产生焦虑甚至恐惧的心理。&lt;/p&gt;

&lt;p&gt;总之，在当前这样的环境下，是你并不熟悉的环境，不是你能掌控而环境，因此导致你对周围的环境有一种隔离感，像是没有依靠的感觉。&lt;/p&gt;

&lt;h3 id=&quot;第三长期积累的心理因素&quot;&gt;第三，长期积累的心理因素&lt;/h3&gt;

&lt;p&gt;你可能会因为曾经的遭遇而感到痛苦，这些遭遇在你的脑中挥之不去，当再次发生或者即将发生这样的事情的时候，你就会感到焦虑和恐惧。&lt;/p&gt;

&lt;p&gt;这些痛苦的场景通常会延伸到我们生活的各个角落，比如说，你可能曾经在小组发言时，磕磕巴巴导致你对公众发言产生了恐惧，致使你在现在上台演讲时都会表现的异常的惊恐，当回忆起自己以前在小组发言时的那些场景，以及当时人们都的表情，你会不自觉的把这些场景延伸到现在的公众演讲以及公众表达上。&lt;/p&gt;

&lt;p&gt;常见比如，人们小时候玩水时掉入水中呛到过，成人后就对游泳产生了恐惧。或者在爬树时掉下来过，就对过山车产生了恐惧等等。&lt;/p&gt;

&lt;p&gt;不仅如此，人们通常会将一种痛苦的经历延展到另一个场景中，比如说，某人会因为小学里遭到过坏学生的殴打，致使他觉得现在自己面对他人安排的工作任务时会感觉自己是被压迫的或受虐待的，因此有很大的抵触情绪。&lt;/p&gt;

&lt;p&gt;总之，人们常常会延展过去的痛苦回忆，将这些痛苦的回忆放入到现在的场景里面去，这使得他们感到恐惧，同时也使他们有一个最佳的逃避借口。&lt;/p&gt;

&lt;h2 id=&quot;如何行动去克服恐惧&quot;&gt;如何行动去克服恐惧？&lt;/h2&gt;

&lt;p&gt;我罗列了所有的方法，有些方法是我自己总结的，有些则是参考《应对焦虑》埃德蒙.伯恩写的这本书。&lt;/p&gt;

&lt;p&gt;（克服恐惧的方法全图）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;观察自己的恐惧&lt;/li&gt;
  &lt;li&gt;提前做好准备&lt;/li&gt;
  &lt;li&gt;积极变化适应环境&lt;/li&gt;
  &lt;li&gt;冥想练习排除杂念&lt;/li&gt;
  &lt;li&gt;渐进式肌肉放松&lt;/li&gt;
  &lt;li&gt;暴露疗法，应对暴露，完全暴露，想象暴露&lt;/li&gt;
  &lt;li&gt;运动，增加运动频率，增加强度增加抗压力&lt;/li&gt;
  &lt;li&gt;自我关怀，爱的言语、规律作息、简化生活、&lt;/li&gt;
  &lt;li&gt;转移注意力，专注于技巧，找人聊天，体验愉悦的事，听音乐&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上这些技巧，可以用在我们生活当中面对焦虑与恐惧时的情况。&lt;/p&gt;

&lt;p&gt;虽然大部分技巧都可以用到演讲中，但我想讲其中在演讲时运用最有效的方法，我们来看下图：&lt;/p&gt;

&lt;p&gt;（克服演讲恐惧的有效方法）&lt;/p&gt;

&lt;h3 id=&quot;观察自己的恐惧&quot;&gt;观察自己的恐惧&lt;/h3&gt;

&lt;p&gt;我们最先要做的是观察自己的恐惧，这是所有技巧的前提。&lt;/p&gt;

&lt;p&gt;（观察恐惧图：来源网络）&lt;/p&gt;

&lt;p&gt;恐惧也是情绪的一种，我们需要观察恐惧发生时候我们的表现，我们的眼神，我们心跳，我们的肌肉，在做怎样的颤抖。通过观察我们身体上的变化，我们可以感知到自己的恐惧。当我们能够观察到恐惧时，恐惧本身就没有这么强烈了，所以我们首先要做的是，用第三者的角度观察自己的恐惧表现。&lt;/p&gt;

&lt;p&gt;通过观察我们能及时的发现我们的精神状态在变化，只要我们观察到恐惧，它就不会再扩散，此时我们就可以想办法逐渐的释放它。&lt;/p&gt;

&lt;p&gt;最糟糕的是你观察不到自己的恐惧，但仍然继续逼迫自己向前对劲，这时你会有一种无力感，同时恐惧会不断的蔓延到你的全身，使你的精神状态陷入更加崩溃的边缘。&lt;/p&gt;

&lt;p&gt;我们只有识别自己的恐惧状态，识别自己的恐惧表现，才能真正的认识到自己的情绪变化，以及事物对我们的影响程度。这为后面的措施，调整打下了好的基础。&lt;/p&gt;

&lt;h3 id=&quot;提前做好准备&quot;&gt;提前做好准备&lt;/h3&gt;

&lt;p&gt;提前做好准备，通常是最明智的选择，也是克服恐惧的最好方法。&lt;/p&gt;

&lt;p&gt;（提前准备图：来源网络）&lt;/p&gt;

&lt;p&gt;它需要我们抽出时间去为这场演讲做准备。&lt;/p&gt;

&lt;p&gt;我需要写演讲稿，并且在演讲前打磨多次。通过自己不断的练习和打磨演讲，将演讲稿打磨成自己认为完美的样子。&lt;/p&gt;

&lt;p&gt;同时，在我们平时的生活中，要为未来的可能的演讲做好准备。所以，在我们平常的生活当中，要注重收集生活的细节，收集演讲的素材，这些素材能够更快更有效的运用在未来的演讲中，让演讲和故事呈现的效果更好。&lt;/p&gt;

&lt;p&gt;因此，提前做好准备，是每个演讲达人的最重要的工作之一。&lt;/p&gt;

&lt;h3 id=&quot;积极拥抱变化适应环境&quot;&gt;积极拥抱变化适应环境&lt;/h3&gt;

&lt;p&gt;（拥抱变化：来源网络）&lt;/p&gt;

&lt;p&gt;很多时候我们避免不了会去自己陌生的地方演讲。这个时候，环境的变化对我们的精神冲击力很大，周围没有熟悉的环境没有熟悉的人，我们感到被隔离脱离群体从而感到恐惧。&lt;/p&gt;

&lt;p&gt;这个时候，我们需要积极的去拥抱变化。与台上台下的工作人员沟通，积极的与嘉宾沟通，与在场的观众沟通。通过这样沟通交流我们能够舒展我们的心情，能够释放我们的紧张情绪，能够释放我们的恐惧情绪。&lt;/p&gt;

&lt;p&gt;如果在现场当我们遇到困难的时候，需要主动寻求帮助。特别是当下环境中的管理者，或者比较熟悉的朋友，通过他们的帮助让自己这份不安的心能够安定下来，减少恐惧的蔓延。&lt;/p&gt;

&lt;p&gt;特别是在演讲开场时，自己在一个陌生的环境下非常容易紧张焦虑和恐惧，此时在开场时需要跟大家有一个沟通交流的时间，留出这样一个前置的时间跟大家沟通交流非常必要，然后再慢慢进入状态，开始自己的演讲。也可以通过说出自己的感受来跟大家做一些互动，这样让自己的紧张情绪和压力能够释放掉，这为后面的事情进展顺利而铺平了道路。&lt;/p&gt;

&lt;h3 id=&quot;放松练习&quot;&gt;放松练习&lt;/h3&gt;

&lt;p&gt;我们可以用身体上的练习和精神的上的练习来放松自己。&lt;/p&gt;

&lt;p&gt;（冥想图：来源网络）&lt;/p&gt;

&lt;p&gt;精神上的练习，可以通过说出感受来接纳自己释放压力。也可以通过就地坐下或稳定站立并闭上眼睛来做冥想，通过专注于自己的呼吸，将杂念排除，也可以通过腹式呼吸来将注意力转移到自己的腹部上，让自己更加专注于当下。&lt;/p&gt;

&lt;p&gt;冥想的要点是专注于呼吸，这会让你排除杂念，专注于当下。&lt;/p&gt;

&lt;p&gt;身体上的练习，也可以起到放松作用。&lt;/p&gt;

&lt;p&gt;（与文无关的放松练习图：来源网络）&lt;/p&gt;

&lt;p&gt;这里有一个叫渐进式肌肉放松法。我们来具体介绍一下：&lt;/p&gt;

&lt;p&gt;腹式呼吸，用腹部呼吸，吸气时肚子吸入空气突出呼气时因气体排除而扁平，呼吸要慢，想象全身的紧张感开始从体内流去。&lt;/p&gt;

&lt;p&gt;撰紧拳头，保持7-10秒，再从开拳头15-20秒，反复循环&lt;/p&gt;

&lt;p&gt;双手前臂抬起，前臂与上臂尽量靠拢，紧绷肱二头肌，保持，然后放松。&lt;/p&gt;

&lt;p&gt;双手手臂向外延展到水平位置，伸肘，拉紧肱三头肌，保持，然后放松。&lt;/p&gt;

&lt;p&gt;尽量抬高眉毛，收缩前额肌肉，保持，然后放松。放松时，想象前额肌肉慢慢舒展、松弛。&lt;/p&gt;

&lt;p&gt;紧闭双眼，绷紧眼周肌肉，保持，然后放松。想象深度放松的感觉在眼镜周围蔓延。&lt;/p&gt;

&lt;p&gt;张大嘴巴，拉伸下颚关节周围的肌肉，绷紧下巴，保持，然后放松。张着嘴，让下巴自然放松。&lt;/p&gt;

&lt;p&gt;头向后仰，尽量靠向后背，收紧脖子后面的肌肉，专注于收紧颈部肌肉的动作，保持，然后放松。&lt;/p&gt;

&lt;p&gt;双肩同时最大限度地向上耸起，绷紧肩部肌肉，保持，然后放松。&lt;/p&gt;

&lt;p&gt;双肩外展，尽量向背部中线靠拢，绷紧肩胛骨周围的肌肉。让肩胛处的肌肉保持绷紧，然后放松。&lt;/p&gt;

&lt;p&gt;深吸一口气，绷紧胸部肌肉，保持10秒，然后慢慢呼气。&lt;/p&gt;

&lt;p&gt;收紧腹部肌肉，保持，然后放松。&lt;/p&gt;

&lt;p&gt;背部弓起，拉紧下背部肌肉，保持，然后放松。&lt;/p&gt;

&lt;p&gt;收紧臀部，保持，然后放松。&lt;/p&gt;

&lt;p&gt;收缩大腿肌肉，保持，然后放松。&lt;/p&gt;

&lt;p&gt;向自己的方向用力伸脚趾，绷紧小腿肌肉，保持，然后放松。&lt;/p&gt;

&lt;p&gt;卷起脚趾，绷紧脚面，保持，然后放松。&lt;/p&gt;

&lt;p&gt;以上是一整套的渐进式肌肉放松法，一整套下来需要20-30分钟，在平时的放松练习可以通过抽取几个对自己比较有效的姿势去做放松练习，这样更容易将这些放松练习融入到我们的生活中，特别是演讲前的放松练习。&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料：&lt;/h3&gt;

&lt;p&gt;《三招搞定演讲构思》朱林滢&lt;/p&gt;

&lt;p&gt;《应对焦虑》埃德蒙.伯恩&lt;/p&gt;

&lt;p&gt;《高效演讲》彼得.迈尔斯，尚恩.尼克斯&lt;/p&gt;
</description>
               <pubDate>Sun, 05 Dec 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/12/05/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B049</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/12/05/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B049</guid>
            </item>
        
            <item>
               <title>读书笔记(四十九) 《游戏引擎架构》#4 低阶渲染器（3）</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247485051&amp;amp;idx=1&amp;amp;sn=e98f1e1971d56ce6543926b4f9bad204&amp;amp;chksm=fc22637ccb55ea6a4c16c5c0d8fd43cff936350490d79f1e377b61e4914300c39c4fc2911da1&amp;amp;token=53415989&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景：&lt;/h1&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了几遍《游戏引擎架构》后对引擎架构的理解又深入了些。&lt;/p&gt;

&lt;p&gt;近段时间有对引擎剖析的想法，正好借这书本对游戏引擎架构做一个完整分析。&lt;/p&gt;

&lt;p&gt;此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节。&lt;/p&gt;

&lt;p&gt;借助《游戏引擎架构》这本书、结合引擎源码和自己的经验，深入分析游戏引擎的历史、架构、模块，最后通过实践简单引擎开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;同时《游戏引擎架构》中部分知识太过陈旧的部分，会重新深挖后总结出自己的观点。&lt;/p&gt;

&lt;p&gt;概述：&lt;/p&gt;

&lt;p&gt;本系列文章对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;时间库&lt;/li&gt;
  &lt;li&gt;自定义容器库&lt;/li&gt;
  &lt;li&gt;字符串散列库&lt;/li&gt;
  &lt;li&gt;内存管理框架&lt;/li&gt;
  &lt;li&gt;RTTI与反射模块&lt;/li&gt;
  &lt;li&gt;图形计算库&lt;/li&gt;
  &lt;li&gt;资产管理模块&lt;/li&gt;
  &lt;li&gt;低阶渲染器&lt;/li&gt;
  &lt;li&gt;剔除与合批模块&lt;/li&gt;
  &lt;li&gt;动画模块&lt;/li&gt;
  &lt;li&gt;物理模块&lt;/li&gt;
  &lt;li&gt;UI底层框架&lt;/li&gt;
  &lt;li&gt;性能剖析器的核心部分&lt;/li&gt;
  &lt;li&gt;脚本系统&lt;/li&gt;
  &lt;li&gt;视觉效果模块&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;本篇内容为列表中的第8个部分的第1节。&lt;/p&gt;

&lt;h1 id=&quot;正文&quot;&gt;正文：&lt;/h1&gt;

&lt;p&gt;简单回顾下前文&lt;/p&gt;

&lt;p&gt;前文我们聊了下显卡在计算机硬件主板中的位置与结构，知道了CPU、GPU的通信介质，并简单介绍了手机上的主板结构。本篇开头对上一篇做一些内容补充，PC和手机的不同硬件组织，以及CPU与其他芯片的通信过程。&lt;/p&gt;

&lt;p&gt;下面我们开始这篇内容&lt;/p&gt;

&lt;p&gt;本次内容会围绕GPU来写，从硬件架构到软件驱动再到引擎架构，目标是帮大家理解GPU硬件的运作原理，理解图形接口的架构，理解引擎低阶渲染器的架构。&lt;/p&gt;

&lt;p&gt;目录：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;主板结构中的显卡&lt;/li&gt;
  &lt;li&gt;GPU功能发展史&lt;/li&gt;
  &lt;li&gt;GPU与CPU的差异&lt;/li&gt;
  &lt;li&gt;GPU硬件特点&lt;/li&gt;
  &lt;li&gt;图形驱动程序架构&lt;/li&gt;
  &lt;li&gt;引擎低阶渲染架构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;内容结构&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CPU硬件结构&lt;/li&gt;
  &lt;li&gt;GPU硬件结构&lt;/li&gt;
  &lt;li&gt;GPU手机管线与PC管线的差异&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;简单回顾下前文，前文我们主要讲了显卡的发展历史，知道了显卡功能和管线是如何一步步转变为现在这样子的。&lt;/p&gt;

&lt;h2 id=&quot;cpu结构与工作原理&quot;&gt;CPU结构与工作原理&lt;/h2&gt;

&lt;p&gt;我们知道，CPU运行时有三类元器件构成，取指器、译码器、运算器（逻辑算术运算器、浮点数运算器、单指令多数据运算器等）。&lt;/p&gt;

&lt;p&gt;这三类元器件代表三个阶段取指阶段（Fetch）、指令译码阶段（Decode）、执行阶段（Execute），它们在CPU内执行的步骤如下图：&lt;/p&gt;

&lt;p&gt;（图-取指-译指-执行三步骤）&lt;/p&gt;

&lt;p&gt;取指阶段为从内存或缓存中取得指令并存放到寄存器中的过程。&lt;/p&gt;

&lt;p&gt;接着，译码器会将寄存器中的指令翻译成操作指令，指令译码器按照预定的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别以及各种获取操作数的方法。在组合逻辑控制的计算机中，指令译码器对不同的指令操作码产生不同的控制电位，以形成不同的微操作序列；在微程序控制的计算机中，指令译码器用指令操作码来找到执行该指令的微程序的入口，并从此入口开始执行。&lt;/p&gt;

&lt;p&gt;运算阶段，则根据指令执行不同的运算单元，完成指令所规定的各种操作，具体实现指令的功能。为此，CPU 的不同部分被连接起来，以执行所需的操作。&lt;/p&gt;

&lt;p&gt;（图-控制单元-运算单元-存储单元）&lt;/p&gt;

&lt;p&gt;因此，通常我们将取指器、译码器统称为控制单元，计算器称为算术逻辑单元（ALU），寄存器和高速缓存称为存储单元。&lt;/p&gt;

&lt;p&gt;除了这三个基本单元，当下这样复杂的CPU中还有包括分支预测器、乱序控制器、内存预加载器等等。&lt;/p&gt;

&lt;p&gt;这里简单介绍下CPU指令流水线、分支预测、乱序执行的原理。&lt;/p&gt;

&lt;h2 id=&quot;指令流水线&quot;&gt;指令流水线&lt;/h2&gt;

&lt;p&gt;起初CPU指令执行是线性的，只靠取指、译码、运算顺序执行三个模块，这导致元器件的工作顺序是线性的，当一个元器件执行时，其他元器件是空等待状态，CPU执行效率比较低。&lt;/p&gt;

&lt;p&gt;为了提高效率，提高空等待的元器件的利用率，对指令执行流水线进行了拆分，并同时增加多个流水线不断减少元器件的空等待装填。如下图：&lt;/p&gt;

&lt;p&gt;（拆分多级流水线）&lt;/p&gt;

&lt;p&gt;将原本线性的三个指令执行顺序，拆分成一个个小模块，让这些独立的小模块可以自顾自的循环工作，减少前后的等待时间，从而提高了指令执行效率。&lt;/p&gt;

&lt;p&gt;用这种方式把一个指令拆分成“取指令 - 指令译码 - 执行指令”这样三个部分，这就是一个三级的流水线。进一步把“执行指令”拆分成“ALU 计算（指令执行）- 内存访问 - 数据写回”，就会变成一个五级的流水线。&lt;/p&gt;

&lt;p&gt;继续拆分，将一个长时间的操作步骤，拆分成更多的步骤，让所有步骤需要执行的时间尽量都差不多长。这样，也就可以解决我们在单指令周期处理器中遇到的复杂指令性能问题。（同时多级流水线会出现许多问题，例如模块间寄存器的写入次数太多，多模块读写同一个资源相互冲突等，这里不细说，CPU最终都有解决方案去解决）现代的 ARM 或 Intel 的 CPU，流水线级数都已经到了 14 级。&lt;/p&gt;

&lt;h2 id=&quot;乱序执行&quot;&gt;乱序执行&lt;/h2&gt;

&lt;p&gt;我们在写程序时，常常会发现函数内几个执行操作顺序并不互相依赖，哪个在前哪个在后都没有太大的关系。此时不仅编译器会对这些不相互依赖的计算操作进行重新顺序排序用于优化CPU执行效率（这也是导致线程不同步的其中一个原因），CPU也会将不相互依赖的指令放在不相同的指令流水线上以加快执行速度。&lt;/p&gt;

&lt;p&gt;（指令在不同CPU流水线上乱序执行：图来源网络）&lt;/p&gt;

&lt;p&gt;我们从图中可以看到，在流水线里，当后面的指令不依赖前面的指令时，就不用等待前面的指令执行完毕后再执行，可以另起一个流水线执行，否则就需要用NOP隔周期等待的方式将执行单元延后计算。因此我们所写的代码的执行顺序其实并不是我们所想象的那样，在CPU中大部分时候其实是乱序执行的，这样元器件的利用率更高，执行效率也更高，而依赖而停顿的次数也更少。&lt;/p&gt;

&lt;p&gt;（乱序执行的流程图）&lt;/p&gt;

&lt;p&gt;乱序执行实际的过程比我们想象的要复杂一些，总体上它会先拆分指令，再分发给执行单元，结束后将结果重新排序，最后提交缓冲。&lt;/p&gt;

&lt;h2 id=&quot;分支预测&quot;&gt;分支预测&lt;/h2&gt;

&lt;p&gt;程序中有很多true或false的判断来跳转下文要执行的指令，这种跳转会使得执行流水线发生停顿，因为要依赖前面代码计算的结果再决定要执行哪段程序，因此流水线不中断并等待结果，这会使CPU执行效率降低。&lt;/p&gt;

&lt;p&gt;在CPU中有分支预测器，它是一种数字电路，在分支指令执行前，猜测哪一个分支会被执行，这样能显著提高pipeline的性能。&lt;/p&gt;

&lt;p&gt;可以理解为，分支预测器会主动猜测分支是true还是false。&lt;/p&gt;

&lt;p&gt;如果猜错了，处理器要flush掉pipeline, 回滚到之前的分支，然后重新热启动，选择另一条路径。 
如果猜对了，处理器不需要暂停，继续往下执行。&lt;/p&gt;

&lt;p&gt;也就是说，如果CPU每次都猜错，处理器将耗费大量时间在停止-回滚-热启动这一周期性过程里。反之，如果侥幸每次都猜对了，那么处理器将从不停止、无需等待的执行后面的指令。&lt;/p&gt;

&lt;p&gt;（分支预测图）&lt;/p&gt;

&lt;p&gt;CPU执行指令遇到条件时不知道该读取哪些指令，需要等待判断条件中的计算结果，这样就中断了后面指令执行流水线使得执行效率下降。于是CPU增加了分支预测器，猜if条件中是True还是False，如果猜对了效率就会提高，如果猜错了，则重新计算。&lt;/p&gt;

&lt;p&gt;分支预测的关键是，预测算法能猜对多少。&lt;/p&gt;

&lt;p&gt;分支预测分为动态分支预测和静态分支预测。动态预测在执行过程中统计了通过率，根据通过率去调整预测方向，静态则始终以一个值作为判断标准。动态预测有好几种，最常见的是双模预测，通过四个状态位来动态调整预测结果。其它常见分支预测器如两级自适应预测器，局部/全局分支预测器，融合分支预测器，Agree预测期，神经分支预测器等。&lt;/p&gt;

&lt;h2 id=&quot;cpu原理小结&quot;&gt;CPU原理小结&lt;/h2&gt;

&lt;p&gt;（CPU抽象元件图）&lt;/p&gt;

&lt;p&gt;现在我们知道了CPU指令周期的工作方式，分为三个步骤，取指、译码、运算。运算后需要寄存器和高速缓存来作为存储器，CPU会从内存中获取指令并最终将数据写入内存。&lt;/p&gt;

&lt;p&gt;我们把CPU中的元件抽象成，取指和译码元件、逻辑运算元件、数据缓存，就有了上面这幅简单抽象的CPU结构图。&lt;/p&gt;

&lt;p&gt;下面我们来看看硬件上的元器件是如何分布的：&lt;/p&gt;

&lt;p&gt;（CPU硬件结构图：来源网络）&lt;/p&gt;

&lt;p&gt;我们看到CPU除了基本的控制器、运算器、寄存器、高速缓存外，还额外放置了乱序执行器、分支预测器、内存预装载器等用于提高CPU效率。这些元器件全部加起来，整个就是一个CPU Core。&lt;/p&gt;

&lt;p&gt;（多核架构图：来源网络）&lt;/p&gt;

&lt;p&gt;实际的设备中通常由多个CPU Core组成多核的架构，每个CPU Core都有自己的高速缓存L1，不同CPU Core之间也有共享的高速缓存L2，通常每级缓存的存取速度有10倍的差距，而内存的存取速度比高速缓存差的更多，对于CPU Core来说可以认为它是一个外部存储设备，通过桥接芯片连接。&lt;/p&gt;

&lt;p&gt;GPU硬件结构与原理&lt;/p&gt;

&lt;p&gt;前面介绍了CPU的内部结构，现代无论是手机还是PC机基本都是多核的，每个核就是1个CPU Core，每个CPU Core里都有取指器和译码器，还有逻辑运算器，以及寄存器和高速缓存。除了上述基本元件外还有其他元器件用于优化CPU执行效率，包括乱序执行器、分支预测器、内存预装载器等。&lt;/p&gt;

&lt;p&gt;GPU图形管线的变迁&lt;/p&gt;

&lt;p&gt;我们从GPU历史里知道，原本显卡只是一个数据传输和画面转换接口，在不断的变革下成了主板上一个独立的芯片，之后就有了GPU的概念。CPU将数据传输到显存再通知GPU处理这些数据，GPU则拥有图形图像的处理流水线，专门处理图像。&lt;/p&gt;

&lt;p&gt;起初图形的顶点、片元都在CPU上计算，到了Voodoo FX显卡时已经将图元生成后的步骤拆分到了GPU上，最后再将顶点处理部分的计算合入到GPU上，此时GPU才真正形成了自己的图形管线。如下图：&lt;/p&gt;

&lt;p&gt;（图形计算管线变迁1-1982年前的纯2D时代）&lt;/p&gt;

&lt;p&gt;1982年前，CPU承担大部分的工作，当时还没有GPU的概念，还只能以显示适配器的名称称呼。&lt;/p&gt;

&lt;p&gt;（图形计算管线变迁2-1996年3dfx Voodoo）&lt;/p&gt;

&lt;p&gt;到1996年，GPU已经可以分担CPU的部分功能，只留下顶点处理部分部分给CPU。&lt;/p&gt;

&lt;p&gt;（图形计算管线变迁3-1998年GeForce）&lt;/p&gt;

&lt;p&gt;到1998年，所有顶点处理和片元处理都由GPU来完成了，但没有可编程部分，管线是固定的，传入顶点后无法控制顶点和片元的变化。&lt;/p&gt;

&lt;p&gt;（图形计算管线变迁4-2002年GeForce FX）&lt;/p&gt;

&lt;p&gt;到2002年，正式加入了可编程着色器，让顶点和片元的计算和展示有了更多变化。&lt;/p&gt;

&lt;p&gt;（图形计算管线变迁5-2006年GeForce 8800）&lt;/p&gt;

&lt;p&gt;到2006年，GPU管线中又增加了细分着色器，pre-Z等节点。&lt;/p&gt;

&lt;p&gt;这部分历史我们也可以通过OpenGL的功能变化来看这段历史的发展过程。&lt;/p&gt;

&lt;p&gt;（来源 wiki）&lt;/p&gt;

&lt;p&gt;1.1 1997 年 3 月，纹理对象，顶点数组&lt;/p&gt;

&lt;p&gt;1.2 1998 年 3 月，3D 纹理、BGRA 和打包像素格式&lt;/p&gt;

&lt;p&gt;1.2.1 1998年10月，ARB 扩展概念&lt;/p&gt;

&lt;p&gt;1.3 2001 年 8 月，多重纹理、多重采样、纹理压缩&lt;/p&gt;

&lt;p&gt;1.4 2002 年 7 月，深度图，GLSlang&lt;/p&gt;

&lt;p&gt;1.5 2003 年 7 月，顶点缓冲对象 (VBO)，遮挡查询&lt;/p&gt;

&lt;p&gt;2.0 2004 年 9 月， GLSL 1.1，MRT，两个纹理的非幂，点精灵，双面模板&lt;/p&gt;

&lt;p&gt;2.1 2006 年 7 月 ，GLSL 1.2，像素缓冲对象 (PBO)，sRGB 纹理&lt;/p&gt;

&lt;p&gt;3.0 2008 年 8 月 ，GLSL 1.3，纹理数组，条件渲染，帧缓冲对象 (FBO)&lt;/p&gt;

&lt;p&gt;3.1 2009 年 3 月， GLSL 1.4，Instancing，纹理缓存对象，统一缓存对象，图元重启&lt;/p&gt;

&lt;p&gt;3.2 2009 年 8 月， GLSL 1.5，几何着色器，多重采样纹理&lt;/p&gt;

&lt;p&gt;3.3 2010 年 3 月， GLSL 3.30，从 OpenGL 4.0 规范向后移植尽可能多的功能&lt;/p&gt;

&lt;p&gt;4.0 2010 年 3 月， GLSL 4.00，GPU 上的曲面细分，具有 64 位精度的着色器&lt;/p&gt;

&lt;p&gt;4.1 2010 年 7 月， GLSL 4.10，开发人员友好的调试输出，与 OpenGL ES 2.0 的兼容性&lt;/p&gt;

&lt;p&gt;4.2 2011 年 8 月， GLSL 4.20，带原子计数器的着色器，绘制传输给Feed back实例，着色器打包，性能改进&lt;/p&gt;

&lt;p&gt;4.3 2012 年 8 月， GLSL 4.30，利用 GPU 并行性的计算着色器、着色器存储缓冲区对象、高质量 ETC2/EAC 纹理压缩、增强的内存安全性、多应用程序稳健性扩展、与 OpenGL ES 3.0 的兼容性&lt;/p&gt;

&lt;p&gt;4.4 2013 年 7 月， GLSL 4.40，缓冲区放置控制，高效异步查询，着色器变量布局，高效多对象绑定，Direct3D 应用程序的流线型移植，无绑定纹理扩展，稀疏纹理扩展&lt;/p&gt;

&lt;p&gt;4.5 2014 年 8 月， GLSL 4.50，直接状态访问 (DSA)，刷新控制，鲁棒性，OpenGL ES 3.1 API 和着色器兼容性，DX11 仿真功能&lt;/p&gt;

&lt;p&gt;4.6 2017 年 7 月， GLSL 4.60，更高效的几何处理和着色器执行，更多信息，无错误上下文，多边形偏移钳位，SPIR-V，各向异性过滤&lt;/p&gt;

&lt;p&gt;经过显卡历史、GPU管线的变化历史、OpenGL的功能变迁史，让我们把GPU看的更清楚。&lt;/p&gt;

&lt;h2 id=&quot;gpu-core结构&quot;&gt;GPU Core结构&lt;/h2&gt;

&lt;p&gt;我们知道现代的 CPU 里除了基本的元器件外，还有许多围绕提高执行效率的元器件，以及增加诸多功能的其他元器件。这些元器件在 GPU 里有点多余了，GPU 的整个处理过程是一个流式处理过程，没有那么多分支条件，以及复杂的依赖关系。&lt;/p&gt;

&lt;p&gt;因此我们可以把 GPU 里这些对应的元器件去掉，只留下取指令、指令译码、ALU 以及执行这些计算需要的寄存器和缓存。如图：&lt;/p&gt;

&lt;p&gt;（GPU元器件瘦身图）&lt;/p&gt;

&lt;p&gt;这样看来GPU core比CPU Core的构造简单的多了，由于传输GPU的数据并不相互依赖的，因此我们可以用很多个GPU Core来并行计算这些数据。&lt;/p&gt;

&lt;p&gt;于是就有了，多GPU Core的结构，如下图：&lt;/p&gt;

&lt;p&gt;（多个Core并行工作图）&lt;/p&gt;

&lt;p&gt;多个Core并行工作时它们使用了相同的取指器并且有相同的代码，为什么不把它们并起来呢。&lt;/p&gt;

&lt;p&gt;前面我们说过SIMD，它把4个数据一起提交并用一个指令执行它完成计算。在GPU中借鉴了SIMD，用了一种跟它很像的处理技术叫做SIMT（Single Instruction Multiple Threads），如下图：&lt;/p&gt;

&lt;p&gt;（ 单指令多数据流管线）&lt;/p&gt;

&lt;p&gt;在SIMT中，向GPU Core输入的是8个图元或片元，同时输出8个结果，每次输入多个数据到GPU Core中，并获得多个结果。SIMT 比 SIMD 更加灵活。&lt;/p&gt;

&lt;p&gt;SIMT可以把多条数据，交给不同的线程去处理。各个线程里面执行的指令流程是一样的，但是可能根据数据的不同，走到不同的条件分支。这样，相同的代码和相同的流程，可能执行不同的具体的指令。这个线程走到的是 if 的条件分支，另外一个线程走到的就是 else 的条件分支了。&lt;/p&gt;

&lt;h2 id=&quot;gpu的分支处理&quot;&gt;GPU的分支处理&lt;/h2&gt;

&lt;p&gt;我们CPU有对分支做预测，让流水线停顿更少，GPU Core也会对分支做优化处理。&lt;/p&gt;

&lt;p&gt;（GPU的分支处理）&lt;/p&gt;

&lt;p&gt;常用的GPU分支处理SIMD里，为每个指令都分配一个ALU做并行处理，用多个周期分别计算分支的两种结果。&lt;/p&gt;

&lt;p&gt;这样做就不会让流水线停滞，但是这样做有效率问题，在一个指令周期里，很多ALU是闲置的。&lt;/p&gt;

&lt;p&gt;因此在SIMD之后，SIMT（Single Instruction，Multiple Threads）技术可以变相的做分支的顺序执行，如下图：&lt;/p&gt;

&lt;p&gt;（SIMT 分支预测并行计算）&lt;/p&gt;

&lt;p&gt;在SIMT中，各个线程里面执行的指令流程是一样的，只是走的不同的分支。相同的代码和相同的流程，执行不同的分支。
可能一些线程走到的是 if 的条件分支，而另外一些线程走到的就是 else 的条件分支，这种并行计算使得计算本身无需依赖上文，也让ALU不再空闲停滞。&lt;/p&gt;

&lt;p&gt;这里简单说下解决SIMIT流水线中的卡顿问题&lt;/p&gt;

&lt;p&gt;拆分存储缓存，让上下文依赖的计算在不同时段同时计算，以提高ALU的利用率。&lt;/p&gt;

&lt;p&gt;（卡顿时启动另一条管线）&lt;/p&gt;

&lt;p&gt;（拆分整个缓存为独立缓存）&lt;/p&gt;

&lt;p&gt;GPU为了不等待分支条件而导致的停顿流水线，就要对每个分支做都做计算。分支内的数据仍然会有依赖关系，依赖关系就会造成卡顿，需要等待计算或等待获取资源。&lt;/p&gt;

&lt;p&gt;因此将原来的一整个缓存，拆分为多个缓存，使得流水线在阻塞时能更好的使用闲置ALU计算下一条数据。这样就能更好的利用ALU计算做优化了。&lt;/p&gt;

&lt;p&gt;现实GPU硬件中的物理架构&lt;/p&gt;

&lt;p&gt;前面我们说的都是抽象的GPU Core结构，下面我们来看下实际中的GPU物理架构。&lt;/p&gt;

&lt;p&gt;看到这些GPU架构可以发现它们虽然彼此有差异，但很多概念相同，下面我们俩理清一下这些架构中组建的概念：&lt;/p&gt;

&lt;p&gt;GPC（Graphics Processing Cluster） ： 图形处理集群，GPU划分多个GPC，每个GPC里有多个TPC，每个TPC里包含了多个SM和1个Rester Engine&lt;/p&gt;

&lt;p&gt;TPC（Texture Processing Cluster） ： 图像处理集群，是由若干个SM、1个纹理单元（Texture Unit）和一些逻辑控制和ALU组成。&lt;/p&gt;

&lt;p&gt;RT Core（Ray Trace Core） ： RT Core是SM里面加了一条专用的流水线(ASIC)来计算射线和三角形求交（可以访问BVH，用于光线追踪）。由于是ASIC专用电路逻辑，与shader code做求交计算相比，性能有数量级的提升。&lt;/p&gt;

&lt;p&gt;Rester Engine ： 光栅引擎，处理它接收到的三角形，并为它负责的那些部分生成像素信息（也处理背面剔除和 Z 剔除）。&lt;/p&gt;

&lt;p&gt;PolyMorp Engine：曲面引擎，是一个带有顶点提取器、视口变换的累积集群，它处理属性设置和流输出，这些都合并到了这个处理器中，极大地扩展了曲面细分和（当发送到光栅引擎时）光栅化性能。&lt;/p&gt;

&lt;p&gt;Thread Engine：线程引擎，调度线程到核的引擎&lt;/p&gt;

&lt;p&gt;SM（Stream Multiprocessor）、SMX、SMM ：SM包含GPU Core内核，指令单位，调度程序。&lt;/p&gt;

&lt;p&gt;Warp Scheduler、Dispatch Unit：负责线程束调度，将软件线程按一捆一捆（不是一个一个）的方式分配到计算核上。一个Warp由32个线程组成，Warp Scheduler的指令通过Dispatch Units派送到Core核上执行。&lt;/p&gt;

&lt;p&gt;SP（Streaming Processors）、Core ：SP有时也叫CUDA core，一个 SP 包括多个 ALU 和 FPU。SP是作用于顶点或像素数据的真正处理单元。&lt;/p&gt;

&lt;p&gt;ALU（Arithmetic Logic Unit）、FPU（Float Point Unit）：ALU 是算术和逻辑单元，FPU 是浮点单元。&lt;/p&gt;

&lt;p&gt;INT32，FP32 ：在GPU里支持单精度运算的Single Precision ALU称之为FP32 core或简称core，而把双精度运算的Double Precision ALU称之为DP unit或者FP64 core。第三代的Kepler架构里，FP64单元和FP32单元的比例是高端机1:3或者低端机1:24，到了第五代比例为1:2，低端型号里仍然保持为1:32。&lt;/p&gt;

&lt;p&gt;SFU（Special Function Unit）：执行特殊数学运算（sin、cos、log等）&lt;/p&gt;

&lt;p&gt;TENSO CORE ： 精度混合计算单元，转换不同精度之间的运算结果，用于执行矩阵乘法的计算单元，精度混合分为整数精度和浮点数精度。&lt;/p&gt;

&lt;p&gt;ROP（Render Output Unit） ：渲染输出单元 ，一个ROP内部有很多ROP单元，在ROP单元中有深度测试和Framebuffer混合，深度和颜色的设置必须是原子操作，否则两个不同的三角形在同一个像素点就会有冲突和错误。&lt;/p&gt;

&lt;p&gt;LD/ST（Load/Store Unit）：加载和存储数据&lt;/p&gt;

&lt;p&gt;Share Memory、L1 Data Cache、L1 Cache、L2 Cache ：共享内存，以及多级的高速缓存&lt;/p&gt;

&lt;p&gt;RF（Register File）：寄存器堆，多个寄存器组成的阵列&lt;/p&gt;

&lt;p&gt;Instruction Cache ：指令缓存&lt;/p&gt;

&lt;p&gt;未完待续…&lt;/p&gt;

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;《How Shader Cores Work》&lt;/p&gt;

&lt;p&gt;https://engineering.purdue.edu/~smidkiff/KKU/files/GPUIntro.pdf&lt;/p&gt;

&lt;p&gt;《CPU体系结构》&lt;/p&gt;

&lt;p&gt;https://my.oschina.net/fileoptions/blog/1633021&lt;/p&gt;

&lt;p&gt;《深入理解CPU的分支预测(Branch Prediction)模型》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/22469702&lt;/p&gt;

&lt;p&gt;《分析Unity在移动设备的GPU内存机制（iOS篇）》&lt;/p&gt;

&lt;p&gt;https://www.jianshu.com/p/68b41a8d0b37&lt;/p&gt;

&lt;p&gt;《针对移动端TBDR架构GPU特性的渲染优化》&lt;/p&gt;

&lt;p&gt;https://gameinstitute.qq.com/community/detail/123220&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Tile-based rendering》&lt;/p&gt;

&lt;p&gt;https://www.imaginationtech.com/blog/a-look-at-the-powervr-graphics-architecture-tile-based-rendering/&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Deferred rendering》&lt;/p&gt;

&lt;p&gt;https://www.imaginationtech.com/blog/the-dr-in-tbdr-deferred-rendering-in-rogue/&lt;/p&gt;

&lt;p&gt;《深入GPU硬件架构及运行机制》&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/timlly/p/11471507.html&lt;/p&gt;

&lt;p&gt;《深入浅出计算机组成原理》&lt;/p&gt;

&lt;p&gt;https://time.geekbang.org/column/article/105401?code=7VZ-Md9oM7vSBSE6JyOgcoQhDWTOd-bz5CY8xqGx234%3D&lt;/p&gt;

&lt;p&gt;《Nvidia Geforce RTX-series is born》&lt;/p&gt;

&lt;p&gt;https://www.fudzilla.com/reviews/47224-nvidia-geforce-rtx-series-is-born?start=2&lt;/p&gt;

&lt;p&gt;《渲染管线与GPU（Shading前置知识）》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/336999443&lt;/p&gt;

&lt;p&gt;《剖析虚幻渲染体系（12）- 移动端专题Part 1（UE移动端渲染分析）》&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/timlly/p/15511402.html&lt;/p&gt;

&lt;p&gt;《tpc-texture-processing-cluster》&lt;/p&gt;

&lt;p&gt;https://gputoaster.wordpress.com/2010/12/11/tpc-texture-processing-cluster/&lt;/p&gt;

&lt;p&gt;《Life of a triangle - NVIDIA’s logical pipeline》&lt;/p&gt;

&lt;p&gt;https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline&lt;/p&gt;

&lt;p&gt;《Rasterisation wiki》&lt;/p&gt;

&lt;p&gt;https://en.wikipedia.org/wiki/Rasterisation&lt;/p&gt;

&lt;p&gt;《PolyMorph engine and Data Caches by Hilbert Hagedoorn》&lt;/p&gt;

&lt;p&gt;https://www.guru3d.com/articles-pages/nvidia-gf100-(fermi)-technology-preview,3.html&lt;/p&gt;

&lt;p&gt;《NVIDIA GPU的一些解析》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/258196004&lt;/p&gt;

&lt;p&gt;《tensor-core-performance-the-ultimate-guide》&lt;/p&gt;

&lt;p&gt;https://developer.download.nvidia.cn/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf&lt;/p&gt;

&lt;p&gt;《Understanding the Understanding the graphics pipeline》&lt;/p&gt;

&lt;p&gt;https://www.seas.upenn.edu/~cis565/LECTURES/Lecture2%20New.pdf&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247485051&amp;amp;idx=1&amp;amp;sn=e98f1e1971d56ce6543926b4f9bad204&amp;amp;chksm=fc22637ccb55ea6a4c16c5c0d8fd43cff936350490d79f1e377b61e4914300c39c4fc2911da1&amp;amp;token=53415989&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Mon, 22 Nov 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B050</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B050</guid>
            </item>
        
            <item>
               <title>读书笔记(四十八) 《游戏引擎架构》#4 低阶渲染器（2）</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484981&amp;amp;idx=1&amp;amp;sn=b5199f84ed73d28916dbb9143eeeb992&amp;amp;chksm=fc226332cb55ea2417b89e499284ab3a552ce591caba09d637912306503e4b457ed45c2e1d2d&amp;amp;token=1023740201&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景：&lt;/h2&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了几遍《游戏引擎架构》后对引擎架构的理解又深入了些。&lt;/p&gt;

&lt;p&gt;近段时间有对引擎剖析的想法，正好借这书本对游戏引擎架构做一个完整分析。&lt;/p&gt;

&lt;p&gt;此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节。&lt;/p&gt;

&lt;p&gt;借助《游戏引擎架构》这本书、结合引擎源码和自己的经验，深入分析游戏引擎的历史、架构、模块，最后通过实践简单引擎开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;同时《游戏引擎架构》中部分知识太过陈旧的部分，会重新深挖后总结出自己的观点。&lt;/p&gt;

&lt;h2 id=&quot;概述&quot;&gt;概述：&lt;/h2&gt;

&lt;p&gt;本系列文章对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;时间库&lt;/li&gt;
  &lt;li&gt;自定义容器库&lt;/li&gt;
  &lt;li&gt;字符串散列库&lt;/li&gt;
  &lt;li&gt;内存管理框架&lt;/li&gt;
  &lt;li&gt;RTTI与反射模块&lt;/li&gt;
  &lt;li&gt;图形计算库&lt;/li&gt;
  &lt;li&gt;资产管理模块&lt;/li&gt;
  &lt;li&gt;低阶渲染器&lt;/li&gt;
  &lt;li&gt;剔除与合批模块&lt;/li&gt;
  &lt;li&gt;动画模块&lt;/li&gt;
  &lt;li&gt;物理模块&lt;/li&gt;
  &lt;li&gt;UI底层框架&lt;/li&gt;
  &lt;li&gt;性能剖析器的核心部分&lt;/li&gt;
  &lt;li&gt;脚本系统&lt;/li&gt;
  &lt;li&gt;视觉效果模块&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;本篇内容为列表中的第8个部分的第1节。&lt;/p&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文：&lt;/h2&gt;

&lt;p&gt;简单回顾下前文&lt;/p&gt;

&lt;p&gt;前文我们聊了下显卡在计算机硬件主板中的位置与结构，知道了CPU、GPU的通信介质，并简单介绍了手机上的主板结构。本篇开头对上一篇做一些内容补充，PC和手机的不同硬件组织，以及CPU与其他芯片的通信过程。&lt;/p&gt;

&lt;p&gt;下面我们开始这篇内容&lt;/p&gt;

&lt;h3 id=&quot;本次内容会围绕gpu来写从硬件架构到软件驱动再到引擎架构目标是帮大家理解gpu硬件的运作原理理解图形接口的架构理解引擎低阶渲染器的架构&quot;&gt;本次内容会围绕GPU来写，从硬件架构到软件驱动再到引擎架构，目标是帮大家理解GPU硬件的运作原理，理解图形接口的架构，理解引擎低阶渲染器的架构。&lt;/h3&gt;

&lt;p&gt;目录：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;主板结构中的显卡&lt;/li&gt;
  &lt;li&gt;GPU功能发展史&lt;/li&gt;
  &lt;li&gt;GPU与CPU的差异&lt;/li&gt;
  &lt;li&gt;GPU硬件特点&lt;/li&gt;
  &lt;li&gt;图形驱动程序架构&lt;/li&gt;
  &lt;li&gt;引擎低阶渲染架构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;继续上篇未结束的内容&lt;/p&gt;

&lt;p&gt;说硬件结构时，我们常常从台式 PC 开始说起，因为智能手机本质上是袖珍型计算机，具有一些额外的无线电，并且在功耗、热量输出等方面极其受限的条件下运行。PC 主要由主板芯片组组成（通常分为连接处理器和内存“北桥”（例如 Intel 的 P45）和连接各种外围总线（例如 PCI-Express、USB 等）的“南桥”（例如 Intel 的 ICH10R） )、中央处理器（例如 CPU – Intel 的 Core 2 Duo）、随机存储器内存 (RAM)、永久存储器（硬盘或固态硬盘）、图形处理单元（例如 GPU – ATI 的 Radeon HD5890） 、电源和多个连接到 PCI 或 PCI-Express 总线的外围设备（例如，内置声音芯片组或附加 Wi-Fi 卡）。&lt;/p&gt;

&lt;p&gt;（三星手机拓扑图：图片来源网络）&lt;/p&gt;

&lt;p&gt;（iPhone6p A8 正面图：图片来源网络）&lt;/p&gt;

&lt;p&gt;（iPhone6p A8 背面图：图片来源网络）&lt;/p&gt;

&lt;p&gt;（苹果A8芯片内部结构：图片来源网络）&lt;/p&gt;

&lt;p&gt;智能手机也具有所有这些组件，只是集成度更高而已。一些主要芯片通常可以包含 CPU、GPU、其他专用协处理器、主板总线、内存控制器、LCD 控制器、声音芯片组、CMOS 摄像头接口、板载内存 ，以及一些外围设备（例如 Cell、Wifi 和蓝牙无线电）。 “应用处理器”是负责一般处理的芯片（类似于 CPU + 主板芯片组），并且可能内置了一些其他功能。 “基带处理器”负责蜂窝无线通信。&lt;/p&gt;

&lt;p&gt;智能手机与 PC 如此相似，那么为什么我们今天在智能手机中看不到像 Core 2 Duo 这样的 x86 CPU？答案是，Core 2 Duo 消耗的功率和产生的热量远远超出手机的可接受范围。典型的台式机 Core 2 Duo CPU 的功耗最高可达 65W，并且需要一个主动冷却系统来防止过热。即使是英特尔的 Atom 芯片也消耗 4W（峰值 TDP），而整个手机通常必须满足低于 1W 的功率。&lt;/p&gt;

&lt;p&gt;今天我们很多智能手机都使用 ARM 芯片， ARM 的运作方式与Intel截然不同。 Intel 自己设计和构建所有处理器，而 ARM 则创建指令集，任何跟随它们的 CPU 都将能够运行与 ARM 兼容的代码。 它还为适合其指令集的处理器创建参考设计，这使公司能够围绕 ARM 的核心设计轻松生产自己的芯片组。 一些公司，如英伟达、三星和德州仪器，只是简单地许可并采用 ARM CPU 参考设计，而其他公司，如高通和 Marvell，只许可指令集并创建自己的处理器以适应它们。 第一个 ARMv7 参考设计是 Cortex A8，其架构可在今天的智能手机中找到。很久以前低端智能手机倾向于使用实现 ARMv6 指令集的 ARM11 芯片，现在已经不复存在。&lt;/p&gt;

&lt;p&gt;这里再补充一下前文的硬件通信过程，每个带有处理程序的硬件都有自己的芯片，CPU是整个电脑的主要计算芯片，GPU则被单独拆分出来作为图像的处理芯片，除了这两个大家熟知的芯片，还有内存芯片，硬盘芯片，声卡芯片，网卡芯片等，这些芯片都通过总线进行数据交互。其中前面我们提到南桥和北桥芯片桥接了快慢不同的芯片之间的通信，让不同种类的设备通信效率更高更有序。&lt;/p&gt;

&lt;p&gt;（PC端CPU与GPU通信结构图）&lt;/p&gt;

&lt;p&gt;（手机端CPU与GPU通信结构图）&lt;/p&gt;

&lt;h3 id=&quot;cpu与其他芯片之间的通信步骤可以简单理解为cpu通过总线发送指令数据到其他芯片芯片收到指令并处理后再通过总线将反馈数据给cpu中间处理过程也会与内存芯片或其他芯片通信后再反馈给cpu&quot;&gt;CPU与其他芯片之间的通信步骤，可以简单理解为，CPU通过总线发送指令数据到其他芯片，芯片收到指令并处理后，再通过总线将反馈数据给CPU，中间处理过程也会与内存芯片或其他芯片通信后再反馈给CPU。&lt;/h3&gt;

&lt;p&gt;（CPU用虚拟地址访问内存拓扑图）&lt;/p&gt;

&lt;h3 id=&quot;cpu-在访问内存时需要通过-mmu-把虚拟地址转化为物理地址再通过总线访问内存mmu-开启后-cpu-看到的所有地址都是虚拟地址cpu-把这个虚拟地址发给-mmu-后mmu-会在页表里查出这个虚拟地址对应的物理地址再通过总线去访问ddr内存芯片&quot;&gt;CPU 在访问内存时需要通过 MMU 把虚拟地址转化为物理地址，再通过总线访问内存。MMU 开启后 CPU 看到的所有地址都是虚拟地址，CPU 把这个虚拟地址发给 MMU 后，MMU 会在页表里查出这个虚拟地址对应的物理地址，再通过总线去访问DDR内存芯片。&lt;/h3&gt;

&lt;p&gt;从这个芯片通信角度看，我们可以明白，总线传输数据的速度，限制了计算机的运行速度。因此我们在优化计算机程序时会常常去减少CPU与GPU之间通信量，原因就是在无法增加带宽上限这个大前提下，尽量减少它们之间通信数据量，从而减少访问消耗时间。&lt;/p&gt;

&lt;h2 id=&quot;gpu功能发展史&quot;&gt;GPU功能发展史&lt;/h2&gt;

&lt;p&gt;第一篇中我们介绍过一些图形接口的发展历史，现在我们来介绍一下GPU硬件的发展历史。硬件的发展史其实是一个商业的竞争过程，中间夹杂着很多商业战略与决策。尤其是当大家都看到GPU这块高新的技术’蛋糕’，有一系列公司参与进来相互竞争。因此这里不过多的介绍商业上的活动与竞争，而是专注于介绍GPU功能的发展过程。&lt;/p&gt;

&lt;p&gt;第一个真正的3D图形卡始于早期的显示控制器，即视频移位器和视频地址生成器（video shifters and video address generators）。它们充当主处理器和显示器之间的直通通道（pass-through）。传入的数据流被转换为串行位图视频输出，例如亮度，颜色以及垂直和水平复合同步，这将像素行保持在显示生成中，并同步每条连续行以及消隐间隔（时间间隔为 结束一条扫描线并开始下一条扫描线）。&lt;/p&gt;

&lt;p&gt;（图片来源网络）&lt;/p&gt;

&lt;p&gt;1970年代开始这些视频适配器出现了一系列的改进，主要兼容了不同分辨率以及不同的视频信号。&lt;/p&gt;

&lt;p&gt;1976年，ANTIC使用直接存储器访问（DMA）处理2D显示指令。&lt;/p&gt;

&lt;p&gt;1978年，英特尔82720图形卡芯片，它能够以256x256的分辨率（或以512x512的单色）显示八种颜色数据。其32KB的显示内存足以绘制线，弧，圆，矩形和字符位图。该芯片还提供了缩放，屏幕分区和滚动的功能。&lt;/p&gt;

&lt;p&gt;1979年，SGI推出了用于工作站的IRIS图形卡-GR1.x图形卡板，其中提供了用于颜色选项，几何图形卡，Z缓冲区和覆盖/底层的单独的外接（子）板。&lt;/p&gt;

&lt;p&gt;从第一块图形卡发明到现在，整个过程可以分为，视频适配器时代，2D时代、3D时代启程、3D时代崛起、3D时代巅峰，这4个部分。&lt;/p&gt;

&lt;p&gt;为此我画了一副时序图并标出了重要节点，便于大家理解显卡的发展史，同时以文字形式告知大家图形显卡的发展历史。&lt;/p&gt;

&lt;p&gt;（显卡发展史）&lt;/p&gt;

&lt;h2 id=&quot;2d时代&quot;&gt;2D时代&lt;/h2&gt;

&lt;p&gt;1981年, IBM推出了个人电脑时，它提供了两种显卡，一种是“单色显卡”(简称 MDA), 一种是 “彩色绘图卡” (简称 CGA), 从名字上就可以看出，MDA是与单色显示器配合运用的, 它可以显示80行x25列的文数字, CGA则可以用在RGB的显示屏上, 它可以绘制的图形和文数字资料。&lt;/p&gt;

&lt;p&gt;1982年，IBM又推出了MGA（Monochrome Graphic Adapter）, 又称Hercules Card (大力士卡),  除了能显示图形外，还保留了原来 MDA 的功能。&lt;/p&gt;

&lt;p&gt;这些显卡均为采纳使用数字方式的，直到MCGA（Multi-Color Graphics Array）的出现，才揭开了采纳使用模拟方式的显卡的序幕。MCGA是整合在 PS/2 Model 25和30上的影像系统。它采纳使用了Analog RGA影像信号, 解析度可高达640x480, 数位RGB和类比RGB不同的的方就像是ON-OFF式切换和微调式切换之间的差别。&lt;/p&gt;

&lt;p&gt;1986年ATI推出了第一款产品，即OEM颜色仿真卡。它用于通过9针DE-9连接器将黑色背景的单色绿色，琥珀色或白色磷光体文本输出到TTL监视器，该卡至少配备了16KB的内存。&lt;/p&gt;

&lt;p&gt;1987年，ATI在其OEM产品线中增加了Graphics Solution Plus系列，该产品线将IBM PC / XT ISA 8位总线用于基于Intel 8086/8088的IBM PC。该芯片通过DIP开关支持MDA，CGA和EGA图形卡模式。ATI EGA 800：16色VGA仿真，支持800x600。&lt;/p&gt;

&lt;p&gt;VGA（Video Graphic Array）即显示绘图阵列，它IBM是在其 PS/2 的Model 50, 60和80内建的影像系统。它的数字模式可以达到720x400色, 绘图模式则可以达到640x480x16色, 以及320x200x256色，这是显卡首次可以相应情况下最大限度显示256种色彩，而这些模式更成为其后所有显卡的共同标准。&lt;/p&gt;

&lt;p&gt;1988年，带有游戏控制器端口和复合输出选项的Small Wonder Graphics解决方案问世（用于CGA和MDA仿真），以及具有扩展EGA和16位VGA支持的EGA Wonder 480和800+，以及VGA 新增了VGA和SVGA支持的Wonder和Wonder 16。&lt;/p&gt;

&lt;p&gt;1988年，Trident 8900/9000显卡，它第一次使显卡成为一个独立的配件出现在电脑里，而不再是集成的一块芯片。而后其推出的Trident 9685更是第一代3D显卡的代表。不过真正称得上开启3D显卡大门的却应该是1996年的GLINT 300SX，虽然当时其3D功能非常简单，却具有里程碑的意义。&lt;/p&gt;

&lt;p&gt;1989年推出了更新的VGA Wonder / Wonder 16系列，其中包括降低成本的VGA Edge 16（Wonder 1024系列）。新功能包括一个总线鼠标端口，并支持VESA功能连接器。这是一个金手指连接器，类似于缩短的数据总线插槽连接器，它通过带状电缆链接到另一个视频控制器，以绕过拥挤的数据总线。&lt;/p&gt;

&lt;p&gt;1991年，Wonder系列的更新继续向前发展。WonderXL卡增加了VESA 32K颜色兼容性和Sierra RAMDAC，从而将最大显示分辨率提高到640x480 @ 72Hz 或 800x600 @ 60Hz。&lt;/p&gt;

&lt;p&gt;1991年5月，ATI的Mach系列与Mach8一同推出。它以芯片或电路板的形式出售，可以通过编程接口（AI）卸载有限的2D绘图操作，例如线条画，颜色填充和位图组合（Bit BLIT）.ATI添加了Wonder的一种变体 XL在扩展PCB上集成了Creative Sound Blaster 1.5芯片。它被称为VGA Stereo-F / X，它能够模拟Sound Blaster单声道文件中的立体声，并且其质量接近FM广播质量。ATI图形卡Ultra ISA（Mach8 + VGA）,将Mach8与VGA Wonder +的图形卡核心（28800-2）结合在一起以实现其3D功能。&lt;/p&gt;

&lt;p&gt;1992年1月，Silicon Graphics Inc（SGI）发布了OpenGL 1.0，这是一个针对2D和3D图形卡的多平台供应商不可知的应用程序编程接口（API）。OpenGL是从SGI专有的API（称为IRIS GL（集成的栅格成像系统图形卡库））演变而来的。最初，OpenGL瞄准的是基于UNIX的专业市场，但是由于开发人员对扩展实施的友好支持，很快将其用于3D游戏。同时微软正在开发自己的竞争对手Direct3D API，并没有确保OpenGL在Windows下也能正常运行。&lt;/p&gt;

&lt;p&gt;1993年11月，ATI宣布68890 PC电视解码器芯片的发布，该芯片首次在Video-It卡里面亮相。借助板载Intel i750PD VCP（视频压缩处理器），该芯片能够以320x240 @ 15 fps或160x120 @ 30 fps捕获视频，并能够实时压缩/解压缩。它还能够通过数据总线与图形卡板通信，从而无需使用加密狗或端口和带状电缆。&lt;/p&gt;

&lt;p&gt;1995年，ATI的Mach8发布，创造了许多著名的首创。它成为第一个以Xclaim形式在PC和Mac计算机上使用的图形卡适配器，并且与S3的Trio一起提供了全动态视频播放加速功能。&lt;/p&gt;

&lt;h2 id=&quot;3d时代开启&quot;&gt;3D时代开启&lt;/h2&gt;

&lt;p&gt;1995年5月，nVidia推出了他们的第一款图形卡芯片NV1，并成为首款能够进行3D渲染，视频加速和集成GUI加速的商业图形卡处理器。供应商发布显卡支持规格的主板（Diamond Edge 3D），D3D（Diamond Edge 3D）图形卡API确认依赖于渲染三角形多边形，而NV1则使用四边形纹理映射。nVidia通过驱动程序添加了有限的D3D兼容性，以将三角形包装为二次曲面，但是市面上仍然很少针对NV1量身定制的游戏。&lt;/p&gt;

&lt;p&gt;1995年11月，ATI宣布了他们的首个3D加速器芯片3D Rage（也称为Mach 64 GT）。&lt;/p&gt;

&lt;p&gt;1995年，3Dfx推出了业界的口碑极佳的3D图形加速卡：Voodoo。3Dfx的专利技术Glide引擎接口一度称霸了整个3D世界，直至D3D和OpenGL的出现才改变了这种局面。&lt;/p&gt;

&lt;p&gt;Voodoo标配为4Mb EDO显存，可以提供在640×480分辨率下3D显示速度和最华丽的画面。Voodoo也有硬伤，它只是一块具有3D加速功能的子卡，运用时需搭配一块具有2D功能的显卡，因此当时S3 765+Voodoo是为人津津乐道的黄金组合。&lt;/p&gt;

&lt;p&gt;S3 765显卡是当时兼容机的标准配置，最大限度支持2MB EDO显存，可以实现高分辨率显示，可以支持1024×768的分辨率，并且在低分辨率下支持最大限度32Bit真彩色，而且性能和价格比也较强。&lt;/p&gt;

&lt;p&gt;1995年，VideoLogic开发了一种基于图块的延迟渲染技术（TBDR），该技术在纹理、阴影和光照应用于剩下的渲染之前，抛弃了所有可见的几何形状，从而消除了对大规模z缓冲(在最终渲染中去除遮挡/隐藏的像素)的需求。这个过程产生的框架被分割成矩形块，每个图块都可以自行进行多边形渲染并发送到输出。一旦计算了帧所需的像素并剔除了多余的多边形(z缓冲只在平铺层发生)，对多边形渲染就开始了，这样就只需要进行最基本的计算。&lt;/p&gt;

&lt;p&gt;1995年，Rendition的VéritéV1000成为第一张具有可编程核心的显卡，它使用了基于MIPS的RISC处理器和Pixel Pipelines(像素管线)。处理器负责设置和组织管线的工作负载。 Vérité1000最初于1995年底开发，后来成为Microsoft用于开发Direct3D的主板之一。&lt;/p&gt;

&lt;p&gt;1996年 3DLabs 研制出Glint，该公司诞生于杜邦的Pixel图形卡部门。GLINT 300SX处理器能够进行OpenGL渲染，片段处理和光栅化。GLINT 300SX增加了2MB的内存。它为纹理和Z缓冲区使用了1MB，为帧缓冲区使用了1MB，但是还提供了一个选项，以增加VRAM的Direct3D兼容性。&lt;/p&gt;

&lt;p&gt;1997年3月ATI推出 3D Rage Pro，它在4MB格式下几乎可以媲美Voodoo Graphics的性能。在使用8MB和AGP接口时，其性能则优于3Dfx卡。它扩展了4kB高速缓存并增加了边缘抗锯齿功能，该显卡改进了Rage II的透视校正，纹理处理能力以及三线性滤波性能。还集成了一个浮点单元，以减少对CPU的依赖以及对DVD的硬件加速和显示支持。&lt;/p&gt;

&lt;p&gt;1997年4月，nVidia推出了RIVA 128（实时交互式视频和动画加速器），并通过渲染三角形多边形增加了Direct3D兼容性，使用新的350nm工艺，并开发了RAMDAC和视频转换器，这是nVidia具有里程碑意义的显卡。&lt;/p&gt;

&lt;p&gt;1998年1月，英特尔发布i740，它结合了R3D/100上两种不同的图形和纹理芯片的资源，实现了AGP纹理，纹理被上传到系统内存中(渲染缓冲区也可以存储在RAM中)。此前一些设计会选择使用显卡的帧缓冲区用来保存纹理，如果帧缓冲区饱和或纹理太大而无法存储在本地图形内存中，则纹理交换到系统RAM中。为了最大程度地减少延迟，英特尔的设计使用了AGP Direct Memory Execute（DiME）功能，该功能仅调用光栅化所需的那些纹理，其余的存储在系统RAM中。性能和图像质量可以接受的，性能大致与上一年的高端产品相匹配，因此该产品大放异彩。&lt;/p&gt;

&lt;p&gt;1998年3月，3Dfx推出Voodoo2。Voodoo2本身带有8Mb/12Mb EDO显存，PCI接口，卡上有双芯片，可以做到单周期多纹理运算。缺点是它的卡身很长，并且芯片发热量相当大，依然只作为一块3D加速子卡，需要一块2D显卡的支持。Voodoo2的推出使得3D加速又到达了一个新的里程碑，依仗Voodoo2的效果、画面和速度，征服了不少当时盛行一时的3D游戏，比如Fifa98，NBA98，Quake2等等。&lt;/p&gt;

&lt;p&gt;1998年7月，Matrox公司发布MGA G200继承了自己超一流的2D水准，3D方面有了革命性的提高，不但可以提供和Voodoo2差不多的处理速度和特技效果，另外还支持DVD硬解码和视频输出，并且独一无二的首创了128位独立双重总线技术，大大提高了性能，配合当时相当走红的AGP总线技术，G200也赢得了不少用户的喜爱。&lt;/p&gt;

&lt;p&gt;1998年的S3的野人系列Savage系列显卡，Savage3D采纳使用128位总线结构及单周期三线性多重贴图技术，最大像素填充率达到了125M Pixels/s，三角形生成速率也达到了每秒500万个。通过S3新设计的AGP引擎和S3TC纹理压缩技术，支持Direct3D与OpenGL，最大显存容量可达8MB SGRAM或SDRAM，支持AGP 4×规范。相应情况下也支持当时流行的如反射和散射、Alpha混合、多重纹理、衬底纹理、边缘抗锯齿、16/24位Z-buffering、Tri-linear Filtering（三线性过滤技术）、S3TC纹理压缩技术等技术。可惜就是受到驱动程序不兼容的严重影响，最终在99年时惨淡收场。&lt;/p&gt;

&lt;h2 id=&quot;3d时代崛起&quot;&gt;3D时代崛起&lt;/h2&gt;

&lt;p&gt;1999年2月，S3发布Savage4，成为的第一张支持多重纹理的显卡，也是第一张支持AGP 4x接口的显卡。&lt;/p&gt;

&lt;p&gt;1999年，NVidia推出TNT2 Ultra、TNT2和TNT2 M64三个版本的芯片，后来又有PRO和VANTA两个版本。这种分类方式也促使后来各个生产厂家对同一芯片进行高中低端的划分，以满足不同层次的消费需要。TNT系列配置了8Mb到32Mb的显存，支持AGP2X/4X，支持32位渲染等等众多技术。nVidia能战胜Voodoo3，与3Dfx公司推行的策略迫使众多生产厂家投奔nVidia也不无关系，促进了TNT系列的推广。&lt;/p&gt;

&lt;p&gt;1999年，Matrox推出 MGA G400，它拥有16Mb/32Mb的显存容量，支持AGP 2X/4X，还有支持大纹理以及32位渲染等等，独特漂亮的EMBM硬件凹凸贴图技术，营造出的完美凹凸感并能实现动态光影效果的技术，并且G400拥有优秀的DVD回放能力，只是价格有些昂贵。通过双显示控制器（被Matrox称为DualHead）驱动两台显示器的能力开始了公司的多显示器支持趋势。&lt;/p&gt;

&lt;p&gt;1999年，nVidia终于爆发了，它在99年末推出了当前革命性的显卡—Geforce 256，彻底打败了3Dfx。代号NV10的GeForce 256支持Cube-Environment Mapping，完全的硬件T&amp;amp;L（Transform &amp;amp; Lighting），把原来有CPU计算的数据直接交给显示芯片处理，大大解放了CPU，也提高了芯片的运用效率。GeForce256拥有4条图形纹理通道，单周期每条通道处理两个象素纹理，工作频率120MHz，全速可以达到480Mpixels/Sec，支持SDRAM和DDR RAM，运用DDR的产品能更好的发挥GeForce256的性能。其不足之处就在于采纳使用了0.22纳米的工艺技术，发热量相对其它的来说很高的。&lt;/p&gt;

&lt;p&gt;2000年7月，3Dfx发布的Voodoo 5引入了T-buffer技术，作为变换和照明的一种替代方法，它基本上采用了一些渲染帧并将它们聚合为一个图像的方法。这产生了稍微模糊的图片，当按帧顺序运行时，可以平滑动画的运动。&lt;/p&gt;

&lt;p&gt;2000年末，3Dfx最终被nVidia收购。&lt;/p&gt;

&lt;h2 id=&quot;3d时代巅峰&quot;&gt;3D时代巅峰&lt;/h2&gt;

&lt;p&gt;2000年，ATI依仗T&amp;amp;L技术打开市场，在经历“曙光女神”的失败后，ATI推出了自己的T&amp;amp;L芯片RADEON 256。RADEON和NVIDIA一样具有高低端的版本，完全硬件T&amp;amp;L，Dot3和环境映射凹凸贴图，还有两条纹理流水线，可以相应情况下处理三种纹理。最出彩的是HYPER-Z技术，大大提高了RADEON显卡的3D速度，拉近了与GEFORCE 2系列的距离，ATI的显卡开始在市场占据主导地位。&lt;/p&gt;

&lt;p&gt;2000年4月，nVidia发布了GeForce 2 GTS（GigaTexel Shader），称为Nvidia Shading Rasterizer，它允许将高光阴影，体积爆炸，折射，波浪，顶点混合，阴影体积，凹凸贴图和高程贴图等效果应用于每个像素通过硬件的基础。&lt;/p&gt;

&lt;p&gt;2000年8月，ATI Radeon DDR上市。在卓越的T&amp;amp;L实现和对即将推出的DirectX 8特性的支持下，Radeon DDR与GeForce 2 GTS一起，通过将对接口的支持集成到芯片本身，引入了DVI输出的使用。&lt;/p&gt;

&lt;p&gt;2001年，nVidia推出的Geforce 3系列，这张卡成为了该领域的新王者，GeForce 3显卡增加了可编程T&amp;amp;L功能，可以对几乎所有的画面效果提供硬件支持。GeForce 3总共具有4条像素管道，填充速率最大限度可以达到每秒钟800 Mpixels。支持DirectX 8，多重采样AA，梅花AA（基本上是2xMSAA +后期处理模糊），8x各向异性滤光以及无与伦比的处理8xAF +三线性滤光的能力以及可编程的顶点着色器，用于更紧密地控制多边形网格运动和更流畅的动画序列。还有LMA(光速内存架构)支持——基本上是Nvidia的HyperZ版本——用于剔除隐藏在屏幕上其他像素后面的像素(Z遮挡剔除)，以及压缩和解压数据以优化带宽使用(Z压缩)。最后，Nvidia实现了负载平衡算法，将其称为Crossbar内存控制器，该控制器由四个独立的内存子控制器（与行业标准的单个控制器相对）组成，从而可以更有效地路由传入的内存请求。&lt;/p&gt;

&lt;p&gt;2001年，ATI则推出Radeon 8500/7500系列，采纳使用0.15微米工艺制造，包括6000万个晶体管，采纳使用了不少新技术(如Truform、Smartshader等)。并根据显卡的核心/显存工作频率分成不同的档次——核心/显存分别为275/550MHz的标准版，核心/显存为250/500MHz的RADEON 8500LE，生产核心/显存频率分别为300/600MHz的Ultra版，以及中端的Radeon 7500，低端的Radeon 7200，7000等产品。值得一提的是Radeon 8500还支持双头显示技术。&lt;/p&gt;

&lt;p&gt;2002年，ATI R300 GPU发布，由最初构成ArtX核心的团队开发，交付出色。它是第一个提供DirectX 9.0支持的应用程序，并且扩展了第一个支持着色器模型2.0，顶点着色器2.0和像素着色器2.0的体系结构。&lt;/p&gt;

&lt;p&gt;2002年，nVidia与ATI的竞争更加白热化。为巩固其图形芯片市场霸主的位，nVidia推出了Geforce 4系列，GeForce4 Ti系列择定确定是最具性能和价格比的，其代号是NV25，主要针对当时的高端图形市场，是DirectX 8时代下最劲爆强大的GPU图形处理器。芯片内部包含的晶体管数量高达6千3百万，运用0.15微米工艺生产，采纳使用了新的PBGA封装，运行频率达到了300MHz，配合频率为650MHz DDR显存，可以实现每秒49亿次的采样。GeForce4 Ti核心内建4条渲染流水线，每条流水线包含2个TMU（材质贴图单元）。Geforce 4系列从高到低，横扫了整个显卡市场。&lt;/p&gt;

&lt;p&gt;2002年，ATI推出R9700/9000/9500系列，首次支持DirectX 9，使其在与NVidia的竞争中抢得先机。同时R9700支持AGP 8X、DirectX 9，核心频率是300MHz，显存时钟是550MHz。RADEON 9700实现了可程序化的革命性硬件架构。符合AGP 8X最新标准，配有8个平等处理的渲染管线，每秒可处理25亿个像素，4个并列的几何处理引擎能处理每秒3亿个形迹及光效多边形。R9000则面向低端的产品，R9500则对标Ti4200。&lt;/p&gt;

&lt;p&gt;2003年的显卡市场依旧为N系与A系所统治。nVidia的Gf FX 5800（NV30）系列拥有32位着色，颜色画面有质的提高，在基础上推出的GeForce FX 5900，提高了晶体管数，降低了核心频率与显存频率，改用了256BIT DDR以提高显存带宽。&lt;/p&gt;

&lt;p&gt;2003年7月，nVidia推出了GF FX 5950/5700系列，以取代GF FX 5900/5600。新的Detonator FX驱动程序大大改善了AA和AF。
2003年9月，ATI推出了RADEON 9800/pro/SE/XT，依仗其超强的性能以及较低的售价，再次打败GF GX 5800。&lt;/p&gt;

&lt;p&gt;2004年是ATI大放异彩的一年，其最大的功臣却是来自于面向中低端的Radeon 9550。2004年最具性能和价格比的显卡，让ATI在低端市场呼风唤雨。R9550基于RV350核心，采纳使用0.13微米制程，核心频率为250MHz，显存频率为400MHz，4条渲染管道，1个纹理单元，相应情况下兼容64bit和128bit。&lt;/p&gt;

&lt;p&gt;2005年，Nvidia推出6000系列显卡，特性包括DirectX 9.0c支持、shader model 3.0(尽管显卡从未完全利用这一点)、Nvidia的PureVideo解码和播放引擎，以及SLI支持。&lt;/p&gt;

&lt;p&gt;2006年10月25日，ATI被AMD收购，总价为54亿美元。&lt;/p&gt;

&lt;p&gt;–&lt;/p&gt;

&lt;p&gt;NVIDIA发展表：&lt;/p&gt;

&lt;p&gt;1995年，NV1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;50K triangles/sec
1M pixel ops/sec
1M transistors
16-bit color
Nearest filtering
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1997年，Riva 128 (NV3), DX3&lt;/p&gt;

&lt;p&gt;1998 – Riva TNT (NV4), DX5&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;32位颜色,
24位Z缓存,
8位模板缓存
双纹理,
双线性过滤
每时钟2像素 (2 ppc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1999 - GeForce 256（NV10）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;固定管线，
支持DirectX 7.0
硬件T&amp;amp;L（Transform &amp;amp; lighting，坐标变换和光照）
立方体环境图（Cubemaps）
DOT3 – bump mapping
2倍各向异性过滤
三线性过滤
DXT纹理压缩
4ppc
引入“GPU”术语
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2001 - GeForce 3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 8.0
Shader Model 1.0
可编程渲染管线
顶点着色器
像素着色器
3D纹理
硬件阴影图
8倍各向异性过滤
多采样抗锯齿（MSAA）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2003 - GeForce FX系列（NV3x）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 9.0
Shader Model 2.0
256顶点操作指令
32纹理 + 64算术像素操作指令
512像素操作指令
着色语言：HLSL、CGSL、GLSL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2004 - GeForce 6系列 (NV4x)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 9.0c
Shader Model 3.0
动态流控制
分支、循环、声明等
顶点纹理读取
高动态范围（HDR）
64位渲染纹理（Render Target）
FP16*4 纹理过滤和混合
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2006 - GeForce 8系列 (G8x)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 10.0
Shader Model 4.0
几何着色器（Geometry Shaders）
没有上限位（No caps bits）
统一的着色器（Unified Shaders）
Vista系统全新驱动
基于GPU计算的CUDA问世
GPU计算能力以GFLOPS计量。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2010 - GeForce 405（GF119）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 11.0
曲面细分（Tessellation）
外壳着色器（Hull Shader）
镶嵌单元（tessellator）
域着色器（Domain Shader）
计算着色器（Compute Shader）
支持Stream Output
Shader Model 5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DirectX 11的渲染管线。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;多线程支持
改进的纹理压缩
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shader Model 5.0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;更多指令、存储单元、寄存器
面向对象着色语言
曲面细分
计算着色器
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2014 - GeForceGT 710（GK208）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 12.0
轻量化驱动层
硬件级多线程渲染支持
更完善的硬件资源管理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2016 - GeForceGTX 1060 6GB&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;首次支持RTX和DXR技术，即光线追踪
引入RT Core（光线追踪核心）
支持RTX光线追踪的显卡列表
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2018 - TITAN RTX（TU102）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectX 12.1，OpenGL 4.5
6GPC，36TPC，72SM，72RT Core，...
8K分辨率，1770MHz主频，24G显存，384位带宽
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;《GPU历史系列》&lt;/p&gt;

&lt;p&gt;https://new.qq.com/omn/20200506/20200506A0GP6H00.html?pc&lt;/p&gt;

&lt;p&gt;《GPU发展史》&lt;/p&gt;

&lt;p&gt;http://www.360doc.com/content/16/0607/16/30123241_565810074.shtml&lt;/p&gt;

&lt;p&gt;《显卡发展史1》&lt;/p&gt;

&lt;p&gt;http://www.360doc.com/content/20/0504/17/32196507_910185372.shtml&lt;/p&gt;

&lt;p&gt;《显卡发展史2》&lt;/p&gt;

&lt;p&gt;http://www.360doc.com/content/20/0504/17/32196507_910185435.shtml&lt;/p&gt;

&lt;p&gt;《显卡发展史3》&lt;/p&gt;

&lt;p&gt;http://www.360doc.com/content/20/0504/17/32196507_910187350.shtml&lt;/p&gt;

&lt;p&gt;《Making Sense of Smartphone Processors: The Mobile CPU/GPU Guide》&lt;/p&gt;

&lt;p&gt;https://www.techautos.com/2010/03/14/smartphone-processor-guide/&lt;/p&gt;

&lt;p&gt;《移动设备GPU架构知识汇总》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/112120206&lt;/p&gt;

&lt;p&gt;《针对移动端TBDR架构GPU特性的渲染优化》&lt;/p&gt;

&lt;p&gt;https://gameinstitute.qq.com/community/detail/123220&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Tile-based rendering》&lt;/p&gt;

&lt;p&gt;https://www.imaginationtech.com/blog/a-look-at-the-powervr-graphics-architecture-tile-based-rendering/&lt;/p&gt;

&lt;p&gt;《A look at the PowerVR graphics architecture: Deferred rendering》&lt;/p&gt;

&lt;p&gt;https://www.imaginationtech.com/blog/the-dr-in-tbdr-deferred-rendering-in-rogue/&lt;/p&gt;

&lt;p&gt;《深入GPU硬件架构及运行机制》&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/timlly/p/11471507.html&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484981&amp;amp;idx=1&amp;amp;sn=b5199f84ed73d28916dbb9143eeeb992&amp;amp;chksm=fc226332cb55ea2417b89e499284ab3a552ce591caba09d637912306503e4b457ed45c2e1d2d&amp;amp;token=1023740201&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Mon, 08 Nov 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/11/08/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B048</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/11/08/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B048</guid>
            </item>
        
            <item>
               <title>读书笔记(四十七) 《游戏引擎架构》#4 低阶渲染器（1）</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484964&amp;amp;idx=1&amp;amp;sn=99fd35baa5ad829cbae99753f14aa64b&amp;amp;chksm=fc226323cb55ea3598d74dd2508841f01e149176b2c857c54c077a2d8a3f493033ce8f57b56f&amp;amp;token=1534435063&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景：&lt;/h2&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了几遍《游戏引擎架构》后对引擎架构的理解又深入了些。&lt;/p&gt;

&lt;p&gt;近段时间有对引擎剖析的想法，正好借这书本对游戏引擎架构做一个完整分析。&lt;/p&gt;

&lt;p&gt;此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节。&lt;/p&gt;

&lt;p&gt;我将借助《游戏引擎架构》这本书、结合引擎源码和自己的经验，深入分析游戏引擎的历史、架构、模块，最后通过实践简单引擎开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;同时《游戏引擎架构》中部分知识太过陈旧的部分，会重新深挖后总结出自己的观点。&lt;/p&gt;

&lt;h2 id=&quot;概述&quot;&gt;概述：&lt;/h2&gt;

&lt;p&gt;本系列文章对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;时间库&lt;/li&gt;
  &lt;li&gt;自定义容器库&lt;/li&gt;
  &lt;li&gt;字符串散列库&lt;/li&gt;
  &lt;li&gt;内存管理框架&lt;/li&gt;
  &lt;li&gt;RTTI与反射模块&lt;/li&gt;
  &lt;li&gt;图形计算库&lt;/li&gt;
  &lt;li&gt;资产管理模块&lt;/li&gt;
  &lt;li&gt;低阶渲染器&lt;/li&gt;
  &lt;li&gt;剔除与合批模块&lt;/li&gt;
  &lt;li&gt;动画模块&lt;/li&gt;
  &lt;li&gt;物理模块&lt;/li&gt;
  &lt;li&gt;UI底层框架&lt;/li&gt;
  &lt;li&gt;性能剖析器的核心部分&lt;/li&gt;
  &lt;li&gt;脚本系统&lt;/li&gt;
  &lt;li&gt;视觉效果模块&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;本篇内容为列表中的第8个部分。&lt;/p&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文：&lt;/h2&gt;

&lt;p&gt;简单回顾下前文&lt;/p&gt;

&lt;p&gt;前文简单提了下C++编译过程、C++内存布局、文件内核读取原理、图形计算的常用库内容，以及大篇幅的引擎资产管理。&lt;/p&gt;

&lt;p&gt;引擎资产管理简单回顾下：&lt;/p&gt;

&lt;p&gt;首先，资产分为DCC资产、中间格式资产、引擎资产，这三者是从制作到导入到使用的顺序。&lt;/p&gt;

&lt;p&gt;其次，引擎资产分为元数据和引擎资产文件，引擎不仅需要资产也需要资产的配置数据。&lt;/p&gt;

&lt;p&gt;接着，引擎内存中的资产有资源映射关系，资产路径、资产GUID、资产实例、资产实例ID。&lt;/p&gt;

&lt;p&gt;最后，引擎通常都有资源包，资源包的格式通常都差不多，头信息（包括索引信息和数据概要信息）+数据文件，压缩可以是整体压缩也可以是每个独立数据单独压缩，通过解剖资源包的数据格式，我们可以知道如何去做差量更新。&lt;/p&gt;

&lt;p&gt;这里补充一点，引擎在读取资产时，与读取资产配置的原理是一样的，它可以有自己的数据格式也可以用标准格式，引擎通常使用自定义格式。通过区分读取不同类型的资产数据来实例化引擎内不同的资源对象，包括网格、材质、动画、粒子、节点、组件配置等。&lt;/p&gt;

&lt;h3 id=&quot;下面我们开始这篇内容&quot;&gt;下面我们开始这篇内容&lt;/h3&gt;

&lt;p&gt;本次内容会围绕GPU来写，从硬件架构到软件驱动再到引擎架构，目标是帮大家理解GPU硬件的运作原理，理解图形接口的架构，理解引擎低阶渲染器的架构。&lt;/p&gt;

&lt;h2 id=&quot;目录&quot;&gt;目录：&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;主板结构中的显卡&lt;/li&gt;
  &lt;li&gt;GPU功能发展史&lt;/li&gt;
  &lt;li&gt;GPU与CPU的差异&lt;/li&gt;
  &lt;li&gt;GPU硬件特点&lt;/li&gt;
  &lt;li&gt;图形驱动程序架构&lt;/li&gt;
  &lt;li&gt;引擎低阶渲染架构&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;主板结构中的显卡&quot;&gt;主板结构中的显卡&lt;/h2&gt;

&lt;p&gt;我们讨论GPU和显卡时经常混为一谈，严格来说GPU是显卡（Video card、Display card、Graphics card）最核心的部件。
显卡除了GPU，还有扇热器、通讯元件、与主板和显示器连接的各类插槽。&lt;/p&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;（主板结构图）&lt;/p&gt;

&lt;p&gt;早期计算机设备中的CPU都是由系统总线来与所有其他设备通信，当图形芯片需要跟CPU和内存大量交换数据时就出现了巨大瓶颈，于是人们设计了高速的北桥芯片和低速的南桥芯片，一来协调CPU、内存、图形设备之间的高速通信，二来专门处理磁盘、USB、键盘鼠标等低速设备。&lt;/p&gt;

&lt;p&gt;从上图中我们看到，离CPU越近，存取数据越快，最近的就是CPU内的组件了，其次是高速缓存总线，再是本地总线，接着是内存总线，最后是高速总线。
我们的GPU被安排在高速总线上，而且总线还有其他设备需要兼顾。因此从通信速度上来说，高速总线并不是那么快，这也是CPU与GPU的通信带宽通常成为瓶颈的其中一个原因。&lt;/p&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;（现实中主板结构图：图片来自网络）&lt;/p&gt;

&lt;p&gt;这里重点介绍下北桥芯片，一个主板上最重要的部分可以说就是主板的芯片组了，主板的芯片组一般由北桥芯片和南桥芯片组成，两者共同组成主板的芯片组。北桥芯片主要负责实现与CPU、内存、AGP接口之间的数据传输，同时还通过特定的数据通道和南桥芯片相连接。南桥芯片主要负责和IDE设备、PCI设备、声音设备、网络设备以及其他的I/O设备的通信。&lt;/p&gt;

&lt;p&gt;主板和CPU发展都现在，北桥芯片慢慢开始消失被集成了CPU中。往后发展，主板芯片组可以看成是以往南桥芯片组的加强版，CPU与主板芯片采用DMI总线进行通信。&lt;/p&gt;

&lt;p&gt;虽然从Lynnfield Core i5/i7开始把北桥集成到CPU上，但是其内部仍是采用QPI总线来通讯，而外部与主板芯片组通讯，其实就是以往主板上南桥与北桥通讯，采用的是DMI总线。因此不能说Lynnfield Core i5/i7是精简了，只是集成度更高而已。&lt;/p&gt;

&lt;h3 id=&quot;手机主板结构与pc不同&quot;&gt;手机主板结构与PC不同&lt;/h3&gt;

&lt;p&gt;以下图片均来源网络&lt;/p&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;图2&lt;/p&gt;

&lt;p&gt;图3&lt;/p&gt;

&lt;p&gt;图4&lt;/p&gt;

&lt;p&gt;手机的主板布局则不同，由于主板不能横向扩展，所以采用双或三层叠层的主板，GPU内嵌在处理器上与CPU同框，且它们周围布满了内存，这样通信更快。
同时由于空间小，功率和高速缓存大小也受到限制。&lt;/p&gt;

&lt;p&gt;多层主板虽然看上去设计巧妙，同时也将几个发热大户都贴在了一起，发热量和速度加倍。&lt;/p&gt;

&lt;p&gt;现代CPU都有一种过热自动降频或者关闭核心的保护措施，因此当玩王者或吃鸡等大型游戏时，就会因为发热而导致它更快的达到阈值温度而降频，这也是很多游戏优化后仍然会卡顿的主要原因。&lt;/p&gt;

&lt;p&gt;因此很多手机引入了水冷（热导管技术）CPU散热技术，用超大一块热导管来散热降温，让CPU不会因为过热而降频或锁核，从而能更长时间甚至全时运行在最大工作频率上，从而发挥出手机宣传时100%的性能。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484964&amp;amp;idx=1&amp;amp;sn=99fd35baa5ad829cbae99753f14aa64b&amp;amp;chksm=fc226323cb55ea3598d74dd2508841f01e149176b2c857c54c077a2d8a3f493033ce8f57b56f&amp;amp;token=1534435063&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考资料&quot;&gt;参考资料：&lt;/h2&gt;

&lt;p&gt;《主板北桥百科》&lt;/p&gt;

&lt;p&gt;https://baike.baidu.com/item/%E4%B8%BB%E6%9D%BF%E5%8C%97%E6%A1%A5/10252486?fr=aladdin&lt;/p&gt;

&lt;p&gt;《总线百科》&lt;/p&gt;

&lt;p&gt;https://baike.baidu.com/item/%E6%80%BB%E7%BA%BF/108823&lt;/p&gt;

&lt;p&gt;《iPhone 11/Pro Max内部结构示意图》&lt;/p&gt;

&lt;p&gt;http://www.itqianyan.com/m/view.php?aid=5479&lt;/p&gt;

&lt;p&gt;《拆解 iPhone 12 系列》&lt;/p&gt;

&lt;p&gt;https://www.igao7.com/news/202010/L2RP34HNtTEZ2yYC.html&lt;/p&gt;

&lt;p&gt;《iPhone 8和X拆解》&lt;/p&gt;

&lt;p&gt;https://m.iphonediule.com/hot/qiangxian.html&lt;/p&gt;
</description>
               <pubDate>Tue, 26 Oct 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/10/26/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B047</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/10/26/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B047</guid>
            </item>
        
            <item>
               <title>读书笔记(四十六) 《如何精彩演讲》#1 演讲的结构</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484952&amp;amp;idx=1&amp;amp;sn=aa014404eb8b71222820f1f3bbff437e&amp;amp;chksm=fc22631fcb55ea0981c20cab77f66da58a2697ab2e8a792f4b4c6ba15a3bc6b57dd2ba7309a1&amp;amp;token=1211671052&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;

&lt;p&gt;不知不觉看完了6本关于演讲的主题，按推荐顺序排列分别是《高效演讲》、《演讲与口才》、《即兴演讲》、《关键对话》、《TED演讲的力量》、《说话的艺术》。&lt;/p&gt;

&lt;p&gt;我希望自己能够对演讲有一个系统性的学习。于是自己在平常的生活和工作中实践了一段时间，前前后后加起来有两个月时间。&lt;/p&gt;

&lt;p&gt;目标是，改善平常的工作、生活中表达的流畅和清晰度，能让人感觉到舒服让人感觉有活力。同时在公众演讲上，希望自己能够很好的呈现自己的所思所想以及所知所学。&lt;/p&gt;

&lt;p&gt;因此开启这个演讲学习系列的总结文章，我将对这6本书的内容加上我的实践经验做一次全面的系统性的总结。&lt;/p&gt;

&lt;h1 id=&quot;内容&quot;&gt;内容&lt;/h1&gt;

&lt;p&gt;这一篇主要讲演讲的结构。&lt;/p&gt;

&lt;p&gt;一场演讲分为三个部分：&lt;/p&gt;

&lt;p&gt;第1个部分是，开场。
第2个部分是，核心内容。
第3个部分是，收尾。&lt;/p&gt;

&lt;p&gt;这三个部分都有各自的技巧，而此篇主要来说说具体的演讲内容中的其中一点，即演讲结构。&lt;/p&gt;

&lt;h2 id=&quot;首先来说一下为什么我们需要结构&quot;&gt;首先来说一下为什么我们需要结构？&lt;/h2&gt;

&lt;p&gt;我们在平常的表达过程当中，一般是不会去刻意在意逻辑。
因此，我们在表述过程当中通常是按照时间线来进行表述。
表达过程当中，通常只是说出了自己脑中不断冒出的想法。
在这样的表达下，对方是很难get到我们想要说的内容的。
一旦我们说的内容稍微多一些时，对方很容易进入完全听不懂我们在说了什么情况。&lt;/p&gt;

&lt;p&gt;那么如何才能让对方听清楚，听明白，听得懂我们在说什么呢？&lt;/p&gt;

&lt;p&gt;我们就得用演讲结构去规范我们的演讲内容，让我们的演讲内容能够更清晰的呈现在听众的脑中，让脑袋更容易去记住我们的要点以及中心思想。&lt;/p&gt;

&lt;h2 id=&quot;那什么才是好的演讲结构呢&quot;&gt;那什么才是好的演讲结构呢？&lt;/h2&gt;

&lt;p&gt;首先演讲的内容必须围绕着一个中心思想去讲。
如果中心思想太多，很难让听众记得住，就不会是一个好的演讲。
因此一个演讲最好只有一个中心思想。
中心思想最好是能引起观众共鸣的。
（在后面的内容中再讲如何让中心思想引起共鸣）&lt;/p&gt;

&lt;p&gt;其次你的演讲内容要有层次感。&lt;/p&gt;

&lt;p&gt;在我们表达时最好有意识的按简单顺序进行表达，这样听众们的大脑才能记得住。
比如第一第二第三；过去现在未来；昨天，今天，明天；最重要其次最后等等。&lt;/p&gt;

&lt;p&gt;除了这种层次方式，还有另一种方式，即故事方式。
（后面的文章中再解剖如何讲好一个故事）&lt;/p&gt;

&lt;p&gt;这里先简单介绍一下一个好故事设计要有4个部分，情景带入、遇到危机、冲突和转折、大好结局。&lt;/p&gt;

&lt;p&gt;除了设计故事内容还要注意好故事有4个要点，篇幅小、通俗易懂、简单容易记、有共鸣。&lt;/p&gt;

&lt;p&gt;接着你的演讲内容，最好以总分总的形式去部署。
即提出观点、剖析观点、总结观点。
在开头时阐述演讲要点，结尾时进行要点总结，中间内容分成三个论点，每个论点都由三个句子组成演讲结构。&lt;/p&gt;

&lt;p&gt;这样的演讲结构即“1-3-3-3-1”形式组成的结构，让人听了更舒服也更容易记得住。&lt;/p&gt;

&lt;p&gt;最后在你平时的讲话和沟通表达的过程中，有意识的将自己说话的内容以层次感的形式表现出来。
一开始你可以在讲话前先先打好附稿或准备稿，按123规律来讲，注意不要把第3点和第1点说重复了。
经过长时间的练习，你可以自如的应对各种各样的演讲模式以及各种各样的即兴发挥。&lt;/p&gt;

&lt;h2 id=&quot;那么怎样才能练好演讲结构呢&quot;&gt;那么怎样才能“练”好演讲结构呢？&lt;/h2&gt;

&lt;p&gt;这里给出三个要点。&lt;/p&gt;

&lt;p&gt;第一，每次讲话前先打好腹稿再开口。&lt;/p&gt;

&lt;p&gt;为每次的公众演讲以及平时的沟通交流都打好腹稿，这是一种非常好的练习方式。
这让你时时刻刻提醒自己，在讲话时要有层次感，将这种方法印刻在脑海里，形成肌肉记忆。
时间长了，你自然而然就有了说话的层次感，形成了强有力的肌肉记忆。&lt;/p&gt;

&lt;p&gt;第二，多上台，多在台上运用演讲层次结构。&lt;/p&gt;

&lt;p&gt;台上的感觉和台下的感觉是完全不同的。
在台上你会更加紧张，也更加容易出错，同时更加在意别人对你的看法，也更加害怕。
多上台，多在台上运用演讲的层次结构，会让你在面对陌生场合，在面对激烈、紧张、恐惧时，也能够自如的运用层次感肌肉记忆，让演讲内容流畅清晰的表达出来。&lt;/p&gt;

&lt;p&gt;第三，发现生活中的主题进行层次训练。&lt;/p&gt;

&lt;p&gt;生活和工作中，周围每天都会发生很多的事情，我们可以通过仔细观察我们生活当中发生的事情，叙述或表达我们的观点。
表达时运用层次结构去跟自己对话，这样的训练每天都可以进行，有助于我们更快更好的形成肌肉记忆。&lt;/p&gt;

&lt;h2 id=&quot;最后告诉大家如何获得练习反馈&quot;&gt;最后告诉大家，如何获得练习反馈。&lt;/h2&gt;

&lt;p&gt;这里有三个方法告诉大家。&lt;/p&gt;

&lt;p&gt;1.录制后自我复盘。&lt;/p&gt;

&lt;p&gt;我们演讲的反馈可以通过自己录制视频，录制音频后进行复听来复盘自己的演讲。&lt;/p&gt;

&lt;p&gt;2.通过他人获得评价。&lt;/p&gt;

&lt;p&gt;可以通过同事、朋友、家人、以及专门演讲圈的人对自己的评价来复盘自己的演讲，从而获得自己改进的方向。&lt;/p&gt;

&lt;p&gt;但要注意，别人对你的负面评价要小心。
通常那些不专业的人给你的评价，都是打击你自信心的一个常见途径。
我们需要的是鼓励，而不是打击。鼓励让我们更有动力前进。而打击则适得其反。&lt;/p&gt;

&lt;p&gt;3.通过专业演讲训练师来反馈。&lt;/p&gt;

&lt;p&gt;最好是通过专业的演讲导师来纠正你的演讲问题，从而给予你正确的演讲练习方式和修正方向。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484952&amp;amp;idx=1&amp;amp;sn=aa014404eb8b71222820f1f3bbff437e&amp;amp;chksm=fc22631fcb55ea0981c20cab77f66da58a2697ab2e8a792f4b4c6ba15a3bc6b57dd2ba7309a1&amp;amp;token=1211671052&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Sun, 17 Oct 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/10/17/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B046</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/10/17/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B046</guid>
            </item>
        
            <item>
               <title>思路探讨(六十六) 人和事</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484939&amp;amp;idx=1&amp;amp;sn=e3df251c9cb4733784e5df9ccd21e3ab&amp;amp;chksm=fc22630ccb55ea1a27f035c062122bf9e40a504d0dac7143b0a383b4f7dc729dc8596cc10958&amp;amp;token=254199611&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;有两周时间没有发文章了，最近有些忙，自己也遇到了瓶颈，正在突破。&lt;/p&gt;

&lt;p&gt;前几个月发现我的表达能力有些弱，表达逻辑混乱，表达时没有自信。&lt;/p&gt;

&lt;p&gt;所以呢，我特别花了一些精力去加强这方面的训练。&lt;/p&gt;

&lt;p&gt;自己做了一些训练，去演讲俱乐部练习演讲，去读书聚会的地方做演讲，以及做主持人的表达工作。&lt;/p&gt;

&lt;p&gt;我希望通过这样的锻炼，提升表达能力。也看了几本书，暂时还没有去总结它们，希望后面能够挤出时间来写些关于这个主题书的读书总结。&lt;/p&gt;

&lt;p&gt;为什么要去做沟通跟表达演讲的训练呢？我内心声音是，我要解决人与人之间的关系，人与人之间的沟通。&lt;/p&gt;

&lt;p&gt;前面10几年都在做练习解决事情的技巧，比如我要写好程序，写书，写文章，都是去总结一些解决事情的经验。&lt;/p&gt;

&lt;p&gt;不管是程序还是项目还是工作分配工作拆解，都是需要我们去解决事情。这是我前面12年做的事情。&lt;/p&gt;

&lt;p&gt;我最近领悟到，其实解决事情只占了30%的部分，剩余的70%的部分并不是靠解决事情来解决的，而是靠做人，解决人与人之间关系来搞定的。&lt;/p&gt;

&lt;p&gt;一个典型的例子，在工作当中如果我们跟上级相处不好的话，跟家人的关系不好，你即使把成绩做出来了，也得不到家人的肯定，和领导的赏识，这对我们的感受有巨大影响，甚至会有强烈的挫败感。&lt;/p&gt;

&lt;p&gt;因此现阶段的我认为，解决人与人之间的关系比解决事情更重要。&lt;/p&gt;

&lt;p&gt;最重要的其实不是事情本身，而是与事情有关的人，这些人才是关键。例如事情提出的人是领导，或与事件有关的人是同事，在他们提出问题时，先确立这是不是个事实。&lt;/p&gt;

&lt;p&gt;我相信大部分时候是事实，也可能是被夸大的事实。当我们去做事时，其实应该首先跟他们聊一聊，跟老板聊一聊，跟这些部门的人来聊一聊，这样的才能体会到对方的感受，以及看到对方究竟想要的。&lt;/p&gt;

&lt;p&gt;跟人联系、寒暄后，才能知道对方的情绪、想法、需要，这时再想办法来解决问题&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484939&amp;amp;idx=1&amp;amp;sn=e3df251c9cb4733784e5df9ccd21e3ab&amp;amp;chksm=fc22630ccb55ea1a27f035c062122bf9e40a504d0dac7143b0a383b4f7dc729dc8596cc10958&amp;amp;token=254199611&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Tue, 12 Oct 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/10/12/%E6%80%9D%E8%B7%AF%E6%8E%A2%E8%AE%A866</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/10/12/%E6%80%9D%E8%B7%AF%E6%8E%A2%E8%AE%A866</guid>
            </item>
        
            <item>
               <title>读书笔记(四十五) 《游戏引擎架构》#3 链接、图形计算库、资产管理模块</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484932&amp;amp;idx=1&amp;amp;sn=13c96ed5d6a24abe43d9a63a5e2aca23&amp;amp;chksm=fc226303cb55ea1527fead1e2d90b435a6eb911cb54e58ffbde95b674f1982859fa6a66f3837&amp;amp;token=1405609858&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景：&lt;/h1&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了好几遍《游戏引擎架构》后对引擎的架构感触颇深。&lt;/p&gt;

&lt;p&gt;近段时间对引擎剖析的想法也较多，正好借着书本对游戏引擎架构做一个完整分析。此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节，使得初学者也能很容易地理解其中的各种概念。&lt;/p&gt;

&lt;p&gt;我的目标是掌握游戏引擎架构知识，我的方法是借助《游戏引擎架构》这本书、结合引擎源码和自己的经验，深入分析游戏引擎的历史、架构、模块。最后通过实践简单引擎开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;由于《游戏引擎架构》此书的部分知识太浅或太过陈旧，所以不得不将部分知识重新深挖后总结出自己的观点。&lt;/p&gt;

&lt;h1 id=&quot;概述&quot;&gt;概述：&lt;/h1&gt;

&lt;p&gt;本章开始对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;时间库&lt;/li&gt;
  &lt;li&gt;自定义容器库&lt;/li&gt;
  &lt;li&gt;字符串散列库&lt;/li&gt;
  &lt;li&gt;内存管理框架&lt;/li&gt;
  &lt;li&gt;RTTI与反射模块&lt;/li&gt;
  &lt;li&gt;图形计算库&lt;/li&gt;
  &lt;li&gt;资产管理模块&lt;/li&gt;
  &lt;li&gt;低阶渲染器&lt;/li&gt;
  &lt;li&gt;剔除与合批模块&lt;/li&gt;
  &lt;li&gt;动画模块&lt;/li&gt;
  &lt;li&gt;物理模块&lt;/li&gt;
  &lt;li&gt;UI核心框架&lt;/li&gt;
  &lt;li&gt;性能剖析器的核心部分&lt;/li&gt;
  &lt;li&gt;脚本系统&lt;/li&gt;
  &lt;li&gt;视觉效果模块&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;本篇内容为列表中的6、7。&lt;/p&gt;

&lt;h1 id=&quot;正文&quot;&gt;正文：&lt;/h1&gt;

&lt;p&gt;简单回顾下前文，前面我们聊了时间库、自定义容器、字符串、内存管理这四个模块的技术原理和特点，它们都是大型软件架构所必备的模块，同时简单讲述了它们在Unreal和Unity中存在的特点。&lt;/p&gt;

&lt;h2 id=&quot;编译链接过程与内存布局&quot;&gt;编译链接过程与内存布局&lt;/h2&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;图2&lt;/p&gt;

&lt;p&gt;简单回顾一下C++编译过程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;源文件.cpp文件被编译成.o文件后由链接器链接成可执行文件或库文件。&lt;/li&gt;
  &lt;li&gt;库分为静态库和动态库。其中静态库为只是简单集合了.o文件，而动态库则是一个完整的编译、链接产物。&lt;/li&gt;
  &lt;li&gt;头文件.h文件不是编译的必需品，它只是包含在源文件中的声明文件。&lt;/li&gt;
  &lt;li&gt;Linux的so和Windows的DLL虽然都是ELF文件格式，但最终格式差了很多，因此不能互相使用。&lt;/li&gt;
  &lt;li&gt;编译器开启优化后（一般是Release时），会优化代码，包括内联、调换代码顺序、更改代码为最优等。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体可以看我前面写的《链接、装载与库》&lt;/p&gt;

&lt;p&gt;http://luzexi.com/2021/06/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B029&lt;/p&gt;

&lt;p&gt;也同时回顾下C++内存布局：
    1.C++内存布局中有，类、变量、内存对齐、虚表、RTTI
    2.类和结构在C++中差异较少
    3.每个变量内存占用量不同，int（32bit）、short int（16bit）、long long int（64bit）、float（4bit）、double（8bit）、char（8bit）等
    4.默认按4字节（32bit）对齐，不足4字节的编译器会补齐
    5.虚函数或虚继承的类有虚表及虚表指针
    6.没有RTTI的情况下，虚表只有当前的虚函数指针
    7.有RTTI的情况下，虚表中有type_info指针
    8.有RTTI的情况下，虚指针指向虚表中的第二个格子，即虚指针，第一个格子为type_info指针&lt;/p&gt;

&lt;p&gt;具体可以看我前面写的《深度探索C++对象模型-总结》&lt;/p&gt;

&lt;p&gt;http://luzexi.com/2020/11/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B014&lt;/p&gt;

&lt;h2 id=&quot;图形计算库&quot;&gt;图形计算库&lt;/h2&gt;

&lt;p&gt;图形库涉及内容有图形元素和运算两种，它们分别包含，点、矢量、矩阵、四元数、图形对象以及相关的运算（包括SIMD）。&lt;/p&gt;

&lt;p&gt;下面我们来介绍一下：&lt;/p&gt;

&lt;p&gt;矢量运算包括：矢量加减法、模、归一化、点积、投影、叉积、线性插值等。&lt;/p&gt;

&lt;p&gt;矩阵有，单位矩阵、转置矩阵、逆矩阵、齐次坐标。矩阵运算，包括矩阵乘法、加减法、除、与、或等。&lt;/p&gt;

&lt;p&gt;四元数运算包括，四元数乘法、加减法、逆运算、旋转矢量、等价的四元数矩阵、旋转线性插值、球面线性插值等。&lt;/p&gt;

&lt;p&gt;各图形转换操作，包括旋转、缩放、投影、平移、LookAt等。&lt;/p&gt;

&lt;p&gt;图形对象包括，直线、线段、球体、平面、包围盒、平截头体、圆形、矩形、三角形等。&lt;/p&gt;

&lt;p&gt;这部分内容可以在Unreal的Engine\Source\Runtime\Core\Public\Math中找到。&lt;/p&gt;

&lt;h3 id=&quot;下面简单介绍下simd&quot;&gt;下面简单介绍下SIMD：&lt;/h3&gt;

&lt;p&gt;硬件加速SIMD运算（Single instruction multiple data），单指令多数据，是指，现代微处理器用一个指令并行地对多个数据执行数学运算，它能帮助我们加速运算。&lt;/p&gt;

&lt;p&gt;游戏引擎中最常用的是SSE模式（Streaming SIMD extensions，SSE），它包裹了4个32位float值，它们都被打包进了一个128位寄存器。&lt;/p&gt;

&lt;p&gt;单个指令可对4对浮点数进行并行运算，如加法或乘法。在计算四元矢量和4x4矩阵相乘时特别有用。&lt;/p&gt;

&lt;p&gt;需要注意的是
1.由于在浮点运算器和SSE寄存器之间传输数据很糟糕，所以不要混合使用普通浮点数和SIMD运算，这样会使得CPU整个指令执行流水线停顿，浪费CPU周期。
2.在VS中用SIMD数据类型__m128声明的临时变量或参数，编译器通常会把它们直接置于SSE寄存器中而非内存栈。
3.动态分配SIMD结构时要注意内存按16字节对齐&lt;/p&gt;

&lt;h2 id=&quot;资产管理模块&quot;&gt;资产管理模块&lt;/h2&gt;

&lt;p&gt;先说文件系统&lt;/p&gt;

&lt;h3 id=&quot;文件系统&quot;&gt;文件系统&lt;/h3&gt;

&lt;p&gt;游戏引擎中的文件系统相对比较简单：
1.每个平台的路径、API不同，对平台需要做些封装。
2.文件读取的阻塞方式分同步和异步。&lt;/p&gt;

&lt;p&gt;其中同步需要阻塞当前进程来等待IO，异步则通过分线程阻塞等待IO，其两者原理是一样，都是调用内核读取文件且都需要等待IO。&lt;/p&gt;

&lt;p&gt;以前写过一篇关于操作系统内核中文件操作的底层原理《链接、装载与库 - 内核运行库》大家可以参考下。&lt;/p&gt;

&lt;p&gt;这里顺便简单回顾一下文件内核原理：&lt;/p&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;图2&lt;/p&gt;

&lt;p&gt;这两张图清晰的表达了内核文件的读写原理：
1.操作系统内核中对每个打开的文件都有个内核对象
2.所有文件内核对象都被集中索引到一个数组中，称为文件打开表
3.文件表数组前三个元素填充的是stdin、stdout、stderr这三个内核对象
4.为了增加读写效率，内核已经实现了文件读取缓冲，会读取一段一段的读取
5.读取步骤，用户程序先开辟一段内存，内核程序则利用缓冲读取，不足时多次读取，结束时返回数据。&lt;/p&gt;

&lt;h2 id=&quot;资产管理器&quot;&gt;资产管理器&lt;/h2&gt;

&lt;p&gt;早前很多引擎都有独立的资产管理器，它被制作成了一个独立的软件，专门用于管理游戏资产，包括网格、材质、纹理、着色器程序、动画、音频、配置等。&lt;/p&gt;

&lt;p&gt;资产管理器本身是一个具有清晰设计、统一、中心化的子系统，负责管理游戏中用到的所有类型的资产，只是现代大多引擎已经将资产管理整合到引擎编辑器中。&lt;/p&gt;

&lt;h3 id=&quot;资产管理器解决了什么问题答案是&quot;&gt;资产管理器解决了什么问题？答案是：&lt;/h3&gt;
&lt;p&gt;1.资产预览，资产在引擎中快速预览，并对不同资产类型区分展示。
2.资产查找，通过查找功能快速查找到资产。
3.资产组合，通过组合信息管理资产各个依赖。
    资产通常组合在一起使用，因此组合信息是资产管理的一部分。
    加载时需要依赖多个资产，引擎通过资产元数据将这些信息保存下来。
4.资产转换，将外部资产导入到引擎中使用。
    外部资产需要经过一定的转换才能在引擎中使用。
    引擎通常有自己的资产导入系统，将外部资产转换为自身使用的数据格式。
5.运行时资产管理，引擎向应用层提供加载和释放资产接口，并管理已加载资产对象。
    资产的加载和释放，引擎需要提供给应用层加载和释放资产的接口，引擎本身也需要对这些资产进行管理。&lt;/p&gt;

&lt;p&gt;下面我们从资产管道、资产类型、运行时资产管理、元文件、资产包，四个方面介绍下引擎中的资产管理器。&lt;/p&gt;

&lt;h3 id=&quot;资产管道&quot;&gt;资产管道：&lt;/h3&gt;

&lt;p&gt;每个资产都需要通过资产管道才能最终被游戏引擎所使用。每个资产管道的始端都是DCC原生格式的源资产（Maya的.ma或.mb、3DMax的.max或.obj、Photoshop的.psd文件等）。资产经过资产管道的导出器、资产编译器、资产链接器，最终生成了游戏引擎可以使用的数据格式。&lt;/p&gt;

&lt;p&gt;第一步，通常DCC工具需要撰写自定义插件（大都已提供现成统一的插件），把DCC里的数据导出为某种中间格式（例如.fbx格式），一般DCC工具都会提供接口或脚本供程序员写导出插件。
第二步，中间格式数据仍然需要经过一定的转换才能被引擎使用，因此引擎通过资产编译器转换中间格式。
第三步，通常多个资产组合后才成为一个完整资产，例如网格文件、材质文件、动画文件、贴图文件等，它们经过资产链接器连接后组合成为完整的资产。&lt;/p&gt;

&lt;h3 id=&quot;资产类型&quot;&gt;资产类型：&lt;/h3&gt;

&lt;p&gt;资产经过资产管道后会生成相应的资产相关文件，包括：
1.资产源文件
2.资产配置文件（元数据文件）
3.资产目标文件&lt;/p&gt;

&lt;p&gt;资产也分为外部和内部资产，外部资产由DCC导出，内部资产则通过引擎生成，例如材质球、动画控制器、蓝图、粒子等。
资产配置文件记录了资产在引擎中的配置信息和依赖关系。
资产目标文件是引擎根据资产源文件和配置信息生成的符合引擎使用格式的资产文件。&lt;/p&gt;

&lt;p&gt;生成资产元数据文件和目标文件的目的是：
1.在保留原资产文件格式的前提下，生成引擎能使用的格式文件。
2.便于引擎获取每个资产文件的配置及依赖关系。
3.便于引擎统一管理资产，管理资产的目的是，提供例如建立资产数据库、查找、同步、打包等功能。&lt;/p&gt;

&lt;p&gt;Unity引擎使用Mate文件存储资产的配置和依赖关系。当引擎导入资产时就会生成相应的Mate文件，并根据这个Mate文件在Library文件夹下生成目标文件，同时根据修改的Mate文件配置来调整或重新生成目标文件。&lt;/p&gt;

&lt;p&gt;Unreal引擎则稍稍有些不同，资产源文件仍然会转换成资产目标文件，只是它把资产配置文件和资产目标文件合并在一个uasset文件中，这样导入后源文件就不再需要。如果你想要获取元数据，UE4也提供了python接口和蓝图接口。&lt;/p&gt;

&lt;h3 id=&quot;运行时资产管理&quot;&gt;运行时资产管理：&lt;/h3&gt;

&lt;p&gt;运行时资产管理通常包括，资产对象管理、资产对象映射管理。&lt;/p&gt;

&lt;p&gt;资产从加载到实例化，在引擎内部必须有一个有效的管理机制，其职责为：
1.同一份资源只会存在一个副本
2.管理资产生命周期，确保不需要时卸载
3.处理复合资产，复合资产依赖多个资产组合而成。
4.维护引用，确保复合资产在内存中的引用关系正确。
5.资产接口，提供资产载入与卸载接口，包含同步和异步的载入方式。&lt;/p&gt;

&lt;p&gt;基于这五个职责，资产在运行时的引擎中必须拥有信息为，资产地址，资产对象，资产对象ID（运行时ID）。&lt;/p&gt;

&lt;p&gt;部分引擎对每个资源都配备了资产唯一ID（GUID），让资产地址与资产唯一ID有绑定关系，这使得引擎在资产迁移时能发挥更好的作用。&lt;/p&gt;

&lt;p&gt;资产对象映射管理包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;资产路径与资产唯一ID的映射关系&lt;/li&gt;
  &lt;li&gt;资产唯一ID与资产对象ID的映射关系&lt;/li&gt;
  &lt;li&gt;资产对象ID与资产对象的映射关系&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有了这些映射关系，引擎就能通过资产路径查找到资产对象，从而保证不重复加载，并维护好各资产之间的引用关系。&lt;/p&gt;

&lt;p&gt;在UE4和Unity上也同样做了这种类型的资产映射关系的管理。&lt;/p&gt;

&lt;h3 id=&quot;资产加载和卸载&quot;&gt;资产加载和卸载&lt;/h3&gt;

&lt;p&gt;资产加载与卸载接口必不可少，通常引擎都会定制一些依赖数据，例如前面提到的资产配置文件（也可以说是资产元数据），多个资产组合达成资源包时，资源包之间的依赖关系也同样需要有数据来维护。&lt;/p&gt;

&lt;p&gt;引擎都会有资源加载的统一接口，包括加载和卸载，同步和异步，资源包和非资源包形式。&lt;/p&gt;

&lt;p&gt;图&lt;/p&gt;

&lt;p&gt;引擎的资产加载和卸载框架各引擎之间稍有不同，不过总体差不了太多，或直接IO调用，或用开启线程后做IO调用，然后通过存储资产对象与映射关系来搭建资产加载和释放的框架，由于各个引擎接口都不一样，不做详细介绍。&lt;/p&gt;

&lt;h3 id=&quot;资源包数据格式&quot;&gt;资源包数据格式&lt;/h3&gt;

&lt;p&gt;通常引擎都会提供类似AssetBundle的资源组合包，便于外部资源下载和更新。&lt;/p&gt;

&lt;p&gt;在UE4和Unity上都有相同的功能，只是命名不同，UE4为Pak，Unity为AssetBundle。&lt;/p&gt;

&lt;p&gt;Pak或AssetBundle中存放着多个复合资产，通过引擎接口加载指定资产。&lt;/p&gt;

&lt;p&gt;图&lt;/p&gt;

&lt;p&gt;为了方便理解，我把数据格式从头往下画。实际中的Pak和AssetBundle数据格式要倒一下，头信息在最底部，资产数据块在最前头。&lt;/p&gt;

&lt;p&gt;资源包数据格式：
1.文件头信息与数据块拆分
2.可整体压缩或部分压缩
3.通过依赖配置加载外部资源包&lt;/p&gt;

&lt;p&gt;资源包可以通俗的认为是一个多文件的组合，它可以自己做压缩，也可以让资产压缩后再组合成文件。&lt;/p&gt;

&lt;p&gt;通常资源包的数据格式由文件头和数据块两部分组成。文件头信息中包含了资产信息和偏移量，通过加载文件头，就能知道资产在文件中的位置、类型、名称、大小等。&lt;/p&gt;

&lt;p&gt;这种资产组织方式使得我们通过差量方式更新资产成为可能。&lt;/p&gt;

&lt;p&gt;图&lt;/p&gt;

&lt;p&gt;根据资源包的数据格式特点，可以规划差量更新步骤：
1.打差量包，包头保持完整，数据块则只加入差量部分
2.下载差量包
3.合并两个资源包文件
4.合并时使用差量包头作为文件头
5.合并时提取原资源包中的不更新部分和差量包中的更新部分放入新的资源包文件中
6.更新完成生成新的资源文件&lt;/p&gt;

&lt;p&gt;这部分内容Unity和Unreal并无太大差异。&lt;/p&gt;

&lt;h2 id=&quot;资产规范&quot;&gt;资产规范&lt;/h2&gt;

&lt;p&gt;资产规范目标：
规范命名，防止程序报错，方便自动化检测，方便筛选找寻。
优化内容，每个项目资源都应该有存在目的。
减短路径，路径简短易寻。&lt;/p&gt;

&lt;p&gt;图&lt;/p&gt;

&lt;p&gt;网上有同学分享的很详细了，这里就不赘述，参考《UE4工程规范》：&lt;/p&gt;

&lt;p&gt;https://github.com/skylens-inc/ue4-style-guide/blob/master/README.md#12-%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B%E8%A1%A8-&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料：&lt;/h3&gt;

&lt;p&gt;《游戏引擎架构》叶劲峰 译&lt;/p&gt;

&lt;p&gt;《游戏引擎原理与实践》 程东哲 著&lt;/p&gt;

&lt;p&gt;《vmath》&lt;/p&gt;

&lt;p&gt;https://github.com/BlackMATov/vmath.hpp#Matrix-Transform-3D&lt;/p&gt;

&lt;p&gt;《从虚函数表到RTTI》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/150579874&lt;/p&gt;

&lt;p&gt;《虚幻引擎4文档》&lt;/p&gt;

&lt;p&gt;https://docs.unrealengine.com/4.27/zh-CN/Basics/AssetsAndPackages/AssetMetadata/&lt;/p&gt;

&lt;p&gt;《UE4工程规范》&lt;/p&gt;

&lt;p&gt;https://github.com/skylens-inc/ue4-style-guide/blob/master/README.md#12-%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B%E8%A1%A8-&lt;/p&gt;

&lt;p&gt;《链接、装载与库 - 内核运行库》&lt;/p&gt;

&lt;p&gt;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484809&amp;amp;idx=1&amp;amp;sn=89091ecce47229ebf10e4855c0ccceca&amp;amp;chksm=fc22608ecb55e998cdff3952057e6d1c6f099463797d57458ec1f4d37a432d46dc3c5bfcfbb5&amp;amp;token=557108361&amp;amp;lang=zh_CN#rd&lt;/p&gt;

&lt;p&gt;《链接、装载与库 - 静态链接》&lt;/p&gt;

&lt;p&gt;http://luzexi.com/2021/06/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B029&lt;/p&gt;

&lt;p&gt;《深度探索C++对象模型-总结》&lt;/p&gt;

&lt;p&gt;http://luzexi.com/2020/11/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B014&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484932&amp;amp;idx=1&amp;amp;sn=13c96ed5d6a24abe43d9a63a5e2aca23&amp;amp;chksm=fc226303cb55ea1527fead1e2d90b435a6eb911cb54e58ffbde95b674f1982859fa6a66f3837&amp;amp;token=1405609858&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Tue, 28 Sep 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/09/28/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B045</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/09/28/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B045</guid>
            </item>
        
            <item>
               <title>读书笔记(四十四) 《心流》#3 专注时人们的表现</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484913&amp;amp;idx=1&amp;amp;sn=9589d58b41c85d01d1aaa68872260539&amp;amp;chksm=fc2260f6cb55e9e08222aa5096a65949a05c98edc73eb005e289e11010c68a4816ed35a50035&amp;amp;token=134603822&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景：&lt;/h1&gt;

&lt;p&gt;在2020年初就买了《心流》，直到2021年1月才读完第一遍，读完的那一刻，我知道我又打开了一个新世界。&lt;/p&gt;

&lt;p&gt;不得不承认就是这本书让我的专注力提高了整整一个级别，这是最让我非常兴奋的地方。而我只是读了第一遍，吸收不到10%的结果，可想而知它的魔力有多么强大。&lt;/p&gt;

&lt;p&gt;于是决定好好回顾总结一下整本书，便开启了第二遍、第三遍的读书之旅。&lt;/p&gt;

&lt;p&gt;读此书的起源是由于我在平时工作、交流、学习时，常常难以集中注意力，因此带着好奇心阅读了这本书。&lt;/p&gt;

&lt;p&gt;我猜大家应该也和我一样，常常为自己难以击中注意力而烦恼，其实我们都希望自己在专注做一件事情的时候能够拥有更高的效率。&lt;/p&gt;

&lt;p&gt;本书的核心是心流的最优体验，简单来说就是“如何让你高兴地专注”。&lt;/p&gt;

&lt;p&gt;作者想表达的是，当你的心流会的体验最佳时，也是学习效率最高的时候，同时也是你最幸福的时刻。&lt;/p&gt;

&lt;p&gt;书本中讲到了关于学习、工作、运动、社交等各方面的心流原理和技巧，我作为知识的搬运工，做了一些精炼和总结的工作。&lt;/p&gt;

&lt;h1 id=&quot;正文&quot;&gt;正文：&lt;/h1&gt;

&lt;p&gt;简单回顾下前文：&lt;/p&gt;

&lt;p&gt;前文我们说了专注力能带给我们幸福，生活让我们感到痛苦的一个重要原因是人们欲望无法填满，我们生活在这个社会被这个的规则所牵动，已经沦为社会的奴隶，在这种环境培养独立意识非常重要同时也非常艰难。&lt;/p&gt;

&lt;h2 id=&quot;控制意识能改善体验的品质&quot;&gt;控制意识能改善体验的品质&lt;/h2&gt;

&lt;p&gt;想要控制意识，首先要了解它。那么意识究竟是如何运作的呢？&lt;/p&gt;

&lt;p&gt;“意识”简单来说就是，某些我们能感觉到的，且有能力引导其方向的东西，诸如情绪、感觉、思想、企图等。&lt;/p&gt;

&lt;p&gt;首先，一个人可以不管外界不管发生什么事，只靠改变意识的内涵，使自己快乐或悲伤。&lt;/p&gt;

&lt;h3 id=&quot;其次意识的力量可以把无助的境况转变为有机会反败为胜的挑战&quot;&gt;其次，意识的力量可以把无助的境况转变为有机会反败为胜的挑战。&lt;/h3&gt;

&lt;p&gt;我们口中所说的奋斗不辍、克服万难的毅力，它不但是成功的要素，也是享受人生的不二法门。&lt;/p&gt;

&lt;h3 id=&quot;最后控制意识需要毅力而培养毅力应该从建立意识的秩序控制感觉与思想着手且最好不要企图走捷径&quot;&gt;最后，控制意识需要毅力，而培养毅力应该从建立意识的秩序、控制感觉与思想着手，且最好不要企图走捷径。&lt;/h3&gt;

&lt;p&gt;瑜伽行者就是控制意识的高手，但跟任何高手一样，他们下了多年功夫苦练，一刻都不懈怠。&lt;/p&gt;

&lt;p&gt;唯有这样投注所有时间与心力，提升操纵内在体验的技巧，方能成为专家。&lt;/p&gt;

&lt;h2 id=&quot;意识的极限&quot;&gt;意识的极限&lt;/h2&gt;

&lt;p&gt;生活中有太多的事情需要我们的注意，但我们顶多同时应付七组资讯，诸如分辨声音、影像、情绪或思想中可辨识的弦外之音等。&lt;/p&gt;

&lt;p&gt;大多数人在每天的三分之一的闲暇时间里，都尽可能避免用脑子，这段空档一半以上都在电视前、杂志、抖音、微信聊天、看app资讯上消磨掉。&lt;/p&gt;

&lt;p&gt;因此对于我们来说，准许哪些资讯进入意识就显得格外重要，这实际上，就决定了我们生活的内涵与品质。&lt;/p&gt;

&lt;h3 id=&quot;神经系统在特定时间内能处理的资讯极为有限每次只能认知和回应一定数量的事件而新涌进来的会把旧的挤掉所以思绪必须井然有序否则就会混乱&quot;&gt;神经系统在特定时间内能处理的资讯极为有限，每次只能认知和回应一定数量的事件，而新涌进来的会把旧的挤掉。所以思绪必须井然有序，否则就会混乱。&lt;/h3&gt;

&lt;h2 id=&quot;注意力是无价的资源&quot;&gt;注意力是无价的资源&lt;/h2&gt;

&lt;p&gt;注意力再强大也无法超越我们前面谈到的限制，它只能在一定时间内处理一定数量的资讯。&lt;/p&gt;

&lt;p&gt;从记忆库中取出资讯，理解、比较、评估，然后做决定，都需要心灵有限的资讯处理能力。&lt;/p&gt;

&lt;h3 id=&quot;控制意识最明显的指标就是能随心所欲地集中注意力不因任何事情而分心若能做到这一点就能在日常生活中找乐趣&quot;&gt;控制意识最明显的指标就是能随心所欲地集中注意力，不因任何事情而分心。若能做到这一点，就能在日常生活中找乐趣。&lt;/h3&gt;

&lt;p&gt;作者举例一位学员E女士：&lt;/p&gt;

&lt;p&gt;她无时无刻不在写作、解决问题、阅读报纸、翻阅当日行程，或只是提出问题，仔细观察周遭事物，并计划下一步的工作。&lt;/p&gt;

&lt;p&gt;她只花很少的时间在日常例行公事上。&lt;/p&gt;

&lt;p&gt;她每天都会抽出时间来位心灵充电，如在湖畔伫立15分钟，闭上眼睛让阳光洒在脸上，或牵着狗在镇外山坡上散步。&lt;/p&gt;

&lt;p&gt;她能充分控制注意力，可以随时把意识关闭，打个盹，然后恢复精神。&lt;/p&gt;

&lt;p&gt;另一位学员R先生：&lt;/p&gt;

&lt;p&gt;说话时目光深邃；听人说话时，往往从多方面分析对话的话语。&lt;/p&gt;

&lt;p&gt;一般人视为理所当然的事常令他感到困惑；在用原创但十分贴切的方式详加分析前，他绝不让任何事轻易溜走。&lt;/p&gt;

&lt;p&gt;R先生尽管不断地磨炼知性，外表却给人一种沉着、宁静的感觉。他似乎永远能察觉到周遭最微小的变动。&lt;/p&gt;

&lt;p&gt;他注意一件事，目的不在于改善或批判它，只要能够观察和了解事实，并表达自己的看法，他就心满意足了。&lt;/p&gt;

&lt;p&gt;R先生不像E女士那样对社会造成立刻的冲击，但他的意识同样复杂而有条理。&lt;/p&gt;

&lt;p&gt;他把注意力尽可能延伸，跟周遭的世界密切结合起来。&lt;/p&gt;

&lt;p&gt;跟E女士一样，他也能充分享受人生。&lt;/p&gt;

&lt;h2 id=&quot;像探照灯那样集中注意力&quot;&gt;像探照灯那样集中注意力&lt;/h2&gt;

&lt;p&gt;大多数人都不能像E女士或R先生那样，把有限的注意力像探照灯一般集中成一道光束，而是任它毫无章法的散开。&lt;/p&gt;

&lt;h3 id=&quot;这说明生活中人们应用注意力的方式足以决定人生的外观与内涵从而反应人们为人处世的现象&quot;&gt;这说明生活中人们应用注意力的方式足以决定人生的外观与内涵，从而反应人们为人处世的现象。&lt;/h3&gt;

&lt;p&gt;例如，在同一个宴会上，外向的人热衷于与人交际；成就不凡的人寻求有用的商界人脉；偏执狂则随时警惕，怕碰到危险等等。&lt;/p&gt;

&lt;p&gt;每种人都有自己的注意力探照灯，这也是不同人之间的差异。&lt;/p&gt;

&lt;p&gt;实际上工作生活中，我们每个职业都在训练自己的集中注意力的方式，例如，软件工程师会更多关注计算机软件运行过程，销售人员更关注人与人之间的交流与谈判，银行家更专注财务状况，运动员更关注动作协调与身体素质等等等等。&lt;/p&gt;

&lt;p&gt;这种探照灯一样的集中方式正是他们在专业领域中学会了如何抓住领域中易受忽略的讯号。&lt;/p&gt;

&lt;h2 id=&quot;内在失序与精神熵&quot;&gt;内在失序与精神熵&lt;/h2&gt;

&lt;p&gt;内在失序的意思是，外来资讯跟当下的意图发生冲突，使我们分心，使我们无法为现实意图而努力，造成对意识极为不利的影响。&lt;/p&gt;

&lt;p&gt;作者举了一个例子，胡里欧破轮胎的故事。他的汽车轮胎破了，于是他一大早小心翼翼地把车开到加油站，把轮胎的气打满，又尽快开到工厂。下班时轮胎气又漏光了，他再到工厂附近加油站打满气后再开回家。胡里欧如法炮制了三天，一整天他都在担心，今晚是否能回家以及明天是否能到岗。这使得他无法专心工作，情绪也变得烦躁不安。&lt;/p&gt;

&lt;h3 id=&quot;每当周围的环境对意识的目标构成威胁就会发生内在的失序现象这称为精神熵&quot;&gt;每当周围的环境对意识的目标构成威胁，就会发生内在的失序现象，这称为精神熵。&lt;/h3&gt;

&lt;h3 id=&quot;它会导致自我解体使效率大打折扣&quot;&gt;它会导致自我解体，使效率大打折扣。&lt;/h3&gt;

&lt;h3 id=&quot;这种状况若持续太久对自身造成严重的损害使自我再也不能集中注意力实现任何目标&quot;&gt;这种状况若持续太久，对自身造成严重的损害，使自我再也不能集中注意力实现任何目标。&lt;/h3&gt;

&lt;h2 id=&quot;井然有序的意识&quot;&gt;井然有序的意识&lt;/h2&gt;

&lt;p&gt;精神熵的反面就是最优体验。&lt;/p&gt;

&lt;h3 id=&quot;当我们专注于资讯与目标时精神能量就会源源不断涌上来这时我们就没有了忧虑情绪也不再猜疑自己的能力&quot;&gt;当我们专注于资讯与目标时，精神能量就会源源不断涌上来，这时我们就没有了忧虑情绪，也不再猜疑自己的能力。&lt;/h3&gt;

&lt;p&gt;我们不再怀疑自己，也是因为我们得到了明确的鼓励，你做的很好。这种积极的反馈，强化了自我，使我们能投入更多的注意力，照顾内心与外在环境的平衡。&lt;/p&gt;

&lt;p&gt;作者举了一个例子。麦德林的工作是装配线上的工人，每天要重复600次的工作，像这样枯燥的工作，他做了五年，但还是觉得很快乐。因为他对工作的态度跟一名奥运选手差不多，常常思索如何打破纪录。他像外科医生一般一丝不苟的设计工具的安放顺序和每一步动作，经过五年的努力，他最好的成绩是28秒装配完一个单元。&lt;/p&gt;

&lt;p&gt;心流的最优体验出现时，一个人可以投入全部的注意力以求实现目标。没有失序现象需要整顿，自我也没有受到任何威胁，因此不需要分心防卫。&lt;/p&gt;

&lt;h3 id=&quot;一个人若能充分掌控意识尽可能创造心流体验生活品质势必会提高&quot;&gt;一个人若能充分掌控意识，尽可能创造心流体验，生活品质势必会提高。&lt;/h3&gt;

&lt;p&gt;在心流中，我们是精神能量的主宰，无论做什么事，都能使意识更有秩序。&lt;/p&gt;

&lt;p&gt;一位攀岩专家简要说明了自己的心流感觉：&lt;/p&gt;

&lt;h3 id=&quot;越来越完美的自我控制产生一种痛快的感觉&quot;&gt;越来越完美的自我控制，产生一种痛快的感觉。&lt;/h3&gt;

&lt;h3 id=&quot;你不断逼迫身体发挥所有极限直到全身隐隐作痛&quot;&gt;你不断逼迫身体发挥所有极限，直到全身隐隐作痛。&lt;/h3&gt;

&lt;h3 id=&quot;然后你会满怀敬畏地回顾自己回顾你所做的一切&quot;&gt;然后你会满怀敬畏地回顾自己，回顾你所做的一切。&lt;/h3&gt;

&lt;h3 id=&quot;那种佩服的感觉简直无法形容&quot;&gt;那种佩服的感觉，简直无法形容。&lt;/h3&gt;

&lt;h3 id=&quot;它带给你一种狂喜一种自我满足&quot;&gt;它带给你一种狂喜，一种自我满足。&lt;/h3&gt;

&lt;h3 id=&quot;只要在这种战役中战胜过自己人生其他战场的挑战也就变得容易多了&quot;&gt;只要在这种战役中战胜过自己，人生其他战场的挑战也就变得容易多了。&lt;/h3&gt;

&lt;p&gt;其实战斗中对抗的不是自己，而是使意识失序的精神熵。&lt;/p&gt;

&lt;h3 id=&quot;体验过心流的人都知道那份深沉的快乐是严格的自律集中注意力换来的&quot;&gt;体验过心流的人都知道，那份深沉的快乐，是严格的自律、集中注意力换来的。&lt;/h3&gt;

&lt;h2 id=&quot;独特性与复杂性的完美结合&quot;&gt;独特性与复杂性的完美结合&lt;/h2&gt;

&lt;p&gt;复杂性是由两种广泛的心理过程造成的，一种是独特化，另一种是整合。&lt;/p&gt;

&lt;p&gt;1.独特化是把自己与他人区分开来，朝独一无二的方向发展。&lt;/p&gt;

&lt;p&gt;2.整合则恰好相反，是借着超越自我的观念和实体与他人连结。&lt;/p&gt;

&lt;p&gt;独特化和整合，两者都是使不可或缺的。如果独特化而不整合，体系就会出现一片混乱，反之也是一样。&lt;/p&gt;

&lt;h3 id=&quot;当我们只有独特化的自我时虽然也能获得极高的成就但会陷入以自我为中心的危险&quot;&gt;当我们只有独特化的自我时，虽然也能获得极高的成就，但会陷入以自我为中心的危险。&lt;/h3&gt;

&lt;h3 id=&quot;同样的如果一个人的自我若是完全建立在整合上固然也能有良好的人际关系和安全感却缺乏独立的个性&quot;&gt;同样的，如果一个人的自我若是完全建立在整合上，固然也能有良好的人际关系和安全感，却缺乏独立的个性。&lt;/h3&gt;

&lt;h3 id=&quot;只有一个人把精神能量平均投注在这两方面既不过分自私也不盲从才算达到自我所追求的复杂性&quot;&gt;只有一个人把精神能量平均投注在这两方面，既不过分自私，也不盲从，才算达到自我所追求的复杂性。&lt;/h3&gt;

&lt;p&gt;复杂的自我能够成功地融合这两种乍看矛盾的过程。心流体验会使自我变得比过去更复杂，这可以说是一种成长。&lt;/p&gt;

&lt;h3 id=&quot;我们在克服挑战时必然会使我们变得更有能力和技巧因此心流就是经由这种过程加深自我独特化的当我们每经历一次心流后我们就会变得更独特更难预测并拥有非凡的技能&quot;&gt;我们在克服挑战时，必然会使我们变得更有能力和技巧，因此心流就是经由这种过程加深自我独特化的。当我们每经历一次心流后，我们就会变得更独特，更难预测，并拥有非凡的技能。&lt;/h3&gt;

&lt;p&gt;选定一个目标，投入全部的注意力，不论做什么事情，都会觉得乐趣无穷。&lt;/p&gt;

&lt;h3 id=&quot;一旦尝到这种快乐我们就会加倍努力重温它的滋味自我就这样开始成长&quot;&gt;一旦尝到这种快乐，我们就会加倍努力，重温它的滋味，自我就这样开始成长。&lt;/h3&gt;

&lt;p&gt;心流之所以重要，不仅是因为他能使现在更快乐，也是因为它会强化我们的自信心。&lt;/p&gt;

&lt;p&gt;虽然心流没有捷径可走，但只要我们了解它的运作方式，就有可能使生活改观。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484913&amp;amp;idx=1&amp;amp;sn=9589d58b41c85d01d1aaa68872260539&amp;amp;chksm=fc2260f6cb55e9e08222aa5096a65949a05c98edc73eb005e289e11010c68a4816ed35a50035&amp;amp;token=134603822&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Wed, 22 Sep 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/09/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B044</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/09/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B044</guid>
            </item>
        
            <item>
               <title>读书笔记(四十三) 《游戏引擎架构》#2 容器、内存、RTTI与反射</title>
               <description>&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484906&amp;amp;idx=1&amp;amp;sn=70443040745649b082a93650d180959e&amp;amp;chksm=fc2260edcb55e9fb318468bd6f0ce4b2df2277338c3aeb1d0f31b00e4dcd01c18fba4b75581b&amp;amp;token=2034706848&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;背景&quot;&gt;背景：&lt;/h1&gt;

&lt;p&gt;作为游戏开发从业者，从业务到语言到框架到引擎，积累了一些知识和经验，特别是在看了好几遍《游戏引擎架构》后对引擎的架构感触颇深。&lt;/p&gt;

&lt;p&gt;近段时间对引擎剖析的想法也较多，正好借着书本对游戏引擎架构做一个完整分析。此书用简明、清楚的方式覆盖了游戏引擎架构的庞大领域，巧妙地平衡了广度与深度，并且提供了足够的细节，使得初学者也能很容易地理解其中的各种概念。&lt;/p&gt;

&lt;p&gt;目标是掌握游戏引擎架构知识，方法是跟随《游戏引擎架构》这本书、结合引擎源码、自己的经验，分析游戏引擎的历史、架构、模块。最后通过实践自主引擎的开发来完成对引擎知识的掌握。&lt;/p&gt;

&lt;p&gt;游戏引擎知识面深而广，所以对这系列的文章书编写范围做个保护，即不对细节进行过多的阐述，重点剖析的是架构、流程以及模块的运作原理。&lt;/p&gt;

&lt;p&gt;虽然参考了《游戏引擎架构》这本书，但由于它的部分知识太陈旧，所以我不得不将这些知识重新深挖后总结自己的观点。&lt;/p&gt;

&lt;h1 id=&quot;概述&quot;&gt;概述：&lt;/h1&gt;

&lt;p&gt;本章开始对引擎中的重要的模块和库进行详细的分析，我挑选了十五个库和模块来分析：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;时间库&lt;/li&gt;
  &lt;li&gt;自定义容器库&lt;/li&gt;
  &lt;li&gt;字符串散列库&lt;/li&gt;
  &lt;li&gt;内存管理框架&lt;/li&gt;
  &lt;li&gt;RTTI与反射模块&lt;/li&gt;
  &lt;li&gt;图形计算库&lt;/li&gt;
  &lt;li&gt;资产管理模块&lt;/li&gt;
  &lt;li&gt;低阶渲染器&lt;/li&gt;
  &lt;li&gt;剔除与合批模块&lt;/li&gt;
  &lt;li&gt;动画模块&lt;/li&gt;
  &lt;li&gt;物理模块&lt;/li&gt;
  &lt;li&gt;UI核心框架&lt;/li&gt;
  &lt;li&gt;性能剖析器的核心部分&lt;/li&gt;
  &lt;li&gt;脚本系统&lt;/li&gt;
  &lt;li&gt;视觉效果模块&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;本篇内容为列表中的前五个。&lt;/p&gt;

&lt;h1 id=&quot;正文&quot;&gt;正文：&lt;/h1&gt;

&lt;p&gt;由易到难，我们从最简单的说起。&lt;/p&gt;

&lt;h2 id=&quot;时间库&quot;&gt;时间库&lt;/h2&gt;

&lt;p&gt;时间库最简单使用的也最多，在引擎中的每个模块都会使用到。其时间信息包括：真实时间、游戏时间、全局时间、相对时间，时间缩放因子。
其常见问题为各平台的时间获取方式不同，因此基本都会针对每个平台分别实现一个时间获取函数。&lt;/p&gt;

&lt;p&gt;测量时间在时间库中占重要位置，游戏中循环调用间隔、帧率、以及移动速度都会使用时间测量的单位来进行。&lt;/p&gt;

&lt;p&gt;通常引擎都会用帧率调控的方法来稳定帧率，例如我们在引擎上设置了30帧/s，那么当本帧耗时小于33ms时，则在主循环结束时让线程在剩下的时间里休眠。
反之，如果主循环耗时大于33ms，则等待到下一帧再执行。通过这样调控的方式来稳定帧率。&lt;/p&gt;

&lt;p&gt;垂直同步是另一种帧率调控方法，由于前置缓冲区和后置缓冲区在交换时会有部分消隐问题导致画面撕裂，因此垂直同步会等待消隐时间，错过了则等待下一次消隐区间，这会让画面更加稳定，但并不保证以某个特定帧率运行而且时常会降低帧率，因此很少有游戏使用这种技术。&lt;/p&gt;

&lt;p&gt;当使用测量时间时，通常都会以帧的形式更新时间跨度，例如计算每帧之间的时间、计算动画当前帧、移动速度下当前帧的移动距离等。除此之外，最常用的如最大帧率、固定帧率、全局时间缩放等，这些因子在很多引擎模块中也常会用到，例如动画时间、音频时间，粒子生命时间等。&lt;/p&gt;

&lt;h2 id=&quot;自定义容器库&quot;&gt;自定义容器库&lt;/h2&gt;

&lt;p&gt;我们在使用的各式各样集合型数据结构也被称为容器，它们的任务都是一样的，存储及管理多个数据元素。
然而细节上各种容器运行方式会有一些差异，它们各自也有各自的优缺点。它们包括但不仅限于：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;数组（Array）&lt;/li&gt;
  &lt;li&gt;动态数组（Dynamic Array）&lt;/li&gt;
  &lt;li&gt;链表（Linked List）&lt;/li&gt;
  &lt;li&gt;堆栈（Stack）&lt;/li&gt;
  &lt;li&gt;队列（Queue）&lt;/li&gt;
  &lt;li&gt;双端队列（Double-ended Queue）&lt;/li&gt;
  &lt;li&gt;优先队列（Priority Queue）&lt;/li&gt;
  &lt;li&gt;树（Tree）&lt;/li&gt;
  &lt;li&gt;二叉查找树（Binary Search Tree，BST）&lt;/li&gt;
  &lt;li&gt;二叉堆（Binary Heap）&lt;/li&gt;
  &lt;li&gt;字典（Dictionary）&lt;/li&gt;
  &lt;li&gt;集合（Set）&lt;/li&gt;
  &lt;li&gt;图（Graph）&lt;/li&gt;
  &lt;li&gt;有向无环图（Directed Acyclic Graph，DAG）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们在操作容器时，常用的操作有：插入、移除、顺序访问（迭代）、随机访问、查找、排序。&lt;/p&gt;

&lt;p&gt;这里介绍下第三方标准库的优缺点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;STL，功能丰富、可移植能力强，但内存分配效率差一些，部分容器算法性能较差。&lt;/li&gt;
  &lt;li&gt;Boost，在STL上更多功能、效率高，发布许可证有些问题。&lt;/li&gt;
  &lt;li&gt;Loki，功能丰富，复杂度较高，性能未知。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;许多引擎都会提供常见的自定义容器实现，建立自定义容器类一般都处于如下原因：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;完全掌控：控制数据结构的内存需求、使用的算法、内存分配规则。&lt;/li&gt;
  &lt;li&gt;优化性能：针对某个业务做出适合性的调整，或借助某些硬件功能可以优化数据结构和算法。&lt;/li&gt;
  &lt;li&gt;可定制性：根据业务需要增加第三方库没有的功能、例如容器性能调试、内存统计、内存快照等。&lt;/li&gt;
  &lt;li&gt;消除外部依赖：当第三方库出现问题时，需要依赖外部的团队，这可能无法提供及时的服务。自定义容器，能在库出现问题时做到可自行修复。&lt;/li&gt;
  &lt;li&gt;并发同步：常用第三方容器在线程间并发同步上的操作可能没有你想的那么完美，用自定义容器就能为自己定制更合适的同步机制。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Unreal容器一看名字就知道是什么，它们包括：
TArray、TArrayView、TBasicArray、FBinaryHeap、TBitArray、TChunkedArray、TCircularBuffer、TCircularQueue、
TDiscardableKeyValueCache、TResourceArray、FHashTable、TIndirectArray、TLinkedList、TIntrusiveLinkedList、TDoubleLinkedList、TList、
TMapBase、TSortableMapBase、TQueue、FScriptArray、TSet、TSortedMap、TSparseArray、TStaticArray、TStaticBitArray、TTripleBuffer、TUnion等等&lt;/p&gt;

&lt;p&gt;Unity容器方面基本上与Unreal差不多，Unreal有的Unity也基本有，各自也都有一些特殊用途的容器，只是这些容器代码分散较开，说明容器部分的架构Unity编排的相对混乱一些。&lt;/p&gt;

&lt;h2 id=&quot;字符串散列库&quot;&gt;字符串散列库&lt;/h2&gt;

&lt;p&gt;字符串在程序中占据了很大的内存，通常有这三个问题，&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;拷贝多&lt;/li&gt;
  &lt;li&gt;拼接多&lt;/li&gt;
  &lt;li&gt;判断相同字符串&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;拷贝多原因&quot;&gt;拷贝多，原因：&lt;/h3&gt;

&lt;p&gt;在函数的形参和返回值上，常常会使用实例的方式去做，这导致字符串拷贝变的频繁。每次字符串拷贝都需要经历，内存分配，内存拷贝，内存销毁这三步骤，可想而知字符串在拷贝上的消耗非常大。&lt;/p&gt;

&lt;h3 id=&quot;拷贝多解决方案&quot;&gt;拷贝多，解决方案：&lt;/h3&gt;

&lt;p&gt;通常是由于业务代码引起的，因此也只有调整业务代码才能缓解。包括修改函数形参类型和返回值类型。&lt;/p&gt;

&lt;h3 id=&quot;拼接多原因&quot;&gt;拼接多，原因：&lt;/h3&gt;

&lt;p&gt;每个字符串在拼接完毕后通常都会生成一个临时的新的字符串，这导致拼接的那几个字符串内存被丢弃而浪费。&lt;/p&gt;

&lt;h3 id=&quot;拼接多解决方案&quot;&gt;拼接多，解决方案：&lt;/h3&gt;

&lt;p&gt;1.编码规范问题，具体业务具体分析
2.用对象池方式重复利用内存&lt;/p&gt;

&lt;h3 id=&quot;判断相同字符串原因&quot;&gt;判断相同字符串，原因：&lt;/h3&gt;

&lt;p&gt;相同字符串的判断逻辑在代码中占比通常比较大，特别是在业务逻辑中。如果只是单纯的比较两个字符串的每个字符，效率会变得非常低下&lt;/p&gt;

&lt;h3 id=&quot;判断相同字符串解决方案&quot;&gt;判断相同字符串，解决方案：&lt;/h3&gt;

&lt;p&gt;使用HashID就能解决这个问题，即把字符串计算成Hash值，用数字比较来代替字符串比较。&lt;/p&gt;

&lt;p&gt;注意，并不是一定要让字符串做Hash才能增加效率，如果相同字符的判断操作比较少，而Hash计算和加入容器的操作比较多，那么就会得不偿失。&lt;/p&gt;

&lt;h4 id=&quot;unreal引擎的字符串fstring构建&quot;&gt;Unreal引擎的字符串FString构建：&lt;/h4&gt;

&lt;p&gt;1.用TArray&lt;TCHAR&gt;动态数组作为容器
2.通过TArray&lt;TCHAR&gt;扩容构建和拼接字符串
3.TArray&lt;TCHAR&gt;使用专门的内存块作为管理，能做到提前分配和释放回池&lt;/TCHAR&gt;&lt;/TCHAR&gt;&lt;/TCHAR&gt;&lt;/p&gt;

&lt;h3 id=&quot;unreal引擎字符串的fname&quot;&gt;Unreal引擎字符串的FName：&lt;/h3&gt;

&lt;p&gt;1.FName和FString有些不同，FName有HashID，是含有一个uint32整数的结构体，而FString则没有
2.FName的字符串存储在FNameEntry实例中
3.每个FName都有自己的HashID，用于比较相同的字符串
4.FNameEntry是一个Char数组+HashID+Header的实例
5.通过FNamePool存储FNameEntry，FNamePool就是一个字典容器，存储着所有的FNameEntry实例
6.FName通过HashID从对象池中获得字符串实体FNameEntry
7.也可以直接通过HashID比较两个字符串是否相同，从而提高效率&lt;/p&gt;

&lt;h3 id=&quot;unity与unreal不同使用对象池方式重复利用字符串内存并且没有字符串散列机制因为引擎内部不需要&quot;&gt;Unity与Unreal不同，使用对象池方式重复利用字符串内存，并且没有字符串散列机制（因为引擎内部不需要）。&lt;/h3&gt;

&lt;h2 id=&quot;内存管理框架&quot;&gt;内存管理框架&lt;/h2&gt;

&lt;p&gt;先介绍下操作系统自身的内存管理方式：&lt;/p&gt;

&lt;p&gt;1.操作系统以进程为单位来运行每个程序。
2.同时为每个进程分配了一个独立的虚拟空间。
3.每个虚拟空间里有内核空间和用户空间之分。
4.内核空间为共享库和内核程序使用的堆栈空间。
5.每个虚拟空间都会拆分成多个段来存储各类数据和程序指令
6.虚拟内存和物理内存之间使用页表进行映射，因此在虚拟空间中连续的内存在物理空间中不一定连续。
7.操作系统会将不使用的内存块交换（Swap）出去成为硬盘空间的一部分，当需要访问时再交换（Swap）回来。&lt;/p&gt;

&lt;p&gt;下面我画了3个图，用图来解释会更容易理解些：&lt;/p&gt;

&lt;p&gt;图1&lt;/p&gt;

&lt;p&gt;图2&lt;/p&gt;

&lt;p&gt;图3&lt;/p&gt;

&lt;p&gt;以上三张图完整的体现了操作系统内存的运作方式。&lt;/p&gt;

&lt;p&gt;1.图1描述了，每个进程都有各自的独立虚拟空间，分为用户空间和内核空间，并且32位和64的空间大小不同。
2.图2描述了，一个虚拟空间中有很多个段，其中包括栈段、堆段、代码段、数据段等。
3.图3描述了，虚拟内存和物理内存通过页表映射，物理内容与硬盘会有一个Swap机制。&lt;/p&gt;

&lt;p&gt;写程序时我们比较关心堆内存，那么操作系统是如何管理堆内存的呢？&lt;/p&gt;

&lt;p&gt;我们来了解下堆内存的分配和释放机制&lt;/p&gt;

&lt;p&gt;图1：&lt;/p&gt;

&lt;p&gt;图2：&lt;/p&gt;

&lt;p&gt;图3：&lt;/p&gt;

&lt;p&gt;为了快速学习，画了三张图方便大家理解堆内存分配机制：
1.图1描述了，堆内存会切割成不同的块
2.图2描述了，堆内存分配设计为大堆、小堆和缓冲堆
3.图3描述了，堆内存分配流程，先找索引再从缓冲区中找最后切割大块内存&lt;/p&gt;

&lt;p&gt;一句话概括为，堆内存以分块方式切割设计，并分为大堆、小堆、和缓冲堆，通过索引和缓冲区来加速内存的分配和释放。&lt;/p&gt;

&lt;h3 id=&quot;游戏引擎中通常不依赖操作系统内存分配机制原因是对于引擎来说操作系统的内存分配效率太差因此每个引擎都有自建的内存管理框架&quot;&gt;游戏引擎中通常不依赖操作系统内存分配机制，原因是对于引擎来说操作系统的内存分配效率太差，因此每个引擎都有自建的内存管理框架。&lt;/h3&gt;

&lt;p&gt;自建的内存管理框架通常由一些分配规则构成，这些分配规则通常会写成内存分配器（Allocator）被用于引擎的各个模块中。&lt;/p&gt;

&lt;p&gt;虽然内存管理框架有很多种，但内存分配的规则都是相似的，每一种内存分配规则我们称为内存分配器（Allocator），下面我们简单列举一下内存分配器的种类及其规则。&lt;/p&gt;

&lt;p&gt;这里列举了11种（还有更多）内存分配器并分别用一句话概括它们：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;线性内存分配器，分配一块大内存，并不断向前分配。&lt;/li&gt;
  &lt;li&gt;环形内存分配器，支持循环利用的内存分配器。&lt;/li&gt;
  &lt;li&gt;双端内存分配器，两个模块共享一个线性内存分配，并从两端分别进行分配。&lt;/li&gt;
  &lt;li&gt;固定大小内存分配器，把一大块内存拆分成固定大小的N个内存块，每次分配一块。&lt;/li&gt;
  &lt;li&gt;泛化的固定大小内存分配器，拆分成M个大块内存，每个大块内存都有自己固定大小的N个内存块， 且不同块间的小内存块的大小不同。&lt;/li&gt;
  &lt;li&gt;散列式页内存分配器，按某个固定大小的页拆分内存块，用多叉树索引方式连接内存块，一次分配多页并调整树形索引。&lt;/li&gt;
  &lt;li&gt;栈型内存分配器，不断向前分配，并按先进后出的原则回收内存。&lt;/li&gt;
  &lt;li&gt;动态合并内存分配器，从一大块内存块开始分配，分配时不断切割，并在回收时合并相邻的内存块。&lt;/li&gt;
  &lt;li&gt;大小池内存分配器，有大内存池和小内存池之分，大小内存池的分配策略不同，小内存池通常使用泛型固定大小内存分配规则，大内存池由于分配频率低因此分配方式更自由些，可切割可固定可合并。&lt;/li&gt;
  &lt;li&gt;批量内存分配器，多次连续分配多个小内存，先临时分配并立即使用，最后提交锁定内存区间。&lt;/li&gt;
  &lt;li&gt;线程安全的内存分配器，在分配时加入了更多的原子操作或同步锁，让多个同线程可以共享一个内存块。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;引擎内存管理框架&quot;&gt;引擎内存管理框架&lt;/h2&gt;

&lt;p&gt;不同种类的引擎中的内存框架其实都是大同小异：&lt;/p&gt;

&lt;p&gt;1.大都使用内存分配器来搭建内存框架
2.多种类型的内存分配器混合使用很常见
3.每个模块都有自己的内存分配器
4.经常多个模块共享一个内存分配器&lt;/p&gt;

&lt;h2 id=&quot;unreal引擎的内存分配框架&quot;&gt;Unreal引擎的内存分配框架&lt;/h2&gt;

&lt;p&gt;Unreal有个HAL（Hardware Abstraction Layer）存放了大部分内存管理内容。&lt;/p&gt;

&lt;p&gt;用图来表示为：&lt;/p&gt;

&lt;p&gt;Unreal的内存管理框架总结：&lt;/p&gt;

&lt;p&gt;1.Unreal有统一的内存分配和释放接口FMemory
2.内存统计，使用的是获取堆栈、回溯堆栈信息的方式
3.虽然内存分配器有很多种，但主内存分配器只能选择一种
4.容器有自己专属的分配器，它封装了FMemory接口
5.UObject等业务逻辑有专属的分配器，它封装了FMemory接口，并在此基础上做了垃圾回收设计。
6.引擎各模块大部分使用了FMemory接口来分配和释放内存，少数封装了自己的分配器。
7.主内存分配器中FMallocBinned为主内存分配器（1、2、3代），其分配原则为大小内存池。&lt;/p&gt;

&lt;p&gt;Unity与Unreal稍有不同，它将GC和引擎内存分开管理，并且引擎独立模块有独立内存分配器自己管理。理论上来说这种做法会更好一些，每个模块需要有合适的内存分配算法。&lt;/p&gt;

&lt;h2 id=&quot;rtti与反射模块&quot;&gt;RTTI与反射模块&lt;/h2&gt;

&lt;p&gt;RTTI(Run-Time Type Information)运行时类型检查，它提供了运行时确定对象类型的方法。&lt;/p&gt;

&lt;p&gt;C++内建的RTTI通常很难满足我们的业务需求，特别是在需要做反射的业务上尤其明显，因此引擎通常都需要自建RTTI并增加反射系统。&lt;/p&gt;

&lt;p&gt;先说C++内建的RTTI。与内建RTTI相关的运算符为：typeid 和 dynamic_cast&lt;/p&gt;

&lt;p&gt;快速回顾下typeid 和 dynamic_cast两个运算符的原理&lt;/p&gt;

&lt;p&gt;我在前面的文章中详细介绍过C++内存模型，可以参考下&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://luzexi.com/2020/11/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B014&quot;&gt;《深度探索C++对象模型》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;每个有虚继承或虚函数的C++类都会在运行时有一个type_info数据，通过虚表中的指针指向type_info数据。&lt;/p&gt;

&lt;p&gt;type_info数据结构包含了类名字和父类指针，因此我们可以通过typeid来获得多态类的type_info数据。&lt;/p&gt;

&lt;p&gt;dynamic_cast就是借助type_info来做的功能，它通过多态的type_info来识别是否可以转换类型。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Point2D pt2d = dynamic_cast&amp;lt;Point2D&amp;gt;(pt);

可以拆解为：
Point2D pt2d = NULL;
type_info type_pt2d = typeid(Point2D);
type_info type_pt = typeid(Point);
if(type_pt2d == type_pt || type_pt.before(&amp;amp;type_pt2d))
{
        pt2d = (Point2D)pt;
}
return pt2d;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;自建rtti&quot;&gt;自建RTTI&lt;/h2&gt;

&lt;p&gt;由于内置C++的RTTI时常无法满足业务需求，所以通常人们都会自己去建立自己的RTTI。&lt;/p&gt;

&lt;p&gt;人们自建RTTI通常是因为：&lt;/p&gt;

&lt;p&gt;1.有反射需求，例如查找类、调用函数、获取变量、遍历属性等。
2.优化性能，包括减少RTTI内存，提高查找type_info效率等。&lt;/p&gt;

&lt;p&gt;Unreal引擎内部RTTI是默认被禁用的，它通过枚举或整数的方式来定制需要识别的类型。
Unity也是一样，在引擎内核中，无法使用内置的RTTI。&lt;/p&gt;

&lt;p&gt;禁用仅限于引擎实时运行库上，在工具套件和编辑器上仍然被使用。&lt;/p&gt;

&lt;h2 id=&quot;反射模块&quot;&gt;反射模块&lt;/h2&gt;

&lt;p&gt;反射模块是建立在自建RTTI之上的，因为反射需要通过RTTI来获取足够多的类、变量、函数的信息。&lt;/p&gt;

&lt;p&gt;通常当我们通过RTTI来建立反射框架时，通常需要对type信息结构做些规划。例如在type结构中增加动态数组或字典容器将变量和函数的名字和类型存储起来。&lt;/p&gt;

&lt;p&gt;示例代码例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct RTTR_LOCAL class_data
{
    class_data(get_derived_info_func func, std::vector&amp;lt;type&amp;gt; nested_types)
    :   m_derived_info_func(func),
        m_nested_types(nested_types),
        m_dtor(create_invalid_item&amp;lt;destructor&amp;gt;())
    {}
    get_derived_info_func       m_derived_info_func;
    std::vector&amp;lt;type&amp;gt;           m_base_types;
    std::vector&amp;lt;type&amp;gt;           m_derived_types;
    std::vector&amp;lt;rttr_cast_func&amp;gt; m_conversion_list;
    std::vector&amp;lt;property&amp;gt;       m_properties;
    std::vector&amp;lt;method&amp;gt;         m_methods;
    std::vector&amp;lt;constructor&amp;gt;    m_ctors;
    std::vector&amp;lt;type&amp;gt;           m_nested_types;
    destructor                  m_dtor;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上代码中我们看到，type实例结构中通常有属性结构实例容器、类型结构实例容器、函数结构实例容器等，目的就是为了存储变量和函数的信息。在实时运行过程中，当我们需要用字符串查找某个函数，或者查找某个变量时，则会从这些容器中去查找。&lt;/p&gt;

&lt;p&gt;自建RTTI是通过对象信息来构建RTTI信息集合代码的，因此每个需要RTTI类的实例都需要写相应的编码，通常都会用宏和自动化来代替繁琐的实例化RTTI编码。&lt;/p&gt;

&lt;p&gt;其基本思想是采用宏来代替常规的变量定义，这样我们就可以在宏函数中将定义的变量添加至自建的反射系统。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;rttr/registration&amp;gt;
using namespace rttr;

struct MyStruct { MyStruct() {}; void func(double) {}; int data; };

RTTR_REGISTRATION
{
    registration::class_&amp;lt;MyStruct&amp;gt;(&quot;MyStruct&quot;)
         .constructor&amp;lt;&amp;gt;()
         .property(&quot;data&quot;, &amp;amp;MyStruct::data)
         .method(&quot;func&quot;, &amp;amp;MyStruct::func);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;unreal的rtti和反射用途较多主要包括蓝图和垃圾回收&quot;&gt;Unreal的RTTI和反射用途较多，主要包括蓝图和垃圾回收。&lt;/h3&gt;

&lt;p&gt;与其他所有引擎一样，Unreal生成反射代码的步骤是：&lt;/p&gt;

&lt;p&gt;1.利用特殊的宏来对变量做标记
2.对C++代码文件进行语法分析
3.用指定的宏提取出对应的数据
4.扫描工具生成RTTI代码
5.最后在初始化时运行生成的代码
6.启动时将收集到的数据保存&lt;/p&gt;

&lt;p&gt;UE4定义了一系列的宏，来帮助开发者将自定义的字段和函数添加至反射系统：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UCLASS，告诉UE这个类是一个反射类。类必须派生自UObject
USTRUCT，可以不用派生自UObject。不支持GC，也不能包含函数
UPROPERTY，定义一个反射的变量
UFUNCTION，定义一个反射的函数
UENUM，告诉UE这是一个反射的枚举类。支持enum, enum class, enum namespace
UINTERFACE，定义一个反射接口类，只能包含函数
UMETA，反射的一些元数据定义，可以通过标签定义一些该变量的属性
UPARAM，定义函数的参数属性。主要就是显示名字和Ref属性
UDELEGATE，告诉UE这是一个可反射的delegate

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一定要声明UCLASS
UCLASS()
class MYGAME_API UMyClass : public UObject
{
    GENERATED_BODY()
    public:
        // 定义一个可反射的函数
        UFUNCTION(BluprintCallable)
        void MyFunc();private:

        // 定义一个可反射的变量
        UPROPERTY(EditAnywhere, BlueprintReadWrite, meta=(AllowPrivateAccess = &quot;true&quot;))
        int MyIntValue;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过声明反射将数据结构、变量和函数添加到反射中。&lt;/p&gt;

&lt;p&gt;UE4的UHT（Unreal Header Tool）模块扫描之后生成的代码反射两个代码文件：&lt;/p&gt;

&lt;p&gt;.generated.h文件：重载各种操作符函数，声明各种构造函数。
.gen.cpp文件：单例实现，构造UClass（提取信息并注册）&lt;/p&gt;

&lt;p&gt;Unity与Unreal的稍有不同，一部分为C++调用Mono，另一部分为C#利用反射调用具体的函数。&lt;/p&gt;

&lt;p&gt;1.C++通过Mono获取C#接口
2.反过来C#调用C++则不需要反射。可以通过数组将C++句柄存储起来，用索引获取句柄的方式调用。
3.C#使用反射调用C#代码则比较常见，在编辑器与业务逻辑交互之间会比较多。&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料：&lt;/h3&gt;

&lt;p&gt;《动态链接与装载》&lt;/p&gt;

&lt;p&gt;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484788&amp;amp;idx=1&amp;amp;sn=8e4f7f36e4ddf6dd2f7c086463c089de&amp;amp;chksm=fc226073cb55e965e5ab259f04293c48f718a2fd72858b11397852d8ba8c7fab656b0f46e248&amp;amp;token=283688006&amp;amp;lang=zh_CN#rd&lt;/p&gt;

&lt;p&gt;《深度探索C++对象模型》&lt;/p&gt;

&lt;p&gt;http://luzexi.com/2020/11/20/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B014&lt;/p&gt;

&lt;p&gt;《malloc和free的实现原理解析》&lt;/p&gt;

&lt;p&gt;https://jacktang816.github.io/post/mallocandfree/&lt;/p&gt;

&lt;p&gt;《UE4 反射系统详细剖析》&lt;/p&gt;

&lt;p&gt;https://cloud.tencent.com/developer/article/1606872&lt;/p&gt;

&lt;p&gt;《UE4内存分配器概述》&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/kekec/p/12012537.html&lt;/p&gt;

&lt;p&gt;《RTTR C++ Reflection Library》&lt;/p&gt;

&lt;p&gt;https://github.com/rttrorg/rttr&lt;/p&gt;

&lt;p&gt;《UE4 MallocBinned2分配器》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/79715624&lt;/p&gt;

&lt;p&gt;《FMallocBinned2内存分配器》&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/kekec/p/14675228.html&lt;/p&gt;

&lt;p&gt;《UE4垃圾回收》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/67055774&lt;/p&gt;

&lt;p&gt;《UE4蓝图》&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/67683606&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1ODY1ODY2NA==&amp;amp;mid=2247484906&amp;amp;idx=1&amp;amp;sn=70443040745649b082a93650d180959e&amp;amp;chksm=fc2260edcb55e9fb318468bd6f0ce4b2df2277338c3aeb1d0f31b00e4dcd01c18fba4b75581b&amp;amp;token=2034706848&amp;amp;lang=zh_CN#rd&quot;&gt;已发布在微信公众号上，点击跳转&lt;/a&gt;&lt;/p&gt;

</description>
               <pubDate>Fri, 17 Sep 2021 00:00:00 +0800</pubDate>
               <link>http://www.luzexi.com/2021/09/17/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B043</link>
               <guid isPermaLink="true">http://www.luzexi.com/2021/09/17/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B043</guid>
            </item>
        
    </channel>
</rss>